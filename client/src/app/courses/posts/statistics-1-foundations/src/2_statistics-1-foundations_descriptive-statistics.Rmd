---
title: "Statistics with R I: Foundations"
chapter: "Chapter 2: Descriptive Statistics — Summarising Data Numerically"
coverImage: 13
author: "Dereck Mezquita"
date: "`r Sys.Date()`"
tags:
  - statistics
  - mathematics
  - descriptive
  - data
  - R
  - biomedical
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7, comment = "#>", warning = FALSE, collapse = TRUE)
```

# Chapter 2: Descriptive Statistics — Summarising Data Numerically

This chapter covers the fundamental numerical summaries used to describe datasets. We learn to quantify centre, spread, position, and shape, implementing each measure from scratch to understand what they truly measure. By the end of this chapter, you will be able to compute and interpret all major descriptive statistics and choose appropriate measures for different data types.

```{r packages, message=FALSE, warning=FALSE}
box::use(
    data.table,
    ggplot2
)
```

```{r load_data, message=FALSE}
# Load NHANES data for examples
nhanes <- data.table$fread("data/primary/nhanes.csv")

# Quick overview
cat("NHANES dataset:", nrow(nhanes), "observations,", ncol(nhanes), "variables\n")
```

## 2.1 Measures of Central Tendency

Central tendency describes where the "middle" of a distribution lies. Three measures dominate: the mean, median, and mode. Each captures a different aspect of centrality, and understanding when to use each is essential for proper data description.

### 2.1.1 The Arithmetic Mean

The **arithmetic mean** is the most familiar measure of central tendency. Informally, it is the "average": add up all values and divide by the count.

**Prose and Intuition**

Imagine placing data points on a number line as physical weights. The mean is the balance point: if you placed a fulcrum at the mean, the number line would balance perfectly. Points far from the mean exert greater leverage; they "pull" the mean toward themselves.

This physical intuition explains why the mean is sensitive to extreme values. A single outlier, far from the others, exerts disproportionate influence on the balance point.

**Mathematical Derivation**

For a sample of n observations $x_1, x_2, \ldots, x_n$, the sample mean is defined as:

$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i = \frac{x_1 + x_2 + \cdots + x_n}{n}$$

Why this formula? The mean has a deeper mathematical justification: it is the value that minimises the sum of squared deviations. That is, if we seek a value $c$ that minimises:

$$\sum_{i=1}^{n} (x_i - c)^2$$

then the unique solution is $c = \bar{x}$.

**Proof:** Take the derivative with respect to $c$ and set it to zero:

$$\frac{d}{dc} \sum_{i=1}^{n} (x_i - c)^2 = -2 \sum_{i=1}^{n} (x_i - c) = 0$$

$$\sum_{i=1}^{n} x_i - nc = 0$$

$$c = \frac{1}{n} \sum_{i=1}^{n} x_i = \bar{x}$$

The mean is thus the "least squares" measure of location.

```{r mean_from_scratch, fig.cap="The mean is the balance point of the distribution"}
set.seed(42)

# Implement mean from scratch
my_mean <- function(x) {
    # Remove NA values
    x <- x[!is.na(x)]

    # Sum all values
    total <- 0
    for (i in seq_along(x)) {
        total <- total + x[i]
    }

    # Divide by count
    return(total / length(x))
}

# Test on NHANES BMI data
bmi_sample <- nhanes[!is.na(BMI), BMI][1:100]

cat("Our implementation:", my_mean(bmi_sample), "\n")
cat("Built-in mean():", mean(bmi_sample), "\n")

# Visualise the mean as balance point
bmi_dt <- data.table$data.table(bmi = bmi_sample)
mean_bmi <- mean(bmi_sample)

ggplot2$ggplot(bmi_dt, ggplot2$aes(x = bmi)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 20,
                   fill = "#56B4E9", colour = "white", alpha = 0.8) +
    ggplot2$geom_density(colour = "#0072B2", size = 1) +
    ggplot2$geom_vline(xintercept = mean_bmi, colour = "red",
               linetype = "dashed", size = 1.2) +
    ggplot2$annotate("text", x = mean_bmi + 2, y = 0.08,
             label = paste("Mean =", round(mean_bmi, 1)),
             colour = "red", size = 5) +
    ggplot2$labs(
        title = "The Mean as Balance Point",
        subtitle = "BMI data from NHANES; mean marked with red dashed line",
        x = "Body Mass Index (kg/m²)",
        y = "Density"
    ) +
    ggplot2$theme_minimal()
```

**Key properties of the mean:**

1. **Uniqueness**: Every dataset has exactly one mean
2. **Sensitivity to outliers**: Extreme values strongly affect the mean
3. **Uses all data**: Every observation contributes to the mean
4. **Algebraic tractability**: Mathematical operations on means are straightforward

### 2.1.2 The Median

The **median** is the middle value when data are arranged in order. Half the observations fall below the median, and half fall above.

**Prose and Intuition**

If the mean is the balance point, the median is the "halfway point" in terms of count. Imagine lining up people by height: the median height is the height of the person standing in the middle, regardless of whether the tallest person is 180 cm or 250 cm. This explains the median's robustness: extreme values do not shift the middle position.

**Mathematical Derivation**

For ordered data $x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$:

$$\text{Median} = \begin{cases}
x_{((n+1)/2)} & \text{if } n \text{ is odd} \\
\frac{x_{(n/2)} + x_{(n/2+1)}}{2} & \text{if } n \text{ is even}
\end{cases}$$

The median minimises the sum of **absolute** deviations (not squared):

$$\sum_{i=1}^{n} |x_i - c|$$

is minimised when $c$ equals the median. This is why the median is more robust: absolute deviations do not give extra weight to extreme values as squared deviations do.

```{r median_from_scratch}
# Implement median from scratch
my_median <- function(x) {
    # Remove NA values
    x <- x[!is.na(x)]

    # Sort the data
    x_sorted <- sort(x)
    n <- length(x_sorted)

    if (n %% 2 == 1) {
        # Odd number of observations: middle value
        middle_index <- (n + 1) / 2
        return(x_sorted[middle_index])
    } else {
        # Even number: average of two middle values
        lower_index <- n / 2
        upper_index <- n / 2 + 1
        return((x_sorted[lower_index] + x_sorted[upper_index]) / 2)
    }
}

# Test
cat("Our implementation:", my_median(bmi_sample), "\n")
cat("Built-in median():", median(bmi_sample), "\n")

# Demonstrate robustness to outliers
normal_data <- c(10, 12, 14, 15, 16, 18, 20)
outlier_data <- c(10, 12, 14, 15, 16, 18, 200)  # 200 is an outlier

cat("\nNormal data:\n")
cat("  Mean:", mean(normal_data), "  Median:", median(normal_data), "\n")

cat("\nWith outlier (200):\n")
cat("  Mean:", mean(outlier_data), "  Median:", median(outlier_data), "\n")
cat("\nThe mean shifted dramatically; the median barely changed.\n")
```

### 2.1.3 The Mode

The **mode** is the most frequently occurring value. Unlike mean and median, the mode can be used with nominal data.

**Prose and Intuition**

The mode identifies the "typical" value in the sense of "most common." In a histogram, the mode corresponds to the peak. Some distributions are **unimodal** (one peak), **bimodal** (two peaks), or **multimodal** (multiple peaks). The presence of multiple modes often signals distinct subgroups in the data.

**Implementation**

```{r mode_from_scratch, fig.cap="Bimodal distributions have two modes indicating distinct subgroups"}
# Implement mode from scratch
my_mode <- function(x) {
    # Remove NA values
    x <- x[!is.na(x)]

    # Create frequency table
    freq_table <- table(x)

    # Find maximum frequency
    max_freq <- max(freq_table)

    # Return all values with maximum frequency
    modes <- names(freq_table)[freq_table == max_freq]

    # Convert back to numeric if possible
    if (is.numeric(x)) {
        modes <- as.numeric(modes)
    }

    return(modes)
}

# Mode works well for discrete/categorical data
education_data <- nhanes[!is.na(Education), Education]
cat("Mode of Education levels:", my_mode(education_data), "\n")
print(table(education_data))

# For continuous data, mode is less useful without binning
# Create bimodal data to illustrate
set.seed(123)
bimodal_data <- c(
    rnorm(200, mean = 25, sd = 3),  # Young adults
    rnorm(150, mean = 55, sd = 5)   # Older adults
)

bimodal_dt <- data.table$data.table(age = bimodal_data)

ggplot2$ggplot(bimodal_dt, ggplot2$aes(x = age)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 30,
                   fill = "#E69F00", colour = "white", alpha = 0.8) +
    ggplot2$geom_density(colour = "#D55E00", size = 1.2) +
    ggplot2$geom_vline(xintercept = c(25, 55), colour = "blue",
               linetype = "dashed", size = 1) +
    ggplot2$labs(
        title = "Bimodal Distribution: Two Peaks Indicate Subgroups",
        subtitle = "Example: Age distribution with young and older adult populations",
        x = "Age (years)",
        y = "Density"
    ) +
    ggplot2$annotate("text", x = 25, y = 0.055, label = "Mode 1", colour = "blue") +
    ggplot2$annotate("text", x = 55, y = 0.055, label = "Mode 2", colour = "blue") +
    ggplot2$theme_minimal()
```

### 2.1.4 Comparing Mean, Median, and Mode

The relationship between mean, median, and mode reveals the shape of a distribution.

```{r compare_measures, fig.cap="Skewness determines the relationship between mean, median, and mode"}
set.seed(456)

# Create three distributions: symmetric, right-skewed, left-skewed
n <- 1000

symmetric <- rnorm(n, mean = 50, sd = 10)
right_skewed <- rgamma(n, shape = 2, rate = 0.1)
left_skewed <- 100 - rgamma(n, shape = 2, rate = 0.1)

# Calculate measures for each
calc_measures <- function(x) {
    # For mode, use density estimation peak
    dens <- density(x)
    mode_val <- dens$x[which.max(dens$y)]

    data.table$data.table(
        mean = mean(x),
        median = median(x),
        mode = mode_val
    )
}

symmetric_measures <- calc_measures(symmetric)
right_measures <- calc_measures(right_skewed)
left_measures <- calc_measures(left_skewed)

# Combine for plotting
plot_data <- data.table$rbindlist(list(
    data.table$data.table(value = symmetric, distribution = "Symmetric"),
    data.table$data.table(value = right_skewed, distribution = "Right-Skewed"),
    data.table$data.table(value = left_skewed, distribution = "Left-Skewed")
))

measures_data <- data.table$rbindlist(list(
    cbind(symmetric_measures, distribution = "Symmetric"),
    cbind(right_measures, distribution = "Right-Skewed"),
    cbind(left_measures, distribution = "Left-Skewed")
))

measures_long <- data.table$melt(
    measures_data,
    id.vars = "distribution",
    variable.name = "measure",
    value.name = "value"
)

ggplot2$ggplot(plot_data, ggplot2$aes(x = value)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 30,
                   fill = "#56B4E9", colour = "white", alpha = 0.7) +
    ggplot2$geom_density(colour = "#0072B2", size = 1) +
    ggplot2$geom_vline(data = measures_long,
               ggplot2$aes(xintercept = value, colour = measure, linetype = measure),
               size = 1) +
    ggplot2$facet_wrap(~distribution, scales = "free", ncol = 1) +
    ggplot2$scale_colour_manual(
        values = c("mean" = "red", "median" = "green", "mode" = "purple")
    ) +
    ggplot2$scale_linetype_manual(
        values = c("mean" = "dashed", "median" = "dotted", "mode" = "solid")
    ) +
    ggplot2$labs(
        title = "Mean, Median, and Mode in Different Distributions",
        subtitle = "Symmetric: all equal; Right-skewed: mode < median < mean; Left-skewed: mean < median < mode",
        x = "Value",
        y = "Density",
        colour = "Measure",
        linetype = "Measure"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "bottom")
```

**Guidelines for choosing:**

| Situation | Best Measure | Reason |
|-----------|--------------|--------|
| Symmetric distribution | Mean | Uses all data efficiently |
| Skewed distribution | Median | Not affected by extreme values |
| Outliers present | Median | Robust to extremes |
| Categorical data | Mode | Only applicable measure |
| Comparing to population mean | Mean | Algebraically compatible |

### 2.1.5 Other Means: Weighted, Trimmed, Geometric, Harmonic

Beyond the arithmetic mean, specialised means serve specific purposes.

**Weighted Mean**

When observations have different importances, we use a weighted mean:

$$\bar{x}_w = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}$$

where $w_i$ is the weight for observation $i$.

```{r weighted_mean}
# Implement weighted mean from scratch
my_weighted_mean <- function(x, w) {
    # Remove NA values (from both x and corresponding weights)
    valid <- !is.na(x) & !is.na(w)
    x <- x[valid]
    w <- w[valid]

    return(sum(w * x) / sum(w))
}

# Example: calculating course grade
# Different assessments have different weights
assignments <- c(85, 90, 78, 92)  # Assignment scores
weights <- c(0.1, 0.1, 0.3, 0.5)  # 10%, 10%, 30%, 50%

cat("Unweighted mean:", mean(assignments), "\n")
cat("Weighted mean:", my_weighted_mean(assignments, weights), "\n")
cat("Built-in weighted.mean():", weighted.mean(assignments, weights), "\n")
```

**Trimmed Mean**

The trimmed mean removes a percentage of extreme values before calculating:

```{r trimmed_mean}
# Implement trimmed mean from scratch
my_trimmed_mean <- function(x, trim = 0.1) {
    x <- x[!is.na(x)]
    x_sorted <- sort(x)
    n <- length(x_sorted)

    # Number of observations to trim from each end
    k <- floor(n * trim)

    # Calculate mean of remaining values
    if (k > 0) {
        trimmed_values <- x_sorted[(k + 1):(n - k)]
    } else {
        trimmed_values <- x_sorted
    }

    return(mean(trimmed_values))
}

# Data with outliers
outlier_data <- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 100)

cat("Regular mean:", mean(outlier_data), "\n")
cat("10% trimmed mean:", my_trimmed_mean(outlier_data, 0.1), "\n")
cat("Built-in mean(trim=0.1):", mean(outlier_data, trim = 0.1), "\n")
cat("Median:", median(outlier_data), "\n")
```

**Geometric Mean**

The geometric mean is appropriate for multiplicative relationships, such as growth rates:

$$\bar{x}_g = \left(\prod_{i=1}^{n} x_i\right)^{1/n} = \exp\left(\frac{1}{n}\sum_{i=1}^{n} \ln(x_i)\right)$$

```{r geometric_mean}
# Implement geometric mean from scratch
my_geometric_mean <- function(x) {
    x <- x[!is.na(x)]

    # All values must be positive
    if (any(x <= 0)) {
        stop("Geometric mean requires all positive values")
    }

    # Use log transformation for numerical stability
    return(exp(mean(log(x))))
}

# Example: average annual growth rate
# Year 1: 10% growth, Year 2: 20% growth, Year 3: -5% loss
growth_factors <- c(1.10, 1.20, 0.95)

# Arithmetic mean suggests 8.33% average growth
cat("Arithmetic mean of growth factors:", mean(growth_factors), "\n")
cat("Suggests average growth of:", (mean(growth_factors) - 1) * 100, "%\n\n")

# Geometric mean gives the true average growth
cat("Geometric mean of growth factors:", my_geometric_mean(growth_factors), "\n")
cat("True average growth:", (my_geometric_mean(growth_factors) - 1) * 100, "%\n\n")

# Verify: $100 after 3 years
initial <- 100
final <- initial * prod(growth_factors)
cat("$100 after 3 years: $", round(final, 2), "\n")
cat("Using geometric mean:", round(initial * my_geometric_mean(growth_factors)^3, 2), "\n")
```

**Harmonic Mean**

The harmonic mean is appropriate for rates and ratios:

$$\bar{x}_h = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_i}}$$

```{r harmonic_mean}
# Implement harmonic mean from scratch
my_harmonic_mean <- function(x) {
    x <- x[!is.na(x)]

    # All values must be positive
    if (any(x <= 0)) {
        stop("Harmonic mean requires all positive values")
    }

    return(length(x) / sum(1 / x))
}

# Example: average speed
# Drive to work at 30 km/h, return at 60 km/h
# What is the average speed?
speeds <- c(30, 60)

cat("Arithmetic mean:", mean(speeds), "km/h (WRONG!)\n")
cat("Harmonic mean:", my_harmonic_mean(speeds), "km/h (CORRECT)\n\n")

# Verify: for distance d each way
# Time to work: d/30 hours
# Time home: d/60 hours
# Total time: d/30 + d/60 = d/20 hours
# Total distance: 2d km
# Average speed: 2d / (d/20) = 40 km/h
cat("Verification: average speed = total distance / total time = 40 km/h\n")
```

**Relationship between means:**

For any dataset with positive values: Harmonic ≤ Geometric ≤ Arithmetic

Equality holds only when all values are identical.

## 2.2 Measures of Dispersion (Spread)

Central tendency tells us where the data are located; dispersion tells us how spread out they are. Two datasets can have identical means yet vastly different spreads.

### 2.2.1 Range and Interquartile Range

**Range**

The simplest measure of spread is the range: the difference between maximum and minimum values.

$$\text{Range} = x_{\text{max}} - x_{\text{min}}$$

```{r range_from_scratch}
# Implement range from scratch
my_range <- function(x) {
    x <- x[!is.na(x)]
    return(max(x) - min(x))
}

# Example with NHANES blood pressure
bp_data <- nhanes[!is.na(BPSysAve), BPSysAve]

cat("Range of systolic BP:", my_range(bp_data), "mmHg\n")
cat("Min:", min(bp_data), " Max:", max(bp_data), "\n")
```

**Limitations:** The range uses only two values and is extremely sensitive to outliers. A single extreme observation can dramatically inflate the range.

**Interquartile Range (IQR)**

The IQR is the range of the middle 50% of the data:

$$\text{IQR} = Q_3 - Q_1$$

where $Q_1$ is the first quartile (25th percentile) and $Q_3$ is the third quartile (75th percentile).

```{r iqr_from_scratch, fig.cap="The IQR captures the spread of the middle 50% of data"}
# Implement quartiles and IQR from scratch
my_quantile <- function(x, p) {
    # Simple linear interpolation method
    x <- sort(x[!is.na(x)])
    n <- length(x)

    # Position of the quantile
    h <- (n - 1) * p + 1

    # Lower and upper indices
    lo <- floor(h)
    hi <- ceiling(h)

    # Linear interpolation
    return(x[lo] + (h - lo) * (x[hi] - x[lo]))
}

my_iqr <- function(x) {
    q1 <- my_quantile(x, 0.25)
    q3 <- my_quantile(x, 0.75)
    return(q3 - q1)
}

# Test
cat("Q1:", my_quantile(bp_data, 0.25), "\n")
cat("Q3:", my_quantile(bp_data, 0.75), "\n")
cat("Our IQR:", my_iqr(bp_data), "\n")
cat("Built-in IQR():", IQR(bp_data), "\n")

# Visualise
bp_dt <- data.table$data.table(bp = bp_data)
q1_val <- quantile(bp_data, 0.25)
q3_val <- quantile(bp_data, 0.75)

ggplot2$ggplot(bp_dt, ggplot2$aes(x = bp)) +
    ggplot2$geom_histogram(bins = 40, fill = "#56B4E9", colour = "white", alpha = 0.8) +
    ggplot2$geom_vline(xintercept = c(q1_val, q3_val),
               colour = "red", linetype = "dashed", size = 1) +
    ggplot2$annotate("rect", xmin = q1_val, xmax = q3_val,
             ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
    ggplot2$annotate("text", x = (q1_val + q3_val) / 2, y = 800,
             label = paste("IQR =", round(q3_val - q1_val, 1)),
             colour = "red", size = 5) +
    ggplot2$labs(
        title = "Interquartile Range: Middle 50% of Data",
        subtitle = "Systolic blood pressure from NHANES",
        x = "Systolic Blood Pressure (mmHg)",
        y = "Count"
    ) +
    ggplot2$theme_minimal()
```

### 2.2.2 Variance

Variance measures the average squared deviation from the mean. Why squared? Squaring ensures all deviations are positive and gives more weight to larger deviations.

**Population Variance**

$$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$$

**Sample Variance**

$$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

**Why divide by n-1?**

This is one of the most common questions in statistics. The intuition: when we estimate the population variance from a sample, we use $\bar{x}$ instead of $\mu$. Since $\bar{x}$ is calculated from the same data, the deviations from $\bar{x}$ are systematically smaller than deviations from $\mu$. Dividing by $n-1$ instead of $n$ corrects this underestimation.

The formal proof involves showing that $E[s^2] = \sigma^2$ when we divide by $n-1$ (making $s^2$ an unbiased estimator), whilst dividing by $n$ gives $E[\hat{\sigma}^2] = \frac{n-1}{n}\sigma^2$.

```{r variance_derivation}
# Implement variance from scratch
# Population variance (divide by N)
my_pop_variance <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)

    # Sum of squared deviations
    ss <- sum((x - mean_x)^2)

    return(ss / n)
}

# Sample variance (divide by n-1)
my_sample_variance <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)

    # Sum of squared deviations
    ss <- sum((x - mean_x)^2)

    return(ss / (n - 1))
}

# Test
bmi_clean <- nhanes[!is.na(BMI), BMI]

cat("Population variance:", my_pop_variance(bmi_clean), "\n")
cat("Sample variance:", my_sample_variance(bmi_clean), "\n")
cat("Built-in var():", var(bmi_clean), "\n")
```

**Demonstrating n-1 correction through simulation:**

```{r bessel_correction, fig.cap="Dividing by n-1 corrects the bias in variance estimation"}
set.seed(789)

# True population
pop_size <- 100000
pop_mean <- 50
pop_sd <- 10
population <- rnorm(pop_size, mean = pop_mean, sd = pop_sd)
true_variance <- var(population) * (pop_size - 1) / pop_size  # Population variance

# Repeatedly sample and estimate variance both ways
n_simulations <- 5000
sample_size <- 10

results <- data.table$data.table(
    sim = 1:n_simulations,
    var_n = numeric(n_simulations),
    var_n_minus_1 = numeric(n_simulations)
)

for (i in 1:n_simulations) {
    samp <- sample(population, sample_size)
    mean_samp <- mean(samp)
    ss <- sum((samp - mean_samp)^2)

    results[i, var_n := ss / sample_size]
    results[i, var_n_minus_1 := ss / (sample_size - 1)]
}

cat("True population variance:", round(true_variance, 2), "\n")
cat("Mean of s² (divide by n):", round(mean(results$var_n), 2),
    "- biased LOW\n")
cat("Mean of s² (divide by n-1):", round(mean(results$var_n_minus_1), 2),
    "- unbiased\n")

# Visualise
results_long <- data.table$melt(
    results,
    id.vars = "sim",
    variable.name = "method",
    value.name = "variance"
)
results_long[, method := factor(method,
    levels = c("var_n", "var_n_minus_1"),
    labels = c("Divide by n (biased)", "Divide by n-1 (unbiased)")
)]

ggplot2$ggplot(results_long, ggplot2$aes(x = variance, fill = method)) +
    ggplot2$geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
    ggplot2$geom_vline(xintercept = true_variance, colour = "black",
               linetype = "dashed", size = 1.2) +
    ggplot2$facet_wrap(~method, ncol = 1) +
    ggplot2$labs(
        title = "Why We Divide by n-1: Bessel's Correction",
        subtitle = paste("5000 samples of size 10; true variance =",
                        round(true_variance, 1)),
        x = "Estimated Variance",
        y = "Count"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

### 2.2.3 Standard Deviation

The standard deviation is the square root of variance:

$$s = \sqrt{s^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$

The standard deviation returns us to the original units of measurement. If we measure weight in kilograms, variance is in "kilograms squared" (which is difficult to interpret), but standard deviation is in kilograms.

```{r sd_from_scratch}
# Implement standard deviation from scratch
my_sd <- function(x) {
    return(sqrt(my_sample_variance(x)))
}

# Test
cat("Our implementation:", my_sd(bmi_clean), "\n")
cat("Built-in sd():", sd(bmi_clean), "\n")

# Interpretation: for approximately normal data
# About 68% of data fall within 1 SD of the mean
# About 95% fall within 2 SD
# About 99.7% fall within 3 SD
mean_bmi <- mean(bmi_clean)
sd_bmi <- sd(bmi_clean)

within_1sd <- mean(bmi_clean >= mean_bmi - sd_bmi &
                   bmi_clean <= mean_bmi + sd_bmi)
within_2sd <- mean(bmi_clean >= mean_bmi - 2*sd_bmi &
                   bmi_clean <= mean_bmi + 2*sd_bmi)
within_3sd <- mean(bmi_clean >= mean_bmi - 3*sd_bmi &
                   bmi_clean <= mean_bmi + 3*sd_bmi)

cat("\nEmpirical rule check for BMI data:\n")
cat("Within 1 SD:", round(within_1sd * 100, 1), "% (expected ~68%)\n")
cat("Within 2 SD:", round(within_2sd * 100, 1), "% (expected ~95%)\n")
cat("Within 3 SD:", round(within_3sd * 100, 1), "% (expected ~99.7%)\n")
```

### 2.2.4 Coefficient of Variation

The coefficient of variation (CV) expresses standard deviation as a percentage of the mean:

$$\text{CV} = \frac{s}{\bar{x}} \times 100\%$$

The CV enables comparison of variability across different scales.

```{r cv_from_scratch}
# Implement coefficient of variation from scratch
my_cv <- function(x) {
    x <- x[!is.na(x)]
    return(sd(x) / mean(x) * 100)
}

# Compare variability of height and weight
height_data <- nhanes[!is.na(Height), Height]
weight_data <- nhanes[!is.na(Weight), Weight]

cat("Height: mean =", round(mean(height_data), 1), "cm,",
    "SD =", round(sd(height_data), 1), "cm,",
    "CV =", round(my_cv(height_data), 1), "%\n")

cat("Weight: mean =", round(mean(weight_data), 1), "kg,",
    "SD =", round(sd(weight_data), 1), "kg,",
    "CV =", round(my_cv(weight_data), 1), "%\n")

cat("\nWeight is relatively more variable than height\n")
cat("(even though height has a larger SD in absolute terms)\n")
```

### 2.2.5 Mean Absolute Deviation

The mean absolute deviation (MAD) is easier to interpret than variance: it is the average distance from the mean.

$$\text{MAD} = \frac{1}{n} \sum_{i=1}^{n} |x_i - \bar{x}|$$

```{r mad_from_scratch}
# Implement mean absolute deviation from scratch
my_mad_mean <- function(x) {
    x <- x[!is.na(x)]
    return(mean(abs(x - mean(x))))
}

# Test
cat("Mean absolute deviation (from mean):", my_mad_mean(bmi_clean), "\n")
cat("Standard deviation:", sd(bmi_clean), "\n")
cat("\nMAD is always smaller than SD because squaring amplifies large deviations\n")
```

### 2.2.6 Robust Measures of Spread

When outliers are present, robust measures outperform classical ones.

**Median Absolute Deviation (MAD)**

The MAD uses the median instead of the mean:

$$\text{MAD} = \text{median}(|x_i - \text{median}(x)|)$$

For comparison with SD, we often multiply by a scale factor (1.4826 for normal data):

```{r robust_mad, fig.cap="MAD is robust to outliers whilst SD is sensitive"}
# Implement median absolute deviation from scratch
my_mad_median <- function(x, constant = 1.4826) {
    x <- x[!is.na(x)]
    med <- median(x)
    return(constant * median(abs(x - med)))
}

# Compare sensitivity to outliers
clean_data <- c(10, 11, 12, 12, 13, 13, 14, 15, 16)
contaminated <- c(10, 11, 12, 12, 13, 13, 14, 15, 100)  # One outlier

cat("Clean data:\n")
cat("  SD:", round(sd(clean_data), 2), "\n")
cat("  MAD:", round(my_mad_median(clean_data), 2), "\n\n")

cat("With outlier (100):\n")
cat("  SD:", round(sd(contaminated), 2), "(increased dramatically)\n")
cat("  MAD:", round(my_mad_median(contaminated), 2), "(barely changed)\n")

# Visualise comparison
set.seed(111)
normal_sample <- rnorm(100, mean = 50, sd = 10)
contaminated_sample <- c(rnorm(95, mean = 50, sd = 10),
                         rnorm(5, mean = 100, sd = 5))  # 5% outliers

comparison_data <- data.table$data.table(
    value = c(normal_sample, contaminated_sample),
    dataset = rep(c("Clean Data", "5% Contaminated"), each = 100)
)

stats_summary <- comparison_data[, .(
    SD = sd(value),
    MAD = mad(value)
), by = dataset]

print(stats_summary)

ggplot2$ggplot(comparison_data, ggplot2$aes(x = value, fill = dataset)) +
    ggplot2$geom_histogram(bins = 25, alpha = 0.7, colour = "white") +
    ggplot2$facet_wrap(~dataset, ncol = 1) +
    ggplot2$labs(
        title = "Robust vs Classical Measures of Spread",
        subtitle = "SD inflates with outliers; MAD remains stable",
        x = "Value",
        y = "Count"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

## 2.3 Measures of Position

Position measures tell us where individual observations stand relative to the distribution.

### 2.3.1 Percentiles and Quantiles

The **p-th percentile** is the value below which p% of the data fall.

**Formal definition:** For data sorted in ascending order, the p-th percentile $P_p$ satisfies:
- At least p% of observations are ≤ $P_p$
- At least (100-p)% of observations are ≥ $P_p$

**Quantiles** are the same concept on a 0-1 scale: the 0.25 quantile equals the 25th percentile.

```{r percentiles_from_scratch}
# Implement percentile function from scratch
# Using linear interpolation (Type 7 in R's quantile function)
my_percentile <- function(x, p) {
    x <- sort(x[!is.na(x)])
    n <- length(x)

    # Convert percentage to proportion if necessary
    if (p > 1) p <- p / 100

    # Calculate the index
    index <- 1 + (n - 1) * p

    # Get lower and upper indices
    lo <- floor(index)
    hi <- ceiling(index)

    # Interpolate
    if (lo == hi) {
        return(x[lo])
    } else {
        return(x[lo] + (index - lo) * (x[hi] - x[lo]))
    }
}

# Test with NHANES BMI
cat("Selected BMI percentiles:\n")
for (p in c(5, 10, 25, 50, 75, 90, 95)) {
    cat(p, "th percentile:", round(my_percentile(bmi_clean, p), 1), "\n")
}

cat("\nBuilt-in quantile() for comparison:\n")
print(quantile(bmi_clean, probs = c(0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95)))
```

### 2.3.2 Quartiles and the Five-Number Summary

Quartiles divide the data into four equal parts:

- Q1 (25th percentile): 25% of data below
- Q2 (50th percentile): median, 50% below
- Q3 (75th percentile): 75% below

The **five-number summary** provides a concise distribution overview:

$$\{\text{Min}, Q_1, \text{Median}, Q_3, \text{Max}\}$$

```{r five_number_summary, fig.cap="The five-number summary provides a concise distribution snapshot"}
# Implement five-number summary from scratch
my_fivenum <- function(x) {
    x <- x[!is.na(x)]
    x_sorted <- sort(x)

    c(
        Min = min(x),
        Q1 = my_percentile(x, 25),
        Median = median(x),
        Q3 = my_percentile(x, 75),
        Max = max(x)
    )
}

# Example with blood pressure
bp_fivenum <- my_fivenum(bp_data)
print(round(bp_fivenum, 1))

# Compare to built-in
cat("\nBuilt-in fivenum():\n")
print(round(fivenum(bp_data), 1))

# Visualise with boxplot
bp_dt <- data.table$data.table(bp = bp_data)

ggplot2$ggplot(bp_dt, ggplot2$aes(y = bp)) +
    ggplot2$geom_boxplot(fill = "#56B4E9", alpha = 0.7, width = 0.3) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Min"],
             label = paste("Min =", round(bp_fivenum["Min"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Q1"],
             label = paste("Q1 =", round(bp_fivenum["Q1"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Median"],
             label = paste("Median =", round(bp_fivenum["Median"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Q3"],
             label = paste("Q3 =", round(bp_fivenum["Q3"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Max"],
             label = paste("Max =", round(bp_fivenum["Max"], 0)), hjust = 0) +
    ggplot2$labs(
        title = "Five-Number Summary Visualised",
        subtitle = "Systolic blood pressure from NHANES",
        y = "Systolic Blood Pressure (mmHg)"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(axis.text.x = ggplot2$element_blank(),
          axis.title.x = ggplot2$element_blank())
```

### 2.3.3 Z-Scores (Standard Scores)

A **z-score** expresses how many standard deviations an observation is from the mean:

$$z = \frac{x - \bar{x}}{s}$$

Z-scores enable comparison across different variables and populations.

```{r zscore_from_scratch, fig.cap="Z-scores standardise data to a common scale"}
# Implement z-score from scratch
my_zscore <- function(x) {
    x_mean <- mean(x, na.rm = TRUE)
    x_sd <- sd(x, na.rm = TRUE)
    return((x - x_mean) / x_sd)
}

# Calculate z-scores for BMI
bmi_z <- my_zscore(bmi_clean)

cat("Original BMI: mean =", round(mean(bmi_clean), 1),
    ", SD =", round(sd(bmi_clean), 1), "\n")
cat("Z-scores: mean =", round(mean(bmi_z), 6),
    ", SD =", round(sd(bmi_z), 6), "\n")

# Interpretation examples
example_bmis <- c(18.5, 25, 30, 40)
example_z <- (example_bmis - mean(bmi_clean)) / sd(bmi_clean)

cat("\nZ-score interpretation:\n")
for (i in seq_along(example_bmis)) {
    cat("BMI =", example_bmis[i], "-> z =", round(example_z[i], 2))
    if (abs(example_z[i]) < 1) {
        cat(" (within 1 SD, typical)\n")
    } else if (abs(example_z[i]) < 2) {
        cat(" (1-2 SD from mean)\n")
    } else {
        cat(" (>2 SD from mean, unusual)\n")
    }
}

# Visualise
z_dt <- data.table$data.table(original = bmi_clean, zscore = bmi_z)

ggplot2$ggplot(z_dt, ggplot2$aes(x = zscore)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 40,
                   fill = "#E69F00", colour = "white", alpha = 0.8) +
    ggplot2$geom_density(colour = "#D55E00", size = 1) +
    ggplot2$geom_vline(xintercept = c(-2, -1, 0, 1, 2),
               linetype = c("dotted", "dashed", "solid", "dashed", "dotted"),
               colour = "blue") +
    ggplot2$labs(
        title = "Z-Score Distribution",
        subtitle = "BMI standardised; vertical lines at z = -2, -1, 0, 1, 2",
        x = "Z-Score",
        y = "Density"
    ) +
    ggplot2$theme_minimal()
```

### 2.3.4 Percentile Ranks

The **percentile rank** of a value tells us what percentage of the data fall at or below that value.

```{r percentile_rank}
# Implement percentile rank from scratch
my_percentile_rank <- function(x, value) {
    x <- x[!is.na(x)]
    # Proportion of values less than or equal to the given value
    return(mean(x <= value) * 100)
}

# Example: what percentile is a BMI of 25?
cat("BMI = 25 is at the", round(my_percentile_rank(bmi_clean, 25), 1),
    "percentile\n")
cat("BMI = 30 is at the", round(my_percentile_rank(bmi_clean, 30), 1),
    "percentile\n")
cat("BMI = 40 is at the", round(my_percentile_rank(bmi_clean, 40), 1),
    "percentile\n")

# Application: clinical reference ranges
# Often defined as 5th to 95th percentile
cat("\nReference range (5th-95th percentile) for BMI:\n")
cat(round(quantile(bmi_clean, 0.05), 1), "to",
    round(quantile(bmi_clean, 0.95), 1), "kg/m²\n")
```

## 2.4 Measures of Shape

Shape describes how values are distributed around the centre.

### 2.4.1 Skewness

**Skewness** measures asymmetry. A distribution is:
- **Positively skewed** (right-skewed): long tail to the right, mean > median
- **Negatively skewed** (left-skewed): long tail to the left, mean < median
- **Symmetric**: mean ≈ median

The sample skewness formula:

$$\text{Skewness} = \frac{n}{(n-1)(n-2)} \sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{s}\right)^3$$

```{r skewness_from_scratch, fig.cap="Skewness measures distributional asymmetry"}
# Implement skewness from scratch
my_skewness <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)
    sd_x <- sd(x)

    # Standardised values cubed
    z_cubed <- ((x - mean_x) / sd_x)^3

    # Adjustment for sample skewness
    adjustment <- n / ((n - 1) * (n - 2))

    return(adjustment * sum(z_cubed))
}

# Create datasets with different skewness
set.seed(222)

symmetric_data <- rnorm(1000, mean = 50, sd = 10)
right_skewed_data <- rgamma(1000, shape = 2, rate = 0.1)
left_skewed_data <- 100 - rgamma(1000, shape = 2, rate = 0.1)

cat("Symmetric data: skewness =", round(my_skewness(symmetric_data), 3), "\n")
cat("Right-skewed data: skewness =", round(my_skewness(right_skewed_data), 3), "\n")
cat("Left-skewed data: skewness =", round(my_skewness(left_skewed_data), 3), "\n")

# Real example: income is typically right-skewed
# Simulate income data
income_data <- rgamma(1000, shape = 2, rate = 0.00005)
cat("\nSimulated income: skewness =", round(my_skewness(income_data), 3), "\n")

# Visualise
skew_dt <- data.table$rbindlist(list(
    data.table$data.table(value = symmetric_data, type = "Symmetric (skew ≈ 0)"),
    data.table$data.table(value = right_skewed_data, type = "Right-Skewed (skew > 0)"),
    data.table$data.table(value = left_skewed_data, type = "Left-Skewed (skew < 0)")
))

skew_dt[, type := factor(type, levels = c("Left-Skewed (skew < 0)",
                                          "Symmetric (skew ≈ 0)",
                                          "Right-Skewed (skew > 0)"))]

ggplot2$ggplot(skew_dt, ggplot2$aes(x = value, fill = type)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 40,
                   colour = "white", alpha = 0.7) +
    ggplot2$geom_density(colour = "black", size = 1) +
    ggplot2$facet_wrap(~type, scales = "free", ncol = 1) +
    ggplot2$labs(
        title = "Skewness: Measuring Asymmetry",
        subtitle = "Positive skew: tail right; Negative skew: tail left; Zero: symmetric",
        x = "Value",
        y = "Density"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

### 2.4.2 Kurtosis

**Kurtosis** measures the "tailedness" of a distribution: how much probability is in the tails versus the centre.

- **Leptokurtic** (kurtosis > 3): heavier tails, more extreme values
- **Mesokurtic** (kurtosis ≈ 3): normal-like tails
- **Platykurtic** (kurtosis < 3): lighter tails, fewer extreme values

The sample kurtosis formula:

$$\text{Kurtosis} = \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{s}\right)^4 - \frac{3(n-1)^2}{(n-2)(n-3)}$$

Note: This gives **excess kurtosis** (normal = 0). Some formulas give raw kurtosis (normal = 3).

```{r kurtosis_from_scratch, fig.cap="Kurtosis measures tail heaviness relative to normal"}
# Implement excess kurtosis from scratch
my_kurtosis <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)
    sd_x <- sd(x)

    # Standardised values to the fourth power
    z_fourth <- ((x - mean_x) / sd_x)^4

    # Sample kurtosis with adjustment
    term1 <- (n * (n + 1)) / ((n - 1) * (n - 2) * (n - 3))
    term2 <- 3 * (n - 1)^2 / ((n - 2) * (n - 3))

    return(term1 * sum(z_fourth) - term2)
}

# Create datasets with different kurtosis
set.seed(333)

normal_data <- rnorm(1000)  # Kurtosis ≈ 0
heavy_tails <- rt(1000, df = 3)  # t-distribution: heavier tails
light_tails <- runif(1000, -2, 2)  # Uniform: lighter tails

cat("Normal data: excess kurtosis =", round(my_kurtosis(normal_data), 3), "\n")
cat("Heavy-tailed (t, df=3): excess kurtosis =", round(my_kurtosis(heavy_tails), 3), "\n")
cat("Light-tailed (uniform): excess kurtosis =", round(my_kurtosis(light_tails), 3), "\n")

# Visualise
kurt_dt <- data.table$rbindlist(list(
    data.table$data.table(value = normal_data, type = "Normal (mesokurtic)"),
    data.table$data.table(value = heavy_tails, type = "Heavy tails (leptokurtic)"),
    data.table$data.table(value = light_tails, type = "Light tails (platykurtic)")
))

# Truncate heavy tails for visualisation
kurt_dt[type == "Heavy tails (leptokurtic)" & abs(value) > 6, value := NA]

ggplot2$ggplot(kurt_dt[!is.na(value)], ggplot2$aes(x = value, fill = type)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 50,
                   colour = "white", alpha = 0.7) +
    ggplot2$geom_density(colour = "black", size = 1) +
    ggplot2$facet_wrap(~type, ncol = 1) +
    ggplot2$labs(
        title = "Kurtosis: Measuring Tail Heaviness",
        subtitle = "Leptokurtic: more outliers; Platykurtic: fewer outliers",
        x = "Value",
        y = "Density"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

### 2.4.3 Why Shape Matters

Distribution shape affects:

1. **Choice of summary statistics**: Use median/IQR for skewed data; mean/SD for symmetric
2. **Statistical tests**: Many tests assume normality; highly non-normal data may require non-parametric methods
3. **Interpretation**: A positively skewed income distribution with mean £50,000 does not mean most people earn £50,000

```{r shape_matters, fig.cap="Shape determines appropriate summary statistics"}
# Compare summaries for symmetric vs skewed data
set.seed(444)

symmetric <- rnorm(500, mean = 100, sd = 15)
skewed <- rgamma(500, shape = 4, rate = 0.04)  # Mean ≈ 100

compare_stats <- data.table$data.table(
    Statistic = c("Mean", "Median", "SD", "IQR", "Skewness"),
    Symmetric = c(mean(symmetric), median(symmetric), sd(symmetric),
                  IQR(symmetric), my_skewness(symmetric)),
    Skewed = c(mean(skewed), median(skewed), sd(skewed),
               IQR(skewed), my_skewness(skewed))
)

print(compare_stats[, lapply(.SD, round, 2), .SDcols = c("Symmetric", "Skewed"),
                    by = Statistic])

cat("\nFor the skewed distribution:\n")
cat("- Mean (", round(mean(skewed), 1), ") > Median (",
    round(median(skewed), 1), ")\n")
cat("- Mean is pulled by the long right tail\n")
cat("- Median is a better 'typical' value\n")
```

## 2.5 Summarising Grouped Data

Often we need to compute statistics for subgroups or summarise data that arrives pre-grouped.

### 2.5.1 Frequency Distributions

A **frequency distribution** shows how observations are distributed across categories or bins.

```{r frequency_distribution, fig.cap="Frequency distributions show how data are distributed across categories"}
# Create frequency distribution for BMI categories
bmi_categories <- cut(
    bmi_clean,
    breaks = c(0, 18.5, 25, 30, 35, 40, Inf),
    labels = c("Underweight", "Normal", "Overweight",
               "Obese I", "Obese II", "Obese III"),
    right = FALSE
)

# Frequency table
freq_table <- data.table$data.table(category = bmi_categories)[, .(
    frequency = .N
), by = category]

freq_table[, `:=`(
    relative_freq = frequency / sum(frequency),
    cumulative_freq = cumsum(frequency),
    cumulative_rel_freq = cumsum(frequency) / sum(frequency)
)]

# Order properly
freq_table <- freq_table[order(match(category,
    c("Underweight", "Normal", "Overweight", "Obese I", "Obese II", "Obese III")))]

print(freq_table)

# Visualise
freq_table[, category := factor(category, levels = c(
    "Underweight", "Normal", "Overweight", "Obese I", "Obese II", "Obese III"
))]

ggplot2$ggplot(freq_table, ggplot2$aes(x = category, y = frequency, fill = category)) +
    ggplot2$geom_bar(stat = "identity", alpha = 0.8) +
    ggplot2$geom_text(ggplot2$aes(label = paste0(round(relative_freq * 100, 1), "%")),
              vjust = -0.5) +
    ggplot2$labs(
        title = "BMI Category Distribution",
        subtitle = "Frequency distribution from NHANES data",
        x = "BMI Category",
        y = "Frequency"
    ) +
    ggplot2$scale_fill_brewer(palette = "RdYlGn", direction = -1) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

### 2.5.2 Computing Statistics from Grouped Data

When we only have grouped data (class intervals and frequencies), we can estimate statistics using class midpoints.

For grouped data with k classes, midpoints $m_i$, and frequencies $f_i$:

**Estimated Mean:**
$$\bar{x} \approx \frac{\sum_{i=1}^{k} f_i m_i}{\sum_{i=1}^{k} f_i}$$

**Estimated Variance:**
$$s^2 \approx \frac{\sum_{i=1}^{k} f_i (m_i - \bar{x})^2}{\sum_{i=1}^{k} f_i - 1}$$

```{r grouped_statistics}
# Create grouped data example
# Suppose we only have this summary table:
grouped_data <- data.table$data.table(
    lower = c(0, 18.5, 25, 30, 35, 40),
    upper = c(18.5, 25, 30, 35, 40, 60),
    frequency = c(156, 2812, 2989, 1923, 1074, 755)
)

grouped_data[, midpoint := (lower + upper) / 2]

# Estimate mean from grouped data
estimated_mean <- sum(grouped_data$frequency * grouped_data$midpoint) /
                  sum(grouped_data$frequency)

# Estimate variance from grouped data
n_total <- sum(grouped_data$frequency)
estimated_var <- sum(grouped_data$frequency *
                    (grouped_data$midpoint - estimated_mean)^2) / (n_total - 1)
estimated_sd <- sqrt(estimated_var)

cat("From grouped data:\n")
cat("  Estimated mean:", round(estimated_mean, 2), "\n")
cat("  Estimated SD:", round(estimated_sd, 2), "\n\n")

cat("From raw data (for comparison):\n")
cat("  Actual mean:", round(mean(bmi_clean), 2), "\n")
cat("  Actual SD:", round(sd(bmi_clean), 2), "\n")
```

### 2.5.3 Efficient Grouped Summaries with data.table

The data.table package excels at computing grouped statistics efficiently.

```{r grouped_summaries_datatable, fig.cap="Grouped statistics reveal patterns across subgroups"}
# Comprehensive grouped summary
grouped_summary <- nhanes[!is.na(BMI) & !is.na(Gender) & !is.na(AgeDecade), .(
    n = .N,
    mean_bmi = mean(BMI),
    sd_bmi = sd(BMI),
    median_bmi = median(BMI),
    iqr_bmi = IQR(BMI),
    min_bmi = min(BMI),
    max_bmi = max(BMI)
), by = .(Gender, AgeDecade)]

# Order by age decade
grouped_summary <- grouped_summary[order(AgeDecade, Gender)]

print(grouped_summary[, lapply(.SD, function(x) if(is.numeric(x)) round(x, 1) else x)])

# Visualise means by group
ggplot2$ggplot(grouped_summary, ggplot2$aes(x = AgeDecade, y = mean_bmi,
                                    fill = Gender, group = Gender)) +
    ggplot2$geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
    ggplot2$geom_errorbar(ggplot2$aes(ymin = mean_bmi - sd_bmi/sqrt(n),
                      ymax = mean_bmi + sd_bmi/sqrt(n)),
                  position = ggplot2$position_dodge(width = 0.9), width = 0.25) +
    ggplot2$labs(
        title = "Mean BMI by Age Decade and Gender",
        subtitle = "Error bars show ± 1 standard error",
        x = "Age Decade",
        y = "Mean BMI (kg/m²)"
    ) +
    ggplot2$scale_fill_manual(values = c("female" = "#CC79A7", "male" = "#0072B2")) +
    ggplot2$theme_minimal() +
    ggplot2$theme(axis.text.x = ggplot2$element_text(angle = 45, hjust = 1))
```

## 2.6 Detecting Outliers

Outliers are observations that lie far from the bulk of the data. They demand attention: they may indicate data errors, or they may be the most interesting observations.

### 2.6.1 What Is an Outlier?

An **outlier** is a data point that differs significantly from other observations. There is no single definition; context determines what counts as "significantly different."

Outliers matter because they can:
- Strongly influence mean and SD
- Distort correlation and regression
- Indicate data entry errors
- Reveal important edge cases or subpopulations

### 2.6.2 The IQR Rule (Tukey's Fences)

The most common outlier detection method uses the IQR:

- **Lower fence:** $Q_1 - 1.5 \times \text{IQR}$
- **Upper fence:** $Q_3 + 1.5 \times \text{IQR}$

Values outside these fences are potential outliers. Values beyond $Q_1 - 3 \times \text{IQR}$ or $Q_3 + 3 \times \text{IQR}$ are sometimes called "extreme outliers."

```{r iqr_outliers, fig.cap="Tukey's fences identify potential outliers using the IQR rule"}
# Implement IQR outlier detection from scratch
detect_outliers_iqr <- function(x, k = 1.5) {
    x <- x[!is.na(x)]
    q1 <- quantile(x, 0.25)
    q3 <- quantile(x, 0.75)
    iqr <- q3 - q1

    lower_fence <- q1 - k * iqr
    upper_fence <- q3 + k * iqr

    is_outlier <- x < lower_fence | x > upper_fence

    list(
        lower_fence = lower_fence,
        upper_fence = upper_fence,
        outliers = x[is_outlier],
        n_outliers = sum(is_outlier),
        proportion = mean(is_outlier)
    )
}

# Apply to blood pressure data
bp_outliers <- detect_outliers_iqr(bp_data)

cat("IQR outlier detection for systolic BP:\n")
cat("Lower fence:", round(bp_outliers$lower_fence, 1), "\n")
cat("Upper fence:", round(bp_outliers$upper_fence, 1), "\n")
cat("Number of outliers:", bp_outliers$n_outliers, "\n")
cat("Proportion:", round(bp_outliers$proportion * 100, 2), "%\n")

# Visualise with boxplot
bp_dt <- data.table$data.table(bp = bp_data)
bp_dt[, is_outlier := bp < bp_outliers$lower_fence | bp > bp_outliers$upper_fence]

ggplot2$ggplot(bp_dt, ggplot2$aes(y = bp)) +
    ggplot2$geom_boxplot(fill = "#56B4E9", alpha = 0.7, width = 0.4,
                 outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    ggplot2$geom_hline(yintercept = c(bp_outliers$lower_fence, bp_outliers$upper_fence),
               colour = "red", linetype = "dashed") +
    ggplot2$annotate("text", x = 0.3, y = bp_outliers$upper_fence + 5,
             label = paste("Upper fence:", round(bp_outliers$upper_fence, 0)),
             colour = "red") +
    ggplot2$annotate("text", x = 0.3, y = bp_outliers$lower_fence - 5,
             label = paste("Lower fence:", round(bp_outliers$lower_fence, 0)),
             colour = "red") +
    ggplot2$labs(
        title = "IQR Rule for Outlier Detection",
        subtitle = "Red dashed lines show Tukey's fences; red circles are outliers",
        y = "Systolic Blood Pressure (mmHg)"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(axis.text.x = ggplot2$element_blank(),
          axis.title.x = ggplot2$element_blank())
```

### 2.6.3 Z-Score Method

The z-score method flags observations more than k standard deviations from the mean:

$$|z| = \left|\frac{x - \bar{x}}{s}\right| > k$$

Common choices: k = 2 or k = 3.

```{r zscore_outliers}
# Implement z-score outlier detection
detect_outliers_zscore <- function(x, k = 3) {
    x <- x[!is.na(x)]
    z <- (x - mean(x)) / sd(x)

    is_outlier <- abs(z) > k

    list(
        threshold = k,
        outliers = x[is_outlier],
        z_scores = z[is_outlier],
        n_outliers = sum(is_outlier),
        proportion = mean(is_outlier)
    )
}

# Apply to blood pressure
bp_z_outliers <- detect_outliers_zscore(bp_data, k = 3)

cat("\nZ-score outlier detection (k = 3):\n")
cat("Number of outliers:", bp_z_outliers$n_outliers, "\n")
cat("Proportion:", round(bp_z_outliers$proportion * 100, 2), "%\n")

# Compare methods
cat("\nComparison of methods:\n")
cat("IQR rule:", bp_outliers$n_outliers, "outliers\n")
cat("Z-score (k=3):", bp_z_outliers$n_outliers, "outliers\n")
cat("Z-score (k=2):", detect_outliers_zscore(bp_data, k = 2)$n_outliers, "outliers\n")
```

**Limitation:** Both mean and SD are sensitive to outliers, so outliers can "mask" themselves.

### 2.6.4 Modified Z-Score Using MAD

A more robust approach uses median and MAD instead of mean and SD:

$$M = \frac{0.6745(x - \text{median})}{\text{MAD}}$$

Values with $|M| > 3.5$ are potential outliers.

```{r modified_zscore, fig.cap="Modified z-scores using MAD are robust to outlier masking"}
# Implement modified z-score
detect_outliers_mad <- function(x, k = 3.5) {
    x <- x[!is.na(x)]
    med <- median(x)
    mad_val <- mad(x)

    # Modified z-score
    m <- 0.6745 * (x - med) / mad_val

    is_outlier <- abs(m) > k

    list(
        threshold = k,
        outliers = x[is_outlier],
        modified_z = m[is_outlier],
        n_outliers = sum(is_outlier),
        proportion = mean(is_outlier)
    )
}

# Apply to blood pressure
bp_mad_outliers <- detect_outliers_mad(bp_data)

cat("Modified z-score outlier detection:\n")
cat("Number of outliers:", bp_mad_outliers$n_outliers, "\n")
cat("Proportion:", round(bp_mad_outliers$proportion * 100, 2), "%\n")

# Demonstrate robustness
set.seed(555)
clean <- rnorm(100, mean = 50, sd = 5)
contaminated <- c(clean, 150, 160, 170)  # Add three extreme outliers

cat("\nDemonstrating masking effect:\n")
cat("Clean data: z-score detects",
    detect_outliers_zscore(clean, k = 3)$n_outliers, "outliers\n")
cat("With 3 extreme outliers added:\n")
cat("  Z-score method detects",
    detect_outliers_zscore(contaminated, k = 3)$n_outliers, "outliers\n")
cat("  Modified z-score detects",
    detect_outliers_mad(contaminated)$n_outliers, "outliers\n")
cat("\n(Z-score method missed some because outliers inflated the SD)\n")
```

### 2.6.5 Outliers in Biomedical Data

In biomedical contexts, outliers require careful consideration:

**Potential causes:**
- Data entry errors (fix or exclude)
- Equipment malfunction (exclude)
- Non-compliance (document, may need to keep)
- True biological extremes (keep, they are real data)
- Different population (investigate)

```{r outliers_decision, fig.cap="Decision framework for handling outliers in biomedical data"}
# Create summary of outlier handling approaches
outlier_decisions <- data.table$data.table(
    Cause = c("Data entry error", "Equipment failure",
              "Non-compliance", "Biological extreme", "Different population"),
    Action = c("Correct if possible, else exclude",
               "Exclude with documentation",
               "Keep but document; sensitivity analysis",
               "Keep (it's real data)",
               "Investigate; may need separate analysis"),
    Example = c("Age = 999 years", "BP = 0 mmHg",
                "Patient skipped doses", "BMI = 55 kg/m²",
                "Pediatric patient in adult study")
)

print(outlier_decisions)

# Example: Investigating an outlier
# Find the highest BMI values
extreme_bmi <- nhanes[!is.na(BMI), .(BMI, Age, Gender, Diabetes, BPSysAve)]
extreme_bmi <- extreme_bmi[order(-BMI)][1:10]

cat("\nTop 10 highest BMI values in NHANES:\n")
print(extreme_bmi)

cat("\nThese are real people with extreme obesity, not errors.\n")
cat("We should NOT automatically exclude them.\n")
```

---

## Communicating to Stakeholders

When explaining descriptive statistics to collaborators or non-technical audiences:

**On central tendency:**
> "The average (mean) gives us the typical value, but it can be misleading if the data are skewed. The median tells us the middle value—half are above, half below. For income data, we often report the median because a few very high earners can inflate the average."

**On spread:**
> "The standard deviation tells us how much values typically differ from the average. For blood pressure, an SD of 15 means most readings fall within about 15 mmHg above or below the average."

**On outliers:**
> "We identified some unusually high values. Before deciding what to do with them, we need to investigate: are these data errors, or are they genuine extreme cases? If they're real, they may be the most important observations in our study."

**On appropriate measures:**
> "Because this distribution is skewed with a long tail to the right, the median is a more representative 'typical' value than the mean. The mean is pulled toward the tail."

---

## Quick Reference

### Summary Statistics Formulae

| Measure | Formula | R Function |
|---------|---------|------------|
| Mean | $\bar{x} = \frac{1}{n}\sum x_i$ | `mean(x)` |
| Median | Middle value | `median(x)` |
| Variance | $s^2 = \frac{1}{n-1}\sum(x_i - \bar{x})^2$ | `var(x)` |
| SD | $s = \sqrt{s^2}$ | `sd(x)` |
| IQR | $Q_3 - Q_1$ | `IQR(x)` |
| CV | $\frac{s}{\bar{x}} \times 100\%$ | `sd(x)/mean(x)*100` |
| Skewness | Third standardised moment | `e1071::skewness(x)` |
| Kurtosis | Fourth standardised moment | `e1071::kurtosis(x)` |

### When to Use Each Measure

| Data Characteristic | Central Tendency | Spread |
|--------------------|------------------|--------|
| Symmetric, no outliers | Mean | SD |
| Skewed | Median | IQR |
| Outliers present | Median | IQR or MAD |
| Comparing across scales | Mean (or median) | CV |
| Categorical data | Mode | — |

### Outlier Detection Summary

| Method | Outlier If | Best When |
|--------|-----------|-----------|
| IQR rule | $x < Q_1 - 1.5 \times \text{IQR}$ or $x > Q_3 + 1.5 \times \text{IQR}$ | General use |
| Z-score | $\|z\| > 3$ | Data approximately normal |
| Modified z-score | $\|M\| > 3.5$ | Outliers may mask each other |

### Five-Number Summary

$$\{\text{Min}, Q_1, \text{Median}, Q_3, \text{Max}\}$$

R: `fivenum(x)` or `summary(x)`

### Grouped Statistics with data.table

```r
# Basic grouped summary
data[, .(
    n = .N,
    mean = mean(variable),
    sd = sd(variable),
    median = median(variable),
    iqr = IQR(variable)
), by = group_variable]
```

### Greek Letter Reference (Descriptive Statistics)

| Symbol | Name | Represents |
|--------|------|------------|
| $\mu$ | mu | Population mean |
| $\sigma$ | sigma | Population standard deviation |
| $\sigma^2$ | sigma squared | Population variance |
| $\bar{x}$ | x-bar | Sample mean |
| $s$ | s | Sample standard deviation |
| $s^2$ | s squared | Sample variance |
