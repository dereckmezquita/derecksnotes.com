---
title: "Statistics with R I: Foundations"
chapter: "Chapter 1: Introduction to Statistics and Data"
coverImage: 13
author: "Dereck Mezquita"
date: 2025-01-14
tags: [statistics, mathematics, probability, data, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Chapter 1: Introduction to Statistics and Data

This foundational chapter establishes the vocabulary and conceptual framework for statistical thinking. We explore what statistics is, why it matters, and how to think critically about data. By the end of this chapter, you will understand the fundamental distinctions that underpin all statistical work: populations versus samples, parameters versus statistics, and the various types of variables we encounter in biomedical research.

```{r packages, message=FALSE, warning=FALSE}
box::use(
    data.table,
    ggplot2
)
```

## 1.1 What Is Statistics?

Statistics is the science of learning from data under uncertainty. At its core, statistics provides a rigorous framework for collecting, organising, analysing, and interpreting information to make informed decisions when we cannot observe everything directly. Unlike deterministic mathematics where equations yield exact answers, statistics acknowledges that real-world data contain variability, and our conclusions must account for this inherent uncertainty.

### 1.1.1 Statistics as the Science of Learning from Data

Consider a physician who wishes to know whether a new drug reduces blood pressure more effectively than the current standard treatment. She cannot administer the drug to every person in the world who might benefit from it. Instead, she studies a carefully selected group of patients, measures their responses, and uses statistical methods to draw conclusions about how the drug would perform in the broader population.

This scenario captures the essence of statistics: we have limited data but wish to make general statements. Statistics provides the mathematical machinery to do this responsibly—quantifying how confident we should be in our conclusions and identifying what remains uncertain.

```{r statistics_essence, fig.cap="Statistics bridges the gap between sample observations and population truth"}
set.seed(42)

# Simulate a population and sample
population_size <- 10000
sample_size <- 100

# True population parameters
true_mean <- 120
true_sd <- 15

# Generate "population" of blood pressure readings
population <- rnorm(population_size, mean = true_mean, sd = true_sd)

# Take a random sample
sample_indices <- sample(1:population_size, sample_size)
sample_data <- population[sample_indices]

# Create visualisation data
plot_data <- data.table$data.table(
    value = c(population, sample_data),
    type = c(
        rep("Population (N = 10,000)", population_size),
        rep("Sample (n = 100)", sample_size)
    )
)

ggplot2$ggplot(plot_data, ggplot2$aes(x = value, fill = type)) +
    ggplot2$geom_density(alpha = 0.6) +
    ggplot2$geom_vline(xintercept = true_mean, linetype = "dashed", colour = "red", size = 1) +
    ggplot2$geom_vline(xintercept = mean(sample_data), linetype = "dotted", colour = "blue", size = 1) +
    ggplot2$labs(
        title = "The Statistical Problem: Learning About Populations from Samples",
        subtitle = paste0(
            "Red dashed line: true population mean (μ = ", true_mean, "); ",
            "Blue dotted line: sample mean (x̄ = ", round(mean(sample_data), 1), ")"
        ),
        x = "Systolic Blood Pressure (mmHg)",
        y = "Density",
        fill = "Data Source"
    ) +
    ggplot2$scale_fill_manual(values = c("Population (N = 10,000)" = "#E69F00", "Sample (n = 100)" = "#56B4E9")) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "bottom")
```

The figure above illustrates the fundamental statistical problem. We observe only the sample (blue), but we wish to learn about the population (orange). The sample mean differs slightly from the population mean—this difference is not an error but a natural consequence of sampling variability. Statistics tells us how to quantify this variability and how confident we can be that our sample reflects the population.

### 1.1.2 A Brief History: From Census to Modern Data Science

Statistical thinking has ancient roots. The word "statistics" derives from the Latin *status*, meaning "state"—reflecting its origins in governmental record-keeping. Ancient civilisations conducted censuses to count populations and levy taxes. The Babylonians collected agricultural data around 3800 BCE, and the Egyptians surveyed their population for pyramid construction.

The mathematical foundations of statistics emerged in the 17th and 18th centuries. Blaise Pascal and Pierre de Fermat developed probability theory whilst corresponding about gambling problems. Jacob Bernoulli proved the law of large numbers, showing that sample proportions converge to true probabilities as sample sizes increase. Thomas Bayes formulated his famous theorem on conditional probability, though it was published posthumously.

The 19th century brought statistics into scientific practice. Adolphe Quetelet applied statistical methods to human characteristics, discovering that many biological measurements follow the normal distribution. Francis Galton developed correlation and regression whilst studying heredity. Karl Pearson formalised these methods and founded the world's first university statistics department at University College London in 1911.

The 20th century saw an explosion of statistical methodology. Ronald Fisher revolutionised experimental design and developed methods still central to research today: analysis of variance, maximum likelihood estimation, and the concepts underlying randomised controlled trials. Jerzy Neyman and Egon Pearson formalised hypothesis testing and confidence intervals. The second half of the century brought computational statistics—methods that would have been impossible without computers.

Today, statistics underpins nearly all empirical research. From genomics to clinical trials, from economics to machine learning, statistical thinking provides the framework for evidence-based conclusions.

### 1.1.3 Descriptive vs Inferential Statistics

Statistics divides into two complementary branches:

**Descriptive statistics** summarises and organises data we have observed. When we calculate an average, create a histogram, or report that 60% of patients responded to treatment, we are describing our data. Descriptive statistics makes no claims beyond the data at hand—it simply characterises what we have observed.

**Inferential statistics** uses sample data to draw conclusions about larger populations. When we calculate a confidence interval, test a hypothesis, or make a prediction, we are using our limited sample to make statements about the broader world. Inference requires probability theory to quantify uncertainty.

```{r descriptive_vs_inferential, fig.cap="Descriptive statistics summarises data; inferential statistics generalises to populations"}
set.seed(123)

# Generate clinical trial data
n_treatment <- 50
n_control <- 50

treatment_response <- rbinom(n_treatment, 1, prob = 0.65)
control_response <- rbinom(n_control, 1, prob = 0.45)

trial_data <- data.table$data.table(
    group = c(rep("Treatment", n_treatment), rep("Control", n_control)),
    response = c(treatment_response, control_response)
)

# Descriptive statistics
descriptive_summary <- trial_data[, .(
    n = .N,
    responders = sum(response),
    proportion = mean(response)
), by = group]

# Inferential statistics (confidence intervals)
# Wilson score interval for proportions
wilson_ci <- function(x, n, conf_level = 0.95) {
    p_hat <- x / n
    z <- qnorm(1 - (1 - conf_level) / 2)

    denom <- 1 + z^2 / n
    centre <- (p_hat + z^2 / (2 * n)) / denom
    margin <- z * sqrt(p_hat * (1 - p_hat) / n + z^2 / (4 * n^2)) / denom

    c(lower = centre - margin, upper = centre + margin)
}

descriptive_summary[, c("ci_lower", "ci_upper") := {
    ci <- wilson_ci(responders, n)
    list(ci[1], ci[2])
}, by = group]

print(descriptive_summary)

# Visualise
ggplot(descriptive_summary, aes(x = group, y = proportion, fill = group)) +
    geom_bar(stat = "identity", width = 0.6, alpha = 0.8) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, size = 1) +
    geom_text(aes(label = paste0(round(proportion * 100, 1), "%")),
              vjust = -0.5, size = 5) +
    labs(
        title = "Clinical Trial Results: Treatment vs Control",
        subtitle = "Error bars show 95% confidence intervals (inferential statistics)",
        x = "Group",
        y = "Response Proportion",
        caption = "Bar heights are descriptive; error bars are inferential"
    ) +
    scale_fill_manual(values = c("Control" = "#999999", "Treatment" = "#0072B2")) +
    theme_minimal() +
    theme(legend.position = "none")
```

In the figure above, the bar heights represent descriptive statistics—they simply report what we observed in our sample. The error bars represent inferential statistics—they tell us the range of plausible values for the true population proportion, accounting for sampling variability.

### 1.1.4 The Role of Statistics in Biomedical Research

Statistics is the backbone of modern biomedical research. Every clinical trial, every epidemiological study, every genomic analysis relies on statistical methods to separate signal from noise and draw valid conclusions.

In **clinical trials**, statistics enables us to determine whether treatments work, how well they work, and in whom they work best. The randomised controlled trial—widely considered the gold standard for establishing causation—is a statistical invention. Statistical methods determine sample sizes, randomisation schemes, and the criteria for concluding that a treatment is effective.

In **epidemiology**, statistics helps us identify risk factors for disease and quantify how strongly they associate with outcomes. Concepts like relative risk, odds ratios, and attributable risk are statistical measures that guide public health decisions.

In **genomics and bioinformatics**, statistics addresses the "curse of dimensionality"—the challenge of analysing thousands or millions of variables (genes) when we have relatively few samples (patients). Multiple testing corrections, false discovery rates, and machine learning methods are statistical tools essential for high-dimensional biology.

In **diagnostic medicine**, statistics underpins our understanding of test accuracy. Sensitivity, specificity, positive predictive value, and negative predictive value are all statistical concepts that help clinicians interpret test results and communicate risk to patients.

## 1.2 Populations and Samples

The distinction between populations and samples is the foundation upon which all of statistical inference rests. Understanding this distinction—and the notation that accompanies it—is essential before proceeding further.

### 1.2.1 Defining the Population of Interest

A **population** is the complete collection of individuals or objects about which we wish to draw conclusions. Defining the population precisely is a critical first step that researchers often overlook.

Consider a researcher studying hypertension. She might define her population as "all adults in the United Kingdom with systolic blood pressure exceeding 140 mmHg." This definition specifies who is included (adults), where they are located (UK), and what characteristic qualifies them (hypertension). The population is finite but very large—millions of people.

Alternatively, the population might be conceptual. A researcher studying a new drug might define the population as "all future patients who might receive this treatment." This population is infinite and hypothetical—it includes people who have not yet been born.

The population definition determines to whom our conclusions apply. If we study only patients at a single hospital in London, our conclusions may not generalise to rural populations or to other countries. This limitation is not a failure of statistics but a reminder that careful population definition is essential.

### 1.2.2 Why We Sample: Practical and Theoretical Reasons

In nearly all situations, studying the entire population is impractical or impossible:

**Cost and time**: A census of all UK hypertensive adults would require examining millions of people—prohibitively expensive and time-consuming for most research purposes.

**Destructive testing**: In quality control, testing whether a light bulb works requires turning it on until it burns out. Testing every bulb would leave none to sell.

**Infinite populations**: When the population is conceptual (future patients), we cannot possibly observe all members.

**Ethical constraints**: In clinical trials, we cannot expose everyone to an experimental treatment. We study samples to determine whether broader use is warranted.

The remarkable fact—proven mathematically through probability theory—is that carefully selected samples can tell us a great deal about populations. A well-designed survey of 1,000 people can accurately estimate the opinions of millions.

### 1.2.3 Parameters vs Statistics: The Fundamental Distinction

A **parameter** is a numerical characteristic of a population. Parameters are typically fixed but unknown—they are the quantities we wish to learn about.

A **statistic** is a numerical characteristic of a sample. Statistics are calculated from our data and are therefore known, but they vary from sample to sample.

This distinction is so fundamental that we use different notation:

| Quantity | Parameter (Population) | Statistic (Sample) |
|----------|------------------------|-------------------|
| Mean | μ (mu) | x̄ (x-bar) |
| Standard deviation | σ (sigma) | s |
| Proportion | π (pi) or p | p̂ (p-hat) |
| Correlation | ρ (rho) | r |
| Regression coefficient | β (beta) | b or β̂ (beta-hat) |
| Size | N | n |

Greek letters typically denote parameters; Roman letters denote statistics. The "hat" symbol (^) indicates an estimator or estimate of a parameter.

```{r parameter_vs_statistic, fig.cap="Statistics vary from sample to sample; parameters are fixed"}
set.seed(456)

# True population parameter
mu <- 100
sigma <- 15

# Draw many samples and calculate sample means
n_samples <- 1000
sample_size <- 30

sample_means <- replicate(n_samples, {
    sample_data <- rnorm(sample_size, mean = mu, sd = sigma)
    mean(sample_data)
})

# Create histogram of sample means
sampling_dist <- data.table(sample_mean = sample_means)

ggplot(sampling_dist, aes(x = sample_mean)) +
    geom_histogram(aes(y = after_stat(density)), bins = 40,
                   fill = "#56B4E9", colour = "white", alpha = 0.8) +
    geom_density(colour = "#0072B2", size = 1.2) +
    geom_vline(xintercept = mu, colour = "red", linetype = "dashed", size = 1.2) +
    labs(
        title = "Statistics Vary; Parameters Are Fixed",
        subtitle = paste0(
            "Each sample of n = ", sample_size, " yields a different x̄. ",
            "The true μ = ", mu, " is fixed (red line)."
        ),
        x = "Sample Mean (x̄)",
        y = "Density",
        caption = paste0("Distribution of ", n_samples, " sample means")
    ) +
    theme_minimal()
```

### 1.2.4 The Goal of Inference: Learning About Populations from Samples

The central problem of statistics is this: we observe a sample and wish to make statements about the population. We calculate statistics and wish to learn about parameters.

This presents two challenges:

1. **Point estimation**: What single value best represents the unknown parameter? If our sample mean is x̄ = 98.5, is this our best guess for μ?

2. **Uncertainty quantification**: How confident should we be in this estimate? Could μ plausibly be 95? 102? 150?

Statistical inference provides formal methods for both tasks. Point estimators give us single "best guesses" for parameters, and interval estimators (confidence intervals) quantify plausible ranges. Hypothesis tests assess whether data are compatible with specific hypotheses about parameters.

### 1.2.5 Notation Conventions

Throughout this course, we adopt standard notation conventions:

- **Random variables** (before data are observed) use capital letters: X, Y, Z
- **Observed values** (after data collection) use lowercase letters: x, y, z
- **Sample size** is denoted n (or n₁, n₂ for different groups)
- **Population size** is denoted N
- **Indices** typically use subscripts: xᵢ denotes the i-th observation
- **Summation** uses sigma notation: $\sum_{i=1}^{n} x_i$ means add all observations

When we write X ~ N(μ, σ²), we mean "X follows a normal distribution with mean μ and variance σ²." The tilde (~) indicates "is distributed as."

## 1.3 Types of Variables

Variables are characteristics that vary among individuals in a study. Understanding variable types is essential because the appropriate analysis depends critically on what type of data we have.

### 1.3.1 Quantitative Variables

**Quantitative variables** represent numerical measurements or counts. They answer "how much?" or "how many?"

Quantitative variables subdivide into two types:

**Continuous variables** can take any value within a range. Height, weight, blood pressure, and temperature are continuous—between any two values, infinitely many other values are possible. A patient's weight might be 72.3 kg, 72.31 kg, or 72.314159 kg, limited only by measurement precision.

**Discrete variables** can take only specific values, typically integers. Number of children, number of hospitalisations, and number of mutations are discrete—you cannot have 2.7 children. Some discrete variables can take many values (age in years ranges from 0 to ~120), whilst others have few (number of arms is typically 0, 1, or 2).

```{r continuous_vs_discrete, fig.cap="Continuous variables take any value; discrete variables take only specific values"}
set.seed(789)

# Generate example data
n <- 200

# Continuous: body temperature in Celsius
temperature <- rnorm(n, mean = 36.8, sd = 0.4)

# Discrete: number of GP visits in past year
gp_visits <- rpois(n, lambda = 3)

# Create plot data
continuous_data <- data.table(value = temperature, type = "Continuous (Body Temperature)")
discrete_data <- data.table(value = gp_visits, type = "Discrete (GP Visits/Year)")

# Plot continuous
p1 <- ggplot(continuous_data, aes(x = value)) +
    geom_histogram(aes(y = after_stat(density)), bins = 30,
                   fill = "#E69F00", colour = "white", alpha = 0.8) +
    geom_density(colour = "#D55E00", size = 1) +
    labs(
        title = "Continuous Variable",
        subtitle = "Body temperature (°C)",
        x = "Temperature",
        y = "Density"
    ) +
    theme_minimal()

# Plot discrete
p2 <- ggplot(discrete_data, aes(x = value)) +
    geom_bar(fill = "#56B4E9", colour = "white", alpha = 0.8) +
    labs(
        title = "Discrete Variable",
        subtitle = "GP visits in past year",
        x = "Number of Visits",
        y = "Count"
    ) +
    theme_minimal()

# Use patchwork or gridExtra would be ideal, but keeping simple for now
# We'll create a combined view
combined_data <- rbind(
    data.table(value = scale(temperature)[, 1], type = "Continuous: Temperature"),
    data.table(value = scale(gp_visits)[, 1], type = "Discrete: GP Visits")
)

ggplot(combined_data, aes(x = value, fill = type)) +
    geom_histogram(aes(y = after_stat(density)), bins = 25,
                   position = "identity", alpha = 0.6, colour = "white") +
    facet_wrap(~type, scales = "free") +
    labs(
        title = "Continuous vs Discrete Variables",
        subtitle = "Continuous variables have smooth distributions; discrete variables have gaps",
        x = "Standardised Value",
        y = "Density"
    ) +
    scale_fill_manual(values = c("#E69F00", "#56B4E9")) +
    theme_minimal() +
    theme(legend.position = "none")
```

### 1.3.2 Qualitative (Categorical) Variables

**Qualitative variables** (also called categorical variables) represent categories or groups rather than numerical quantities. They answer "what type?" or "which category?"

Categorical variables subdivide into two types:

**Nominal variables** have categories with no natural ordering. Blood type (A, B, AB, O), sex (male, female), and eye colour (brown, blue, green) are nominal. We cannot say that blood type A is "more than" or "less than" blood type B—they are simply different.

**Ordinal variables** have categories with a meaningful order, but the distances between categories are not necessarily equal. Disease severity (mild, moderate, severe), education level (primary, secondary, tertiary), and pain rating scales (1 to 10) are ordinal. We know that "severe" is worse than "moderate," but we cannot say it is exactly twice as bad.

```{r categorical_variables, fig.cap="Nominal variables have no order; ordinal variables have meaningful order"}
# Create example categorical data
categorical_examples <- data.table(
    variable = c(
        rep("Blood Type (Nominal)", 4),
        rep("Disease Severity (Ordinal)", 3)
    ),
    category = c(
        "A", "B", "AB", "O",
        "Mild", "Moderate", "Severe"
    ),
    count = c(
        42, 10, 4, 44,  # Blood type frequencies
        35, 45, 20      # Severity frequencies
    )
)

# Ensure proper ordering for ordinal
categorical_examples[variable == "Disease Severity (Ordinal)",
                     category := factor(category, levels = c("Mild", "Moderate", "Severe"))]

ggplot(categorical_examples, aes(x = category, y = count, fill = variable)) +
    geom_bar(stat = "identity", alpha = 0.8) +
    facet_wrap(~variable, scales = "free_x") +
    labs(
        title = "Nominal vs Ordinal Categorical Variables",
        subtitle = "Blood type categories have no order; severity has a meaningful order",
        x = "Category",
        y = "Count"
    ) +
    scale_fill_manual(values = c("#CC79A7", "#009E73")) +
    theme_minimal() +
    theme(legend.position = "none")
```

### 1.3.3 Scales of Measurement: Nominal, Ordinal, Interval, Ratio

In the 1940s, psychologist Stanley Smith Stevens proposed four scales of measurement, distinguished by the mathematical operations that are meaningful for each:

**Nominal scale**: Categories are labels with no order. Only equality comparisons are meaningful. (Is blood type A equal to blood type B? No.) Mathematical operations like addition or averaging are meaningless.

**Ordinal scale**: Categories have a meaningful order, but differences between categories are not necessarily equal. Rankings and order statistics are meaningful. (Is severe worse than moderate? Yes. Is the difference between mild and moderate equal to the difference between moderate and severe? We cannot say.)

**Interval scale**: Differences between values are meaningful and equal, but there is no true zero point. Temperature in Celsius or Fahrenheit is interval—the difference between 20°C and 30°C equals the difference between 30°C and 40°C, but 0°C does not mean "no temperature." Ratios are not meaningful (40°C is not "twice as hot" as 20°C).

**Ratio scale**: Has all properties of interval scale plus a meaningful zero point. Height, weight, blood pressure, and temperature in Kelvin are ratio—zero means absence of the quantity, and ratios are meaningful (40 kg is twice as heavy as 20 kg).

```{r scales_of_measurement, fig.cap="Four scales of measurement with progressively more mathematical operations"}
scales_data <- data.table(
    scale = factor(
        c("Nominal", "Ordinal", "Interval", "Ratio"),
        levels = c("Nominal", "Ordinal", "Interval", "Ratio")
    ),
    example = c("Blood type", "Pain rating", "Temperature (°C)", "Weight (kg)"),
    equality = c("✓", "✓", "✓", "✓"),
    order = c("✗", "✓", "✓", "✓"),
    difference = c("✗", "✗", "✓", "✓"),
    ratio = c("✗", "✗", "✗", "✓")
)

print(scales_data)

# Visual representation
scales_visual <- data.table(
    scale = rep(c("Nominal", "Ordinal", "Interval", "Ratio"), each = 4),
    operation = rep(c("Equality (=)", "Order (<, >)", "Difference (+, -)", "Ratio (×, ÷)"), 4),
    meaningful = c(
        TRUE, FALSE, FALSE, FALSE,  # Nominal
        TRUE, TRUE, FALSE, FALSE,   # Ordinal
        TRUE, TRUE, TRUE, FALSE,    # Interval
        TRUE, TRUE, TRUE, TRUE      # Ratio
    )
)

scales_visual[, scale := factor(scale, levels = c("Ratio", "Interval", "Ordinal", "Nominal"))]
scales_visual[, operation := factor(operation, levels = c("Equality (=)", "Order (<, >)",
                                                          "Difference (+, -)", "Ratio (×, ÷)"))]

ggplot(scales_visual, aes(x = operation, y = scale, fill = meaningful)) +
    geom_tile(colour = "white", size = 1) +
    geom_text(aes(label = ifelse(meaningful, "✓", "✗")), size = 6) +
    labs(
        title = "Stevens' Scales of Measurement",
        subtitle = "Each higher scale permits additional mathematical operations",
        x = "Mathematical Operation",
        y = "Scale Type"
    ) +
    scale_fill_manual(values = c("TRUE" = "#009E73", "FALSE" = "#D55E00"), guide = "none") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 1.3.4 Identifying Variable Types in Practice

Let us examine real variables from the NHANES dataset:

```{r variable_classification}
# Load NHANES data (assuming it's available from our downloaded CSVs)
# For this example, we'll work with the structure
nhanes_vars <- data.table(
    variable = c("Age", "Gender", "Race1", "Education", "BMI",
                 "SmokeNow", "Diabetes", "BPSysAve", "DiabetesAge",
                 "nPregnancies"),
    description = c(
        "Age in years", "Male/Female", "Race/ethnicity",
        "Educational attainment", "Body mass index",
        "Currently smoking?", "Diabetes status",
        "Systolic blood pressure", "Age at diabetes diagnosis",
        "Number of pregnancies"
    ),
    type = c(
        "Quantitative (discrete)", "Qualitative (nominal)", "Qualitative (nominal)",
        "Qualitative (ordinal)", "Quantitative (continuous)",
        "Qualitative (nominal)", "Qualitative (nominal)",
        "Quantitative (continuous)", "Quantitative (discrete)",
        "Quantitative (discrete)"
    ),
    scale = c(
        "Ratio", "Nominal", "Nominal", "Ordinal", "Ratio",
        "Nominal", "Nominal", "Ratio", "Ratio", "Ratio"
    )
)

print(nhanes_vars)
```

**Key considerations when classifying variables:**

1. **Context matters**: Age in years is discrete, but age measured precisely would be continuous. In practice, we often treat age as continuous because it can take many values.

2. **Coding is not meaning**: A variable coded as 1, 2, 3, 4 might be nominal (disease type), ordinal (severity), or continuous (actual count). The numbers themselves do not determine the variable type.

3. **Derived variables**: BMI is continuous even though it is calculated from height and weight. The calculation does not change its nature.

## 1.4 Data Collection Methods

How data are collected determines what conclusions we can draw. The distinction between observational studies and experiments is fundamental to understanding the limits of statistical inference.

### 1.4.1 Observational Studies

In an **observational study**, researchers observe individuals without intervening or manipulating anything. They record characteristics and outcomes as they naturally occur.

**Cross-sectional studies** collect data at a single point in time. A survey of blood pressure levels across the UK population is cross-sectional. These studies are efficient and provide snapshots of prevalence, but they cannot establish temporal relationships—we cannot tell whether exposure preceded outcome.

**Case-control studies** start with outcomes (cases with disease, controls without) and look backward to identify exposures. If we recruit lung cancer patients (cases) and healthy controls, then compare their smoking histories, we have a case-control study. These studies are efficient for rare diseases but are susceptible to recall bias—people with disease may remember exposures differently.

**Cohort studies** follow a group of individuals over time, measuring exposures at baseline and observing who develops outcomes. The Framingham Heart Study followed thousands of residents, recording cardiovascular risk factors and tracking heart disease incidence for decades. Cohort studies establish temporal sequence but are expensive, time-consuming, and susceptible to loss to follow-up.

```{r study_designs, fig.cap="Three types of observational study designs"}
design_data <- data.table(
    design = c("Cross-sectional", "Case-control", "Cohort"),
    time_direction = c("Snapshot", "Backward", "Forward"),
    starts_with = c("Population sample", "Outcome (cases)", "Exposure (cohort)"),
    strength = c("Efficient, prevalence", "Rare diseases", "Temporal sequence"),
    limitation = c("No temporal order", "Recall bias", "Expensive, attrition")
)

print(design_data)
```

### 1.4.2 Experimental Studies

In an **experimental study**, researchers actively intervene—they assign subjects to different conditions and observe outcomes. The defining feature is that the researcher controls who receives which treatment.

The **randomised controlled trial (RCT)** is the gold standard experimental design. Participants are randomly assigned to treatment or control groups, interventions are administered, and outcomes are compared. Random assignment ensures that, on average, groups are comparable in all respects except the treatment received.

```{r rct_simulation, fig.cap="Randomisation creates comparable groups on both observed and unobserved characteristics"}
set.seed(101)

# Simulate an RCT
n_total <- 200

# Create baseline characteristics
rct_data <- data.table(
    id = 1:n_total,
    age = round(rnorm(n_total, mean = 55, sd = 10)),
    baseline_bp = round(rnorm(n_total, mean = 145, sd = 12)),
    smoker = rbinom(n_total, 1, prob = 0.25)
)

# Randomise to treatment
rct_data[, treatment := sample(c("Treatment", "Control"), n_total, replace = TRUE)]

# True treatment effect
treatment_effect <- -10  # Treatment reduces BP by 10 mmHg on average

# Simulate outcome
rct_data[, outcome_bp := baseline_bp +
             ifelse(treatment == "Treatment", treatment_effect, 0) +
             rnorm(n_total, mean = 0, sd = 8)]

# Check balance
balance_check <- rct_data[, .(
    mean_age = mean(age),
    mean_baseline_bp = mean(baseline_bp),
    prop_smoker = mean(smoker),
    mean_outcome_bp = mean(outcome_bp),
    n = .N
), by = treatment]

print("Balance check after randomisation:")
print(balance_check)

# Visualise outcomes
ggplot(rct_data, aes(x = treatment, y = outcome_bp, fill = treatment)) +
    geom_boxplot(alpha = 0.7) +
    geom_point(position = position_jitter(width = 0.2), alpha = 0.3) +
    labs(
        title = "RCT Results: Blood Pressure After Treatment",
        subtitle = "Randomisation ensures groups are comparable at baseline",
        x = "Group",
        y = "Systolic Blood Pressure (mmHg)"
    ) +
    scale_fill_manual(values = c("Control" = "#999999", "Treatment" = "#0072B2")) +
    theme_minimal() +
    theme(legend.position = "none")
```

### 1.4.3 The Crucial Difference: Association vs Causation

**Observational studies can show association; only experiments can establish causation.**

This distinction is fundamental. When we observe that coffee drinkers have lower rates of depression, we have found an association. But this does not mean coffee prevents depression. Perhaps people prone to depression drink less coffee. Perhaps a third factor—like socioeconomic status—affects both coffee consumption and depression risk.

The problem is **confounding**: variables that affect both the exposure and the outcome can create spurious associations or mask real ones.

```{r confounding_illustration, fig.cap="Confounding creates misleading associations"}
set.seed(202)

n_obs <- 500

# Confounder: socioeconomic status (0-100 scale)
ses <- runif(n_obs, 20, 80)

# Coffee consumption depends on SES
coffee <- 0.5 + 0.02 * ses + rnorm(n_obs, 0, 0.3)
coffee <- pmax(0, pmin(coffee, 5))  # Cups per day, bounded

# Depression risk depends on SES (not coffee!)
# Lower SES = higher depression risk
depression_prob <- 0.5 - 0.005 * ses
depression <- rbinom(n_obs, 1, plogis(depression_prob))

confound_data <- data.table(ses, coffee, depression)

# Naive analysis: coffee vs depression (ignoring SES)
naive_result <- confound_data[, .(
    depression_rate = mean(depression),
    n = .N
), by = .(coffee_group = cut(coffee, breaks = c(0, 1, 2, 3, 5)))]

print("Naive analysis (ignoring confounder):")
print(naive_result)

# The naive analysis suggests coffee is protective, but this is confounding!
# High SES -> more coffee AND less depression

ggplot(confound_data, aes(x = coffee, y = depression, colour = ses)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "glm", method.args = list(family = "binomial"),
                colour = "red", se = TRUE) +
    labs(
        title = "The Confounding Problem",
        subtitle = "Coffee appears protective, but SES (colour) drives both variables",
        x = "Coffee Consumption (cups/day)",
        y = "Depression (1 = Yes, 0 = No)",
        colour = "SES"
    ) +
    theme_minimal()
```

**Why randomisation solves confounding:**

In an RCT, random assignment ensures that, on average, the treatment and control groups are balanced on all characteristics—observed and unobserved. Any confounder is equally distributed across groups. Therefore, any difference in outcomes must be due to the treatment itself.

This is why RCTs are the gold standard for causal inference. When an RCT is not possible (for ethical or practical reasons), we must use observational data cautiously and employ statistical methods to adjust for known confounders.

## 1.5 Sampling Methods

The way we select our sample determines how well it represents the population. Poor sampling can render an otherwise well-designed study useless.

### 1.5.1 Probability Sampling Methods

In **probability sampling**, every member of the population has a known, non-zero probability of being selected. This allows us to make valid inferences about the population.

**Simple random sampling (SRS)** gives every possible sample of size n an equal probability of being selected. It is the theoretical ideal and the basis for most statistical formulae.

```{r simple_random_sample}
# Implement simple random sampling from scratch
simple_random_sample <- function(population, n) {
    N <- length(population)
    if (n > N) stop("Sample size cannot exceed population size")

    # Generate random indices without replacement
    indices <- sample(1:N, size = n, replace = FALSE)

    return(population[indices])
}

# Demonstrate
set.seed(303)
population <- 1:1000
srs_sample <- simple_random_sample(population, n = 50)
print(paste("SRS sample of 50 from population of 1000:"))
print(head(srs_sample, 20))
```

**Stratified sampling** divides the population into homogeneous subgroups (strata) and samples from each stratum. This ensures representation of all subgroups and can increase precision.

```{r stratified_sampling}
# Implement stratified sampling from scratch
stratified_sample <- function(data, strata_var, n_per_stratum) {
    # data: data.table with population
    # strata_var: name of stratification variable
    # n_per_stratum: vector of sample sizes per stratum

    strata <- unique(data[[strata_var]])

    if (length(n_per_stratum) == 1) {
        n_per_stratum <- rep(n_per_stratum, length(strata))
    }

    samples <- list()
    for (i in seq_along(strata)) {
        stratum_data <- data[get(strata_var) == strata[i]]
        n <- min(n_per_stratum[i], nrow(stratum_data))
        indices <- sample(1:nrow(stratum_data), size = n, replace = FALSE)
        samples[[i]] <- stratum_data[indices]
    }

    return(rbindlist(samples))
}

# Create example population
set.seed(404)
pop_data <- data.table(
    id = 1:1000,
    region = sample(c("North", "South", "East", "West"), 1000,
                    replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)),
    income = rnorm(1000, mean = 50000, sd = 15000)
)

# Stratified sample: 25 from each region
strat_sample <- stratified_sample(pop_data, "region", 25)
print("Stratified sample (25 per region):")
print(strat_sample[, .N, by = region])
```

**Cluster sampling** divides the population into clusters (often geographic), randomly selects clusters, and samples all individuals within selected clusters. This is practical when a complete population list is unavailable.

**Systematic sampling** selects every k-th individual from a list after a random start. This is simple to implement but can introduce bias if the list has periodic patterns.

```{r systematic_sampling}
# Implement systematic sampling from scratch
systematic_sample <- function(population, n) {
    N <- length(population)
    k <- floor(N / n)  # Sampling interval

    # Random start between 1 and k
    start <- sample(1:k, 1)

    # Select every k-th element
    indices <- seq(from = start, to = N, by = k)

    # Take first n indices (in case of rounding issues)
    indices <- indices[1:min(n, length(indices))]

    return(population[indices])
}

# Demonstrate
set.seed(505)
sys_sample <- systematic_sample(1:1000, n = 50)
print("Systematic sample (every 20th, random start):")
print(head(sys_sample, 20))
```

### 1.5.2 Non-Probability Sampling Methods

In **non-probability sampling**, the probability of selection is unknown. Inference to the broader population is problematic, but these methods are sometimes the only practical option.

**Convenience sampling** selects whoever is readily available. Surveying patients who happen to be in a clinic on a particular day is convenience sampling. It is easy but prone to bias.

**Quota sampling** ensures specific subgroups are represented in predetermined proportions. Unlike stratified sampling, selection within quotas is non-random.

**Snowball sampling** asks initial participants to recruit others, used for hard-to-reach populations. Studies of injection drug users often use snowball sampling.

### 1.5.3 Sampling Bias and How to Minimise It

**Sampling bias** occurs when the sample systematically differs from the population. Common sources include:

- **Undercoverage**: Some population members have zero probability of selection (e.g., homeless individuals in a telephone survey)
- **Non-response**: Selected individuals refuse to participate, and non-respondents may differ from respondents
- **Self-selection**: Volunteers differ from non-volunteers (e.g., clinical trial participants tend to be healthier)

```{r sampling_bias_simulation, fig.cap="Non-response can create substantial bias"}
set.seed(606)

# Simulate a population with a variable that affects both response and outcome
n_pop <- 10000

pop <- data.table(
    id = 1:n_pop,
    health_consciousness = rnorm(n_pop, mean = 50, sd = 15)  # 0-100 scale
)

# True population mean of some health behaviour
pop[, exercise_hours := 2 + 0.05 * health_consciousness + rnorm(n_pop, 0, 1)]
true_mean <- mean(pop$exercise_hours)

# Non-response depends on health consciousness
# Health-conscious people more likely to respond
pop[, response_prob := plogis(-2 + 0.04 * health_consciousness)]
pop[, responds := rbinom(n_pop, 1, response_prob)]

# Sample of respondents
respondents <- pop[responds == 1]
biased_mean <- mean(respondents$exercise_hours)

cat("True population mean:", round(true_mean, 2), "hours/week\n")
cat("Respondent mean:", round(biased_mean, 2), "hours/week\n")
cat("Bias:", round(biased_mean - true_mean, 2), "hours/week\n")
cat("Response rate:", round(100 * mean(pop$responds), 1), "%\n")

# Visualise
ggplot(pop, aes(x = exercise_hours, fill = factor(responds))) +
    geom_histogram(position = "identity", alpha = 0.6, bins = 40) +
    geom_vline(xintercept = true_mean, colour = "red", linetype = "dashed", size = 1) +
    geom_vline(xintercept = biased_mean, colour = "blue", linetype = "dotted", size = 1) +
    labs(
        title = "Non-Response Bias",
        subtitle = "Health-conscious individuals are more likely to respond",
        x = "Exercise (hours/week)",
        y = "Count",
        fill = "Responded",
        caption = "Red dashed: true mean; Blue dotted: respondent mean"
    ) +
    scale_fill_manual(values = c("0" = "#999999", "1" = "#0072B2"),
                      labels = c("Non-respondent", "Respondent")) +
    theme_minimal()
```

### 1.5.4 Implementing Random Sampling in R

Here is a comprehensive function for various sampling methods:

```{r sampling_functions}
# Comprehensive sampling function
sample_data <- function(data, method = "srs", n = NULL,
                        strata_var = NULL, cluster_var = NULL,
                        n_clusters = NULL, frac = NULL) {

    data <- as.data.table(data)
    N <- nrow(data)

    if (method == "srs") {
        # Simple random sample
        if (is.null(n)) stop("n required for SRS")
        indices <- sample(1:N, size = min(n, N), replace = FALSE)
        return(data[indices])

    } else if (method == "stratified") {
        # Stratified sample
        if (is.null(strata_var)) stop("strata_var required for stratified sampling")

        if (!is.null(frac)) {
            # Proportional allocation
            result <- data[, .SD[sample(.N, size = ceiling(.N * frac))], by = strata_var]
        } else if (!is.null(n)) {
            # Equal allocation
            result <- data[, .SD[sample(.N, size = min(n, .N))], by = strata_var]
        } else {
            stop("Either n or frac required")
        }
        return(result)

    } else if (method == "cluster") {
        # Cluster sample
        if (is.null(cluster_var) || is.null(n_clusters)) {
            stop("cluster_var and n_clusters required")
        }

        clusters <- unique(data[[cluster_var]])
        selected_clusters <- sample(clusters, size = min(n_clusters, length(clusters)))
        return(data[get(cluster_var) %in% selected_clusters])

    } else if (method == "systematic") {
        # Systematic sample
        if (is.null(n)) stop("n required for systematic sampling")
        k <- floor(N / n)
        start <- sample(1:k, 1)
        indices <- seq(from = start, to = N, by = k)[1:n]
        indices <- indices[!is.na(indices)]
        return(data[indices])

    } else {
        stop("Unknown method")
    }
}

# Demonstrate all methods
set.seed(707)

demo_pop <- data.table(
    id = 1:500,
    region = sample(c("A", "B", "C", "D"), 500, replace = TRUE),
    cluster = sample(1:20, 500, replace = TRUE),
    value = rnorm(500, 100, 15)
)

srs <- sample_data(demo_pop, method = "srs", n = 50)
strat <- sample_data(demo_pop, method = "stratified", strata_var = "region", n = 12)
clust <- sample_data(demo_pop, method = "cluster", cluster_var = "cluster", n_clusters = 5)
sys <- sample_data(demo_pop, method = "systematic", n = 50)

cat("SRS sample size:", nrow(srs), "\n")
cat("Stratified sample size:", nrow(strat), "\n")
cat("Cluster sample size:", nrow(clust), "\n")
cat("Systematic sample size:", nrow(sys), "\n")
```

## 1.6 Sources of Bias and Variability

All studies are subject to error. Understanding the types of error—and their consequences—is essential for interpreting results and designing better studies.

### 1.6.1 Selection Bias

**Selection bias** occurs when the subjects included in the study are not representative of the target population. The classic example is the 1936 *Literary Digest* poll, which predicted Alf Landon would defeat Franklin Roosevelt in a landslide. The magazine surveyed its subscribers and telephone owners—but in 1936, these were predominantly wealthy Republicans. Roosevelt won by a historic margin.

Selection bias is not reduced by larger samples. A biased sample of 10 million is still biased.

### 1.6.2 Measurement Bias (Systematic Error)

**Measurement bias** occurs when the measurement procedure systematically over- or under-estimates the true value. A miscalibrated blood pressure monitor that consistently reads 5 mmHg too high introduces measurement bias. Unlike random measurement error, systematic error does not average out with repeated measurements.

### 1.6.3 Response Bias

**Response bias** occurs when respondents provide inaccurate answers. Sources include:

- **Social desirability**: People underreport smoking and overreport exercise
- **Recall bias**: Patients with disease remember exposures differently
- **Question wording**: Leading questions elicit biased responses
- **Acquiescence**: Some respondents agree regardless of content

### 1.6.4 Survivorship Bias

**Survivorship bias** occurs when we observe only "survivors" and ignore those who did not survive to be observed. The famous example involves World War II bombers: engineers examined planes returning from missions to determine where to add armour. Abraham Wald realised they were looking at survivors—the holes showed where planes could be hit and still return. Armour should go where returning planes were *not* hit, because those planes never returned.

In medical research, survivorship bias can make treatments appear more effective than they are if we only analyse patients who survived long enough to be evaluated.

### 1.6.5 Random Variability vs Systematic Bias

A crucial distinction exists between **random error** (variability) and **systematic error** (bias):

**Random error** causes estimates to scatter around the true value. It arises from natural variation and measurement imprecision. Larger samples reduce random error—this is why we calculate standard errors that decrease as n increases.

**Systematic error** causes estimates to be consistently wrong in one direction. Larger samples do not help—indeed, they may make us more confident in an incorrect answer.

```{r bias_vs_variability, fig.cap="Random error scatters around truth; systematic error shifts away from truth"}
set.seed(808)

# True value
true_value <- 100

# Scenario 1: Low bias, low variability (ideal)
scenario1 <- rnorm(20, mean = true_value, sd = 2)

# Scenario 2: Low bias, high variability
scenario2 <- rnorm(20, mean = true_value, sd = 10)

# Scenario 3: High bias, low variability
scenario3 <- rnorm(20, mean = true_value + 15, sd = 2)

# Scenario 4: High bias, high variability
scenario4 <- rnorm(20, mean = true_value + 15, sd = 10)

# Create target plot data
target_data <- data.table(
    x = c(scenario1, scenario2, scenario3, scenario4) - true_value,
    y = c(
        rnorm(20, 0, 2), rnorm(20, 0, 10),
        rnorm(20, 15, 2), rnorm(20, 15, 10)
    ) - c(rep(0, 20), rep(0, 20), rep(15, 20), rep(15, 20)),
    scenario = rep(c(
        "Low Bias, Low Variability\n(Accurate & Precise)",
        "Low Bias, High Variability\n(Accurate, not Precise)",
        "High Bias, Low Variability\n(Precise, not Accurate)",
        "High Bias, High Variability\n(Neither)"
    ), each = 20)
)

# Simplify to 2D scatter
set.seed(909)
target_data_2d <- data.table(
    scenario = rep(c(
        "Low Bias, Low Variability",
        "Low Bias, High Variability",
        "High Bias, Low Variability",
        "High Bias, High Variability"
    ), each = 50),
    x = c(
        rnorm(50, 0, 2),
        rnorm(50, 0, 8),
        rnorm(50, 10, 2),
        rnorm(50, 10, 8)
    ),
    y = c(
        rnorm(50, 0, 2),
        rnorm(50, 0, 8),
        rnorm(50, 0, 2),
        rnorm(50, 0, 8)
    )
)

ggplot(target_data_2d, aes(x = x, y = y)) +
    geom_point(alpha = 0.6, colour = "#0072B2") +
    geom_point(aes(x = 0, y = 0), colour = "red", size = 4, shape = 3, stroke = 2) +
    facet_wrap(~scenario, ncol = 2) +
    labs(
        title = "Bias vs Variability: The Target Analogy",
        subtitle = "Red cross marks the true value; points show sample estimates",
        x = "Deviation from Truth (x-axis)",
        y = "Deviation from Truth (y-axis)"
    ) +
    coord_fixed() +
    theme_minimal()
```

## 1.7 Introduction to Statistical Software and Reproducibility

Modern statistics is computational. The ability to write reproducible code is not optional—it is essential for credible science.

### 1.7.1 Why Reproducibility Matters

The **reproducibility crisis** has shaken many scientific fields. High-profile studies have failed to replicate, and many published findings appear to be false. Contributing factors include:

- Underpowered studies finding spurious effects
- Selective reporting of favourable results
- Flexible data analysis ("p-hacking")
- Lack of pre-registration
- Insufficient documentation of methods

Computational reproducibility—the ability to re-run analysis code and obtain identical results—is a minimum standard. If we cannot reproduce our own work, we cannot expect others to trust it.

### 1.7.2 Setting Seeds for Reproducible Random Processes

Many statistical methods involve random number generation: simulation, bootstrapping, cross-validation, and random sampling. Without proper seed-setting, results change each time the code runs.

```{r set_seed_demo}
# Without set.seed(): different results each time
cat("Without set.seed():\n")
cat("Run 1:", mean(rnorm(100)), "\n")
cat("Run 2:", mean(rnorm(100)), "\n")
cat("Run 3:", mean(rnorm(100)), "\n")

# With set.seed(): reproducible results
cat("\nWith set.seed(42) before each run:\n")
set.seed(42); cat("Run 1:", mean(rnorm(100)), "\n")
set.seed(42); cat("Run 2:", mean(rnorm(100)), "\n")
set.seed(42); cat("Run 3:", mean(rnorm(100)), "\n")
```

**Best practice**: Set a seed at the beginning of each script or before each stochastic operation. Document the seed used.

### 1.7.3 Organising Statistical Projects

A well-organised project facilitates reproducibility:

```
project/
├── data/
│   ├── raw/           # Original, unmodified data
│   └── processed/     # Cleaned data ready for analysis
├── src/               # R scripts and Rmd files
├── output/
│   ├── figures/       # Generated plots
│   └── tables/        # Generated tables
├── docs/              # Documentation
└── README.md          # Project overview
```

**Key principles:**

- Never modify raw data; always create processed versions
- Separate data, code, and output
- Document everything
- Use version control (git)
- Name files descriptively and consistently

### 1.7.4 Introduction to R Markdown for Reproducible Reports

R Markdown combines prose, code, and output in a single document. When you compile ("knit") an Rmd file, the code executes and results are embedded in the output. This document you are reading was written in R Markdown.

**Advantages:**

- Analysis and report are one document
- Changes to code automatically update results
- Eliminates copy-paste errors
- Creates documentation as a by-product

---

## Communicating to Stakeholders

When explaining study design concepts to collaborators or non-technical audiences:

**On populations and samples:**
> "We cannot study every patient in the country, so we carefully select a representative group. Statistics helps us learn about all patients from this sample, and tells us how confident we can be in those conclusions."

**On observational vs experimental studies:**
> "When we observe without intervening, we can see that two things are related, but we cannot prove one causes the other. Randomised trials let us make causal conclusions because random assignment ensures the groups are comparable."

**On bias:**
> "Bias means our sample doesn't accurately represent the population. This cannot be fixed by collecting more data—we need to collect data better."

**On reproducibility:**
> "Every number in this report comes from documented code that can be re-run. If you want to verify any result, we can show you exactly how it was calculated."

---

## Quick Reference

### Key Terminology

| Term | Definition |
|------|------------|
| Population | Complete collection of individuals about which we wish to draw conclusions |
| Sample | Subset of the population actually observed |
| Parameter | Numerical characteristic of a population (Greek letters: μ, σ, π) |
| Statistic | Numerical characteristic of a sample (Roman letters: x̄, s, p̂) |
| Descriptive statistics | Methods for summarising observed data |
| Inferential statistics | Methods for drawing conclusions about populations from samples |

### Variable Type Classification

| Question | If Yes | If No |
|----------|--------|-------|
| Is it numerical? | Quantitative | Qualitative |
| Can it take any value in a range? | Continuous | Discrete (if quantitative) |
| Is there a natural order? | Ordinal | Nominal (if qualitative) |

### Scales of Measurement

| Scale | Equality | Order | Difference | Ratio | Example |
|-------|----------|-------|------------|-------|---------|
| Nominal | ✓ | ✗ | ✗ | ✗ | Blood type |
| Ordinal | ✓ | ✓ | ✗ | ✗ | Pain rating |
| Interval | ✓ | ✓ | ✓ | ✗ | Temperature (°C) |
| Ratio | ✓ | ✓ | ✓ | ✓ | Weight (kg) |

### Study Design Summary

| Design | Direction | Best For | Main Limitation |
|--------|-----------|----------|-----------------|
| Cross-sectional | Snapshot | Prevalence | No temporal order |
| Case-control | Backward | Rare diseases | Recall bias |
| Cohort | Forward | Incidence, causation | Expensive, attrition |
| RCT | Forward | Causal inference | Ethical constraints |

### R Functions for Sampling

```r
# Simple random sample
sample(x, size = n, replace = FALSE)

# Stratified sample (with data.table)
data[, .SD[sample(.N, size = n)], by = strata_var]

# Set seed for reproducibility
set.seed(seed_value)
```

### Greek Letter Reference

| Symbol | Name | Represents |
|--------|------|------------|
| μ | mu | Population mean |
| σ | sigma | Population standard deviation |
| σ² | sigma squared | Population variance |
| π or p | pi | Population proportion |
| ρ | rho | Population correlation |
| β | beta | Population regression coefficient |
| θ | theta | Generic parameter |
