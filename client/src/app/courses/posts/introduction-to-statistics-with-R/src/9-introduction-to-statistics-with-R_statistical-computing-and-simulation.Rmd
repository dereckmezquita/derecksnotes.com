---
title: "Introduction to Statistics with R: Statistical Computing and Simulation"
blurb: "Blue and white mean war"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Statistical Computing and Simulation

## Efficient R Programming

As your analyses grow more complex, efficiency in R becomes crucial. Efficiency spans multiple dimensions: code speed, memory usage, and developer productivity. Strategies like **vectorisation**, **profiling**, and judicious data structures help ensure that computations remain manageable, responsive, and scalable—even with large datasets or complicated models.

### Code Optimisation and Vectorisation

**Vectorisation:**
- R is optimised for vector and matrix operations. Instead of writing explicit loops, apply vectorised functions that operate on entire vectors at once.
- Example: Instead of a loop to add 1 to each element of a vector `x`, use `x + 1`.
- This leverages C-level implementations and internal optimisations, often yielding huge performance gains.

**Avoiding Loops When Possible:**
- Loops are not always bad, especially with modern hardware and compilers. But native vector operations, `lapply` or `apply` family, and functions from data.table often run faster.
- If looping is unavoidable, consider `Rcpp` or `inline` C/C++ code for computationally heavy inner loops.

**Data.table vs. dplyr:**
- data.table::fread() is fast for reading large files and data.table operations are known for speed.
- data.table syntax uses `:=` for by-reference modifications, reducing copy overhead.
- data.table indexing and keys provide efficient subsetting and joins.

**R Demonstration (Vectorisation Example):**

\`\`\{r vectorisation-demo, echo=TRUE, message=FALSE, warning=FALSE\}
x <- rnorm(1e6)
system.time({
  y_loop <- numeric(length(x))
  for (i in seq_along(x)) {
    y_loop[i] <- x[i] + 1
  }
})

system.time({
  y_vec <- x + 1
})
\`\`\`

Check timings: The vectorised version should be significantly faster.

### Profiling and Benchmarking R Code

**Profiling:**
- Profiling identifies bottlenecks in code.
- `Rprof()` records the time spent in each function call. After running `Rprof()`, use `summaryRprof()` to see which functions dominate run time.
- Helps identify where optimisations or algorithmic changes yield the greatest improvements.

**Benchmarking:**
- Use `microbenchmark::microbenchmark()` or `bench::mark()` to compare code snippets.
- Run multiple trials and compare median times to ensure stable performance estimates.
- Informs decisions about choosing faster data structures, methods, or libraries.

**R Demonstration (Profiling):**

\`\`\{r profiling-demo, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE\}
Rprof("profile_out.txt")
# run some computations...
Rprof(NULL)
summaryRprof("profile_out.txt")
\`\`\`

Review output to see which functions consume the most time.

### Memory Management and Data Structures for Performance

**Memory Efficiency:**
- Large datasets can tax memory. Minimising copies and using reference semantics help.
- data.table modifies in place with `:=`, avoiding unnecessary copies.
- Use `gc()` to check memory usage if needed.

**Choosing Appropriate Data Structures:**
- Matrices vs. data frames: Matrices store homogeneous data, often faster for linear algebra operations.
- `data.table` for large, tabular data. It is memory-efficient and offers fast subset, merge, and grouping operations.
- Sparse matrices (from `Matrix` package) for sparse data reduce memory footprint and speed up computations.

**R Demonstration (Memory Efficiency with data.table):**

\`\`\{r memory-demo, echo=TRUE, message=FALSE, warning=FALSE\}
library(data.table)
dt <- data.table::data.table(x=rnorm(1e6), y=rnorm(1e6))
print(object.size(dt), units="MB")

# Modify a column in place without copying entire dt
dt[, x := x + 10]
print(object.size(dt), units="MB") # should remain similar
\`\`\`

Observe that the size does not significantly increase after modification.

---

**Key Takeaways:**

- **Vectorisation:**  
  Replace loops with vector or matrix operations for speed gains.
  
- **Profiling and Benchmarking:**  
  Identify slow code segments and compare alternative methods to ensure optimal performance.

- **Memory Management and Data Structures:**
  Choose efficient containers (data.table, sparse matrices) and memory-friendly modifications.
  
By integrating these efficient programming practices, you elevate your R code from straightforward prototypes to production-level analyses—faster run times, lower memory usage, and a smoother development experience. This efficiency ultimately translates into greater scalability and better utilisation of computational resources. 



## Simulation and Monte Carlo Methods

Simulation techniques form a cornerstone of modern statistical practice, enabling exploration of complex models, approximations of integrals, and inference when closed-form solutions are elusive. **Monte Carlo methods**, in particular, rely on generating pseudo-random draws from known or modelled distributions to approximate expectations, test hypotheses, and experiment with hypothetical scenarios. From generating random numbers to applying variance reduction strategies and bootstrapping, these approaches greatly extend our analytical toolkit.

### Generating Pseudo-Random Numbers

**Pseudo-Random Number Generators (PRNGs):**
- R’s built-in PRNGs produce sequences of numbers that appear random but are generated by deterministic algorithms.
- Set seeds for reproducibility using `set.seed()`.
- Functions like `rnorm()`, `runif()`, `rpois()` produce samples from common distributions.

**Practical Tips:**
- Always set seeds in reproducible analyses so results can be replicated.
- For large simulations, ensure enough randomness by choosing different seeds or leveraging parallel streams of random numbers.

**R Demonstration (Random Numbers):**

\`\`\{r random-numbers-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
x <- rnorm(1000, mean=5, sd=2)
dt_rn <- data.table::data.table(x=x)
ggplot(dt_rn, aes(x=x)) +
  geom_histogram(binwidth=0.5, fill="skyblue", colour="black") +
  labs(title="Histogram of Simulated Normal Data", x="X", y="Count")
\`\`\`

Check the shape and location match the chosen parameters.

### Monte Carlo Integration and Variance Reduction Techniques

**Monte Carlo Integration:**
- Approximate integrals by averaging function values at random points.
- Useful when analytic integration is tough or impossible.
- For a function $f(x)$, approximate:
  $$\int f(x) dx \approx \frac{1}{N}\sum_{i=1}^N f(X_i),$$
  where $X_i$ are samples from the distribution of interest.

**Variance Reduction:**
- Naive Monte Carlo can be noisy. Techniques reduce variance and improve efficiency:
  - **Antithetic Variates:** Use negatively correlated draws to stabilise estimates.
  - **Control Variates:** Leverage a function with known mean to reduce variance in target integral.
  - **Importance Sampling:** Sample from a distribution that emphasises important regions, weighting samples accordingly.
  
These techniques decrease the number of samples needed for desired accuracy, saving computational effort.

**R Demonstration (Simple Monte Carlo Integration):**

\`\`\{r monte-carlo-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(456)
N <- 100000
x <- runif(N, min=0, max=1)
f_values <- sqrt(1 - x^2) # quarter-circle integral
estimate <- mean(f_values) # approximate integral over [0,1]
actual <- pi/4
c(estimate=estimate, actual=actual, error=abs(estimate-actual))
\`\`\`

Check how close the Monte Carlo estimate is to the known integral of a quarter-circle.

### Bootstrap Methods Revisited with Simulation

**Bootstrap:**
- A resampling method that approximates the sampling distribution of estimators by drawing with replacement from the observed data.
- Useful for constructing confidence intervals, assessing estimator variability, or performing hypothesis tests without closed-form solutions.

**Parametric vs. Nonparametric Bootstrap:**
- **Nonparametric Bootstrap:** Resample from the empirical distribution of observed data.
- **Parametric Bootstrap:** Simulate from a fitted parametric model.

**Example:**
- Estimate the standard error of the mean:
  - Draw bootstrap samples from the data.
  - Compute the mean for each bootstrap sample.
  - The variability of these means approximates the standard error.

**R Demonstration (Bootstrap):**

\`\`\{r bootstrap-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(789)
x <- rnorm(100, mean=10, sd=3)
B <- 1000
means <- numeric(B)
for (b in 1:B) {
  sample_idx <- sample(seq_along(x), replace=TRUE)
  means[b] <- mean(x[sample_idx])
}
boot_se <- sd(means)
c(bootstrap_se=boot_se, naive_se=sd(x)/sqrt(length(x)))
\`\`\`

Compare the bootstrap-based standard error to the theoretical approximation.

---

**Key Takeaways:**

- **Random Number Generation:**  
  Essential for simulations, ensure reproducibility and suitable distributions.

- **Monte Carlo Integration & Variance Reduction:**  
  Approximate integrals and expectations using random sampling. Techniques like antithetic variates and importance sampling improve efficiency.

- **Bootstrap & Simulation-Based Inference:**
  When analytic solutions are out of reach, bootstraps provide flexible approaches to quantify uncertainty.

Simulation and Monte Carlo methods form an indispensable part of modern statistical toolkits, bridging theory and practice. Whether estimating integrals, assessing estimator variability, or performing complex inferences, these approaches shine where analytical solutions falter, offering robust, intuitive ways to understand and interrogate data and models.  

## Numerical Optimisation and Solvers

Complex statistical models, likelihood evaluations, and parameter estimation often reduce to numerical optimisation problems: finding parameter values that maximise or minimise a function (e.g., a log-likelihood). Since closed-form solutions are rarely available, numerical optimisation techniques step in. Choosing the right optimiser, understanding gradient information, and ensuring stable convergence are key for reliable and efficient results.

### Gradient-Based vs. Derivative-Free Optimisation

**Gradient-Based Optimisation:**
- Methods like Newton-Raphson, BFGS, or Conjugate Gradient rely on gradients (and sometimes Hessians) of the objective function.
- Faster convergence when gradient information is accurate and the function is smooth.
- Challenges:  
  - Requires analytic or numeric gradients.  
  - Sensitive to starting values, may converge to local minima if the landscape is rugged.

**Derivative-Free Methods:**
- Nelder-Mead (simplex) or genetic algorithms approximate optima without needing gradients.
- Useful when gradients are hard to compute or the objective is noisy, non-smooth, or defined by simulations.
- Often slower and may require more function evaluations, but more robust in messy conditions.

### Common R Optimisation Tools (`optim()`, `nloptr`, `Rcpp`)

**`optim()` Function:**
- A versatile built-in R function for general-purpose optimisation.
- Methods include "BFGS", "L-BFGS-B" (for box constraints), "CG", "Nelder-Mead".
- Ideal for moderate-dimensional problems and when you can provide reasonable initial guesses.

**`nloptr` Package:**
- Interfaces to the NLopt library of optimisation routines.
- Provides a wide range of global and local algorithms, including derivative-free and gradient-based methods.
- Flexible and powerful, can handle complex constraints.

**`Rcpp`:**
- For highly customised optimisations, write C++ code via `Rcpp` for speed.
- Integrate with specialised C++ libraries or implement custom gradient calculations.
- Improves performance when pure R solutions are slow.

**R Demonstration (Using `optim()`):**

\`\`\{r optim-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Example: Minimising a simple quadratic function: f(x) = (x-3)^2
f <- function(x) (x-3)^2

res <- optim(par=0, fn=f, method="BFGS")
res$par   # should be close to 3
\`\`\`

Check the result: The optimizer should find a minimum near `x=3`.

### Handling Convergence and Numerical Stability

**Convergence Diagnostics:**
- Check `convergence` field in `optim()` output: 0 means successful convergence.
- If not converged, try:
  - Different initial values.
  - A different algorithm.
  - Scaling or transforming parameters.

**Stability Tips:**
- Scale parameters so they are on a similar order of magnitude. Prevents numerical issues.
- Use numerical derivatives (with `grad()` functions or approximations) if analytic gradients are complicated, but beware of increased computation time and potential numerical noise.
- For challenging likelihood surfaces with multiple local maxima, run optimisation from multiple starting points to ensure a global optimum.

**Robustness Measures:**
- If the function is noisy, consider smoothing techniques or approximate methods (e.g., Bayesian approaches or simulation-based methods).
- Check Hessian matrices and condition numbers for curvature information. Poorly conditioned problems indicate the need for reparameterisation or regularisation.

---

**Key Takeaways:**

- **Gradient-Based vs. Derivative-Free:**  
  If gradients are available and smooth, gradient-based methods are often faster. Otherwise, derivative-free methods provide more robust alternatives.

- **Tools in R:**  
  `optim()` is a go-to for general problems, `nloptr` for advanced optimisations, and `Rcpp` for custom, high-speed implementations.

- **Ensuring Convergence and Stability:**
  Tune initial values, scale parameters, experiment with algorithms, and consider multiple runs to confirm global solutions.

Numerical optimisation is a delicate but essential skill, bridging theory and applied modelling. By mastering these techniques and understanding the strengths and limitations of each method, you can confidently tackle complex estimation tasks and push your analyses to new frontiers.  

## Parallel and Distributed Computing

As datasets and computational demands grow, single-core computation may become a limiting factor. **Parallel and distributed computing** techniques help harness the power of multiple CPU cores, clusters, and even cloud infrastructure to significantly speed up large-scale simulations, model fitting, and data processing. Embracing these approaches allows analysts to scale their work efficiently and meet modern data challenges head-on.

### Parallel Processing in R (multicore, `parallel` package)

**Multicore Parallelism:**
- Most modern machines have multiple CPU cores that can work simultaneously on different tasks.
- R’s built-in `parallel` package provides functions like `mclapply()` on Unix-like systems (macOS, Linux) to distribute tasks across cores.
- On Windows, `mclapply()` is unavailable, but one can use `parLapply()` with a parallel cluster.

**Forking vs. PSOCK Clusters:**
- **Forking** (on Unix): Creates child processes that share memory with the parent. Lightweight, but not available on Windows.
- **PSOCK Clusters:** Platform-independent, start worker processes that communicate over sockets. Slightly more overhead but portable.

**R Demonstration (Basic Parallel Loop):**

\`\`\{r parallel-demo, echo=TRUE, message=FALSE, warning=FALSE\}
library(parallel)
set.seed(123)
x <- rnorm(1e6)

# Using a PSOCK cluster with half the available cores
cl <- makeCluster(detectCores() / 2)
clusterExport(cl, "x")

res <- parLapply(cl, 1:10, function(i) mean(sample(x, 1e5)))
stopCluster(cl)
mean(unlist(res))
\`\`\`

Check how quickly large computations run when distributed among multiple cores.

### High-Performance Computing (HPC) Environments

**Cluster and Supercomputers:**
- Large research institutes or universities often maintain HPC clusters.
- Jobs submitted via schedulers (e.g., SLURM) allow running R code on hundreds or thousands of cores.
- Special libraries (e.g., `snow`, `future`, `BatchJobs`) facilitate cluster computations.

**Vectorised and Parallelised Libraries:**
- Many linear algebra operations in R rely on BLAS/LAPACK libraries. Using optimised, multi-threaded BLAS like OpenBLAS or MKL can yield significant speed-ups in matrix operations.
- For large-scale matrix computations, switching to these high-performance libraries can accelerate analyses with minimal code changes.

### Cloud Computing and Scalable Analytics

**Cloud Infrastructure:**
- Cloud platforms like AWS, Google Cloud, Azure provide on-demand clusters.
- Scale out by spawning multiple instances running R in parallel.
- Perfect for temporary large workloads: pay-as-you-go, no need to maintain physical servers.

**Containerisation and Orchestration:**
- Tools like Docker standardise R environments for reproducibility.
- Kubernetes or AWS Batch orchestrate containers across many nodes, enabling robust, scalable pipelines.
- Combined with parallel R code, easily scale from a single workstation to a large compute cluster.

**R Tools for Cloud:**
- `future` and `furrr` packages enable a unified interface to run tasks locally, on HPC, or in the cloud with minimal code changes.
- `drake` or `targets` workflows help orchestrate complex dependency graphs and parallelise tasks seamlessly.

---

**Key Takeaways:**

- **Multicore and `parallel`:**  
  Easily leverage local CPU cores. On Unix systems, `mclapply()` is convenient; otherwise, PSOCK clusters work universally.

- **HPC Environments:**  
  Run R on supercomputers or large clusters. Use schedulers, parallel BLAS, and dedicated packages to handle huge workloads efficiently.

- **Cloud and Scalable Analytics:**  
  Dynamically scale resources on the cloud. Containerisation and orchestration tools, combined with parallel R code, allow flexible, large-scale data processing and modelling.

By employing parallel and distributed computing techniques, you transform R from a desktop statistician’s toolbox into an engine capable of handling massive datasets and computationally expensive tasks. Embracing these strategies ensures that as data and model complexities increase, your analyses remain timely, cost-effective, and ready to exploit modern computing architectures.  