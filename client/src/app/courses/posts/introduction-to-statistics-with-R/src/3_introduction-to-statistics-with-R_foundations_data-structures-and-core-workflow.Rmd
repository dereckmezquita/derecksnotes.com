---
title: "Introduction to Statistics with R"
subtitle: "Foundations: Data Structures and Core Workflows"
blurb: "File war tea"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

### Chapter 3: Data Structures and Core Workflow

*In this chapter, we enter the heart of data manipulation in R. Before diving into complex analyses and modelling, you must understand the fundamental building blocks that make up your datasets. By the end of this chapter, you will be comfortable working with basic data structures—vectors, matrices, arrays, lists, data frames—and understand how `data.table::` provides a powerful extension for efficient data handling. We will also discuss factors and categorical variables, an essential concept in statistical modelling.*

*Think of this as learning the alphabet of data manipulation. Just as you combine letters to form words and then sentences, you will combine these data structures to form meaningful analyses. Each structure brings its own flavour, constraints, and advantages.*


#### 3.1 Vectors, Matrices, and Arrays in Base R

*Vectors, matrices, and arrays form the DNA of R’s data handling. They are the simplest data structures and often the underlying components of more complex ones. Understanding them allows you to perform basic computations and lays the groundwork for more sophisticated transformations.*

- **Vectors**:  
  A vector is a one-dimensional collection of elements, all of the same type (e.g., numeric, character, logical). You can think of a vector as a line of items on a shelf, each item can be easily identified by its position.  
  For example, a numeric vector:
  
  \\`\\`\\`r
  x <- c(1, 2, 3, 4, 5)
  x
  # Basic operations
  mean_x <- mean(x)
  sd_x <- sd(x)
  print(mean_x)
  print(sd_x)
  \\`\\`\\`
  
  Conceptually, if \( x = (x_1, x_2, \ldots, x_n) \), then:
  
  $$
  \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i
  $$
  
  This formula is directly applicable to vectors, making computations intuitive.

- **Matrices**:  
  A matrix is a two-dimensional structure with rows and columns, and all elements must be of the same type. It can be viewed as a table of numbers, where each entry \( x_{ij} \) represents the element in row \( i \) and column \( j \). Matrices are crucial for linear algebra computations, like:
  
  $$
  X = \begin{pmatrix}
  x_{11} & x_{12} \\
  x_{21} & x_{22}
  \end{pmatrix}
  $$
  
  In R:
  
  \\`\\`\\`r
  M <- matrix(1:9, nrow = 3, ncol = 3)
  M
  # Subset the element in the 2nd row, 3rd column
  elem_2_3 <- M[2, 3]
  print(elem_2_3)
  # Compute row sums or column means
  row_sums <- rowSums(M)
  col_means <- colMeans(M)
  print(row_sums)
  print(col_means)
  \\`\\`\\`

- **Arrays**:  
  Arrays are generalisations of matrices to more than two dimensions. If a matrix is a 2D grid, arrays can be 3D, 4D, or higher. These structures are useful for storing complex, multi-dimensional data, such as spatial or temporal data sets.  
  For example:
  
  \\`\\`\\`r
  A <- array(1:24, dim = c(2, 3, 4))
  A
  # Extract an entire matrix slice from the first dimension
  slice <- A[ , , 1]
  print(slice)
  \\`\\`\\`
  
  Mathematically, if you imagine arrays as multi-dimensional vectors, they provide a systematic way to address elements with multiple indices. This becomes powerful when dealing with datasets that naturally have multiple dimensions (e.g., latitude, longitude, time).

*In practice, you will often convert arrays or matrices into data frames or data.tables for more convenient manipulation. Still, a solid understanding of these fundamentals ensures you know what’s happening under the hood.*


#### Demonstration: A Realistic Numeric Computation

Imagine you have daily temperature readings for multiple cities over several weeks, stored in a matrix where rows represent days and columns represent cities:

\\`\\`\\`r
# Hypothetical temperature data: 30 days, 3 cities
temp_mat <- matrix(rnorm(90, mean = 20, sd = 5), nrow = 30, ncol = 3)
colnames(temp_mat) <- c("CityA", "CityB", "CityC")

# Compute mean temperature per city
mean_temps <- colMeans(temp_mat)
print(mean_temps)

# Visualise the distribution of temperatures for one city:
city_data <- data.table::data.table(temperature = temp_mat[ , "CityA"])
ggplot2::ggplot(data = city_data, ggplot2::aes(x = temperature)) +
  ggplot2::geom_histogram(binwidth = 1, fill = "lightblue", colour = "black") +
  ggplot2::labs(title = "Distribution of Daily Temperatures for CityA",
                x = "Temperature (°C)",
                y = "Frequency")
\\`\\`\\`


---

#### 3.2 Lists, Data Frames, and Introducing data.table

*While vectors, matrices, and arrays are strictly homogenous (all elements share the same type), real-world datasets are often heterogeneous. You might have numeric variables, categorical labels, and textual metadata all in one structure. This is where lists, data frames, and `data.table::` come into play.*

- **Lists**:  
  A list can contain elements of different types—vectors, matrices, even other lists. Lists are like boxes inside boxes, allowing you to store complex data structures.  
  For example:
  
  \\`\\`\\`r
  my_list <- list(numbers = 1:5, city_names = c("London", "Paris", "Berlin"), mat = matrix(1:4, 2, 2))
  my_list$numbers
  my_list$city_names
  \\`\\`\\`
  
  You might think of a list as a toolbox with different compartments, each holding a different tool.

- **Data Frames**:  
  A data frame is a special type of list where each element (column) has the same length, representing a dataset with rows as observations and columns as variables. Data frames are the most common structure for statistical analysis because they mimic the tabular format used in spreadsheets and CSV files.  
  For instance:
  
  \\`\\`\\`r
  df <- data.frame(
    id = 1:5,
    height = c(170, 165, 180, 175, 160),
    city = c("London", "London", "Paris", "Berlin", "Berlin")
  )
  df
  # Subset by column name
  df$city
  # Filter rows (although we'll soon rely on data.table for efficiency)
  df[df$city == "London", ]
  \\`\\`\\`
  
  Conceptually, a data frame is like a matrix where each column can have a different type (numeric, character, factor), making it flexible for real-world data.

- **data.table:: for Efficiency and Speed**:  
  While data frames are great, `data.table::` offers a high-performance extension that enhances filtering, aggregation, and merging operations. A `data.table` is a special kind of data frame that keeps the same tabular format but provides a more efficient syntax and better performance with large datasets.[^1]
  
  \\`\\`\\`r
  # Convert df to a data.table
  dt <- data.table::as.data.table(df)
  
  # Efficient filtering and summarisation:
  city_mean_height <- dt[ , .(mean_height = mean(height)), by = city]
  print(city_mean_height)
  \\`\\`\\`
  
  If you consider a data frame as a standard family car, `data.table::` is like a sports car—faster and more agile when handling large or complex routes. You will rely on `data.table::` extensively throughout this book to manage data efficiently before running statistical analyses or visualisations.

*This combination of structures—lists, data frames, and data.tables—provides the flexibility to represent complex real-world data in R, bridging the gap between raw data and meaningful analysis.*


#### Demonstration: Combining Data into a data.table

Suppose you have separate vectors for ages, weights, and city labels of a sample of individuals:

\\`\\`\\`r
ages <- c(34, 29, 45, 52, 23)
weights <- c(70, 68, 82, 95, 60)
cities <- c("London", "Berlin", "Berlin", "Paris", "London")

# Combine into a data.table
people <- data.table::data.table(age = ages, weight = weights, city = cities)
print(people)

# Compute average weight by city
avg_weight_city <- people[ , .(avg_weight = mean(weight)), by = city]
print(avg_weight_city)

# Visualise the distribution of weights by city
ggplot2::ggplot(data = people, ggplot2::aes(x = city, y = weight)) +
  ggplot2::geom_boxplot(fill = "pink", colour = "black") +
  ggplot2::labs(title = "Weight Distribution by City",
                x = "City",
                y = "Weight (kg)")
\\`\\`\\`


---

#### 3.3 Factors and Categorical Variables

*Factors are a special data structure used to represent categorical variables. Instead of just storing text labels, factors store categories and their possible levels internally as integers, making some computations and statistical modelling more efficient and meaningful.*

- **What Are Factors?**:  
  Suppose you have a variable representing gender with possible values: “Male”, “Female”. A factor for this variable would store these as levels, ensuring that analyses like summary statistics or modelling treat these as categories rather than arbitrary strings.
  
  $$
  \text{Factor} = \{\text{Levels: "Male", "Female"}\}
  $$

- **Creating and Using Factors**:  
  \\`\\`\\`r
  gender <- factor(c("Male", "Female", "Female", "Male", "Male"))
  gender
  levels(gender)
  # You can reorder levels or add new ones:
  gender <- factor(gender, levels = c("Female", "Male"))
  gender
  \\`\\`\\`

- **Why Use Factors?**:  
  Many statistical models (like regression or ANOVA) treat categorical variables differently from numeric ones. By converting a character vector to a factor, you inform R that these values are categories. This ensures, for example, that when fitting a linear model, each level of a factor is included as a separate indicator variable.

- **Factors in data.table**:  
  When reading data from CSV, strings might come in as characters. You can easily convert them to factors:
  
  \\`\\`\\`r
  people[ , city := factor(city)]  # Convert city column to a factor
  str(people)  # Check structure to see city is now a factor
  \\`\\`\\`
  
  Using factors ensures clarity in your analysis. For instance, if you have a factor with levels representing different experimental treatments, summary statistics and modelling functions will respect these categories.

*Imagine factors as a well-organised pantry where each shelf is a category. Instead of throwing all ingredients in randomly, you label shelves “Spices”, “Grains”, “Vegetables” to better manage and understand what you have.*


#### Demonstration: The Effect of Factors in a Plot

Let’s create a small dataset to show how treating a variable as a factor changes a plot:

\\`\\`\\`r
# Suppose we have a dataset with species and measurements
species <- factor(c("Setosa", "Versicolor", "Virginica", "Setosa", "Versicolor"))
measure <- c(5.1, 6.0, 6.3, 4.9, 5.8)

data <- data.table::data.table(species = species, measure = measure)

# Plotting measure by species
ggplot2::ggplot(data = data, ggplot2::aes(x = species, y = measure)) +
  ggplot2::geom_boxplot(fill = "lightgreen", colour = "darkgreen") +
  ggplot2::labs(title = "Measurement by Iris Species",
                x = "Species",
                y = "Measurement")
\\`\\`\\`

Here, treating species as a factor ensures the boxplot recognises them as distinct categories. If we had left them as characters, the plot might look the same, but the underlying statistical procedures would treat them differently if we later applied more complex analyses.


---

*By mastering these fundamental data structures—vectors, matrices, arrays, lists, data frames, data.tables, and factors—you build a strong foundation for all statistical tasks in R. Just as a chef masters basic knife skills before crafting complex dishes, you now have the essential tools to shape data into forms that lend themselves to insightful exploration and analysis.*


[^1]: data.table was developed with performance in mind. For large-scale computations, it often outperforms other solutions, making it ideal for heavy-duty data wrangling tasks.

#### 3.4 Dates, Times, and Character Encoding

*Data often involve temporal components—dates, times, and sometimes even time zones. Handling these correctly is crucial for analyses involving chronological order, seasonality, or event durations. Likewise, textual data may contain various character encodings (UTF-8, Latin-1), and managing these encodings ensures that your analyses remain accurate and your output readable.*

- **Dates and Times in Base R**:  
  R provides classes like `Date` and `POSIXct`/`POSIXlt` for representing dates and times. Working with these classes allows you to perform operations such as adding days, calculating time differences, or extracting specific date components (like the month).
  
  For example:
  
  \\`\\`\\`r
  # Create a Date object
  my_date <- as.Date("2021-10-15")
  my_date
  
  # Create a POSIXct object for times
  my_time <- as.POSIXct("2021-10-15 14:30:00", tz = "UTC")
  my_time
  
  # Extracting components
  year_val <- format(my_date, "%Y")
  hour_val <- format(my_time, "%H")
  print(year_val)
  print(hour_val)
  \\`\\`\\`

  Mathematically, think of dates and times as points on a timeline \( t \in \mathbb{R} \), where each value corresponds to a specific instant. Operations like \( t_2 - t_1 \) yield durations, making it easier to handle intervals or delays in processes.

- **Time Zone Awareness**:  
  POSIXct supports time zones. If you have global data—like transactions from different countries—ensuring that you convert all times to a common reference (often UTC) is important. This prevents confusion and errors when comparing or aggregating events that occur simultaneously but are recorded in different local times.

- **Character Encoding**:  
  Text data can contain accented characters, emojis, or symbols from various alphabets. R typically uses UTF-8 on most modern systems, but older files or certain APIs might return text in other encodings.  
  Ensuring that your strings are correctly encoded avoids issues like “garbled” text or comparison errors.
  
  \\`\\`\\`r
  # Check encoding
  enc <- Encoding("Café")
  print(enc)
  
  # Convert encoding if needed
  new_str <- iconv("Café", from = "UTF-8", to = "ASCII//TRANSLIT")
  print(new_str)
  \\`\\`\\`
  
  Treating character encoding carefully ensures that when you summarise or plot textual data, your labels and legends appear as intended.


#### Demonstration: Time Series Plot

Imagine you have daily sales data with timestamps. After converting these timestamps to a Date object, you might plot daily totals:

\\`\\`\\`r
# Hypothetical data: daily sales
# Let's generate a month's worth of dates
dates <- seq(as.Date("2021-10-01"), as.Date("2021-10-31"), by = "day")
sales <- rnorm(length(dates), mean = 500, sd = 50)
sales_data <- data.table::data.table(date = dates, sales = sales)

ggplot2::ggplot(data = sales_data, ggplot2::aes(x = date, y = sales)) +
  ggplot2::geom_line(colour = "blue") +
  ggplot2::geom_point(size = 2, colour = "red") +
  ggplot2::labs(title = "Daily Sales in October 2021",
                x = "Date",
                y = "Sales Amount")
\\`\\`\\`

Here, treating `date` as a proper Date object ensures the x-axis is scaled correctly as time progresses from start to end of the month.

---

#### 3.5 Data Import and Export: CSV, Excel, Databases, and APIs

*Real-world data rarely come as neat R objects. Instead, you will frequently import data from external sources—CSV files, Excel spreadsheets, relational databases, or even web APIs. Similarly, after analysing and transforming data, you might need to export it for reporting or further sharing. Mastering import/export is like mastering the doors and windows of your data “house.”*

- **CSV Files**:  
  CSV (comma-separated values) files are ubiquitous. Use `data.table::fread()` for efficient reading:
  
  \\`\\`\\`r
  my_data <- data.table::fread("data/my_data.csv")
  print(my_data)
  \\`\\`\\`

  Exporting data to CSV:
  
  \\`\\`\\`r
  data.table::fwrite(my_data, "output/my_data_export.csv")
  \\`\\`\\`
  
  This ensures you can easily share your results or feed them into other systems.

- **Excel Files**:  
  While CSV is simpler, sometimes data arrive in Excel workbooks. You can use `readxl::read_excel()`[^1] to read these:
  
  \\`\\`\\`r
  excel_data <- readxl::read_excel("data/survey_data.xlsx", sheet = 1)
  print(excel_data)
  \\`\\`\\`
  
  After processing, if you need to write back to Excel, consider packages like `openxlsx::write.xlsx()` explicitly referenced. However, CSV is often preferred for reproducibility and simplicity.

- **Databases**:  
  For large-scale applications, data might reside in a SQL database. You can connect using `DBI::dbConnect()`, `RSQLite::`, `RPostgres::`, or other database backends. Retrieving data from a database often involves writing SQL queries and reading the result into a `data.table::` object:
  
  \\`\\`\\`r
  # Hypothetical example (not run):
  # con <- DBI::dbConnect(RSQLite::SQLite(), "data/my_database.db")
  # query_result <- DBI::dbGetQuery(con, "SELECT * FROM sales_table")
  # sales_dt <- data.table::as.data.table(query_result)
  # DBI::dbDisconnect(con)
  \\`\\`\\`
  
  This approach is powerful for large datasets or production environments.

- **APIs and Web Data**:  
  Some data come from online APIs. You can use `httr::GET()` to fetch JSON or CSV data from a URL:
  
  \\`\\`\\`r
  # Hypothetical URL:
  url <- "https://example.com/data_api?format=csv"
  api_data <- data.table::fread(url)
  print(api_data)
  \\`\\`\\`
  
  Working with APIs may involve authentication, pagination, or parsing JSON. Packages like `jsonlite::fromJSON()` can help if the API returns JSON. Always store your API keys and credentials securely (e.g., environment variables) rather than hardcoding them in scripts.


#### Demonstration: Reading a Public Dataset from the Web

For instance, consider the well-known iris dataset is already built into R, but let’s say you found a CSV version online:

\\`\\`\\`r
iris_url <- "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
iris_dt <- data.table::fread(iris_url)
print(iris_dt)

# Summary stats
summary_dt <- iris_dt[ , .(
  mean_sepal_length = mean(SepalLength),
  mean_sepal_width = mean(SepalWidth),
  mean_petal_length = mean(PetalLength),
  mean_petal_width = mean(PetalWidth)
)]
print(summary_dt)

# Visualise
ggplot2::ggplot(data = iris_dt, ggplot2::aes(x = SepalLength, y = PetalLength, colour = Species)) +
  ggplot2::geom_point() +
  ggplot2::labs(title = "Iris Sepal vs. Petal Length",
                x = "Sepal Length",
                y = "Petal Length")
\\`\\`\\`

---

#### 3.6 Data Cleaning and Transformation with Base R and data.table

*Raw data often come messy: missing values, outliers, inconsistent variable names, or unwanted rows and columns. Cleaning and transforming data is a critical step before any statistical modelling or visualisation. Think of it as washing and chopping ingredients before cooking—it might not be glamorous, but it’s essential.*[^2]

- **Identifying Missing Values**:  
  R represents missing data as `NA`. Use `is.na()` to detect them. For example:
  
  \\`\\`\\`r
  # Suppose we have a vector with some missing values
  vals <- c(1, 2, NA, 4, NA)
  is.na(vals)
  # Count how many are missing
  sum(is.na(vals))
  \\`\\`\\`
  
  Deciding how to handle missing data—imputation, removal, or analysis as-is—depends on the context and the statistical methods you plan to use.

- **Filtering and Subsetting**:  
  Using `data.table::`, you can filter rows based on conditions or select specific columns:
  
  \\`\\`\\`r
  # From iris_dt, select rows where Species is "setosa"
  setosa_data <- iris_dt[Species == "setosa"]
  print(setosa_data)
  
  # Select only SepalLength and Species columns
  subset_data <- iris_dt[ , .(SepalLength, Species)]
  print(subset_data)
  \\`\\`\\`

- **Creating New Variables**:  
  Derive new variables using arithmetic or conditional logic. For example, if you want to classify iris flowers into “small” and “large” based on SepalLength:
  
  \\`\\`\\`r
  iris_dt[ , size_class := ifelse(SepalLength > 5.5, "large", "small")]
  table(iris_dt$size_class)
  \\`\\`\\`

- **String Cleaning and Recoding**:  
  Sometimes textual data need standardisation—removing trailing spaces, changing case, or recoding categories. Base R functions like `tolower()`, `trimws()` help clean strings. You can also use `sub()` or `gsub()` to replace patterns.
  
  \\`\\`\\`r
  # Suppose we have city names with inconsistent spacing
  city_data <- data.table::data.table(city = c("London ", "london", " Berlin", "berLin "))
  city_data[ , city := trimws(city)]
  city_data[ , city := toupper(city)]
  print(city_data)
  \\`\\`\\`

- **Aggregations and Group Operations**:  
  Use `data.table::` group-by syntax to summarise data. For example, computing mean SepalLength by Species:
  
  \\`\\`\\`r
  mean_by_species <- iris_dt[ , .(mean_sepal = mean(SepalLength)), by = Species]
  print(mean_by_species)
  \\`\\`\\`
  
  This approach is akin to mathematical group summaries:
  
  $$
  \bar{x}_g = \frac{\sum_{i \in g} x_i}{|g|}
  $$
  
  where \( g \) denotes a group and \( x_i \) are the values in that group.

- **Chaining Operations**:  
  `data.table::` allows chaining multiple operations in a readable way:
  
  \\`\\`\\`r
  # Filter setosa, compute mean petal length, and sort by mean value
  result <- iris_dt[Species == "setosa", .(mean_petal = mean(PetalLength)), by = Species][order(mean_petal)]
  print(result)
  \\`\\`\\`

*By mastering these cleaning and transformation steps, you ensure that your data is well-prepared for the subsequent steps of modelling, exploration, and inference.*


#### Demonstration: A Realistic Cleaning Scenario

Suppose you have a dataset of flight delays that includes missing values and inconsistent airline names. After importing, you might do:

\\`\\`\\`r
# Hypothetical: flight_data.csv with columns: airline, delay, destination
flight_data <- data.table::fread("https://example.com/flight_data.csv")

# Clean airline names: remove trailing spaces, uppercase
flight_data[ , airline := toupper(trimws(airline))]

# Check how many missing delays
sum(is.na(flight_data$delay))

# Impute missing delays with median delay
median_delay <- median(flight_data$delay, na.rm = TRUE)
flight_data[is.na(delay), delay := median_delay]

# Summarise average delay by airline
avg_delay_by_airline <- flight_data[ , .(avg_delay = mean(delay)), by = airline]
print(avg_delay_by_airline)

# Visualise:
ggplot2::ggplot(data = avg_delay_by_airline, ggplot2::aes(x = airline, y = avg_delay)) +
  ggplot2::geom_col(fill = "lightblue") +
  ggplot2::coord_flip() +
  ggplot2::labs(title = "Average Flight Delay by Airline",
                x = "Airline",
                y = "Average Delay (minutes)")
\\`\\`\\`

In this demonstration, we handled missing values, standardised text, and aggregated data to gain insights.

---

*By understanding dates and times, mastering data import and export techniques, and becoming adept at data cleaning and transformation, you set the stage for robust, reproducible analyses. These skills ensure that when you model or visualise your data later, you trust the underlying structure and values.*


[^1]: If you need to handle Excel files, `readxl::` is a common choice. It does not require external dependencies, unlike some other options.  
[^2]: Data cleaning can consume a significant portion of analysis time. Investing in good practices pays off later when you have reliable, tidy datasets ready for modelling.  

#### 3.7 Efficient Data Aggregation and Reshaping (melt, dcast in data.table)

*As your datasets grow, you will often need to summarise them or reorganise their structure to answer different analytical questions. Aggregation means taking raw data and collapsing it into summarised statistics—like total sales per month or mean measurement per group. Reshaping means transforming a dataset from wide format to long format, or vice versa, enabling different types of analyses or plots. In `data.table::`, the `melt()` and `dcast()` functions are your reliable tools for reshaping data efficiently.*[^1]

- **Aggregation with data.table::**:  
  Suppose you have a dataset with multiple numeric columns and a grouping variable. Instead of manually computing summaries for each group, `data.table::` lets you do it in one line:
  
  \\`\\`\\`r
  # Hypothetical dataset of sales: columns are region, product, and sales_amount
  sales_data <- data.table::data.table(
    region = rep(c("North", "South", "East", "West"), each = 5),
    product = rep(c("WidgetA", "WidgetB", "WidgetC", "WidgetD", "WidgetE"), 4),
    sales_amount = runif(20, 50, 500)
  )
  
  # Compute mean sales by region
  mean_sales_region <- sales_data[ , .(mean_sales = mean(sales_amount)), by = region]
  print(mean_sales_region)
  \\`\\`\\`
  
  This approach mirrors the mathematical idea of grouping data by a categorical variable \( g \) and computing a statistic:
  
  $$
  \text{Aggregated Value} = f(\{x_i : i \in g\})
  $$
  
  where \( f \) could be mean, sum, median, etc.

- **Wide vs. Long Format**:  
  Data often come in “wide” format where each variable gets its own column, or “long” format where each observation of a variable-value pair appears as a separate row. Some analyses (like time series plots or facetting) are easier in long format, while others (like correlation matrices) may require wide format. The `melt()` and `dcast()` functions from `data.table::` help you switch between these formats efficiently.

- **Melt: From Wide to Long**:  
  `melt()` takes a dataset and turns specified columns into “value” and “variable” columns. For example, consider a dataset of monthly temperatures in different cities:
  
  \\`\\`\\`r
  temp_data <- data.table::data.table(
    city = c("London", "Paris", "Berlin"),
    Jan = c(5, 4, 1),
    Feb = c(6, 5, 2),
    Mar = c(8, 7, 4)
  )
  
  # Melt into long format
  long_temp <- data.table::melt(temp_data, id.vars = "city", variable.name = "month", value.name = "temperature")
  print(long_temp)
  \\`\\`\\`
  
  Now each row represents a (city, month, temperature) triplet, making it easier to plot or analyse monthly trends.

- **Dcast: From Long to Wide**:  
  `dcast()` is the inverse operation, allowing you to pivot data back to wide format. If you have a long dataset with columns “group”, “variable”, and “value,” you can cast it into a wide table:
  
  \\`\\`\\`r
  wide_temp <- data.table::dcast(long_temp, city ~ month, value.var = "temperature")
  print(wide_temp)
  \\`\\`\\`
  
  The formula `city ~ month` tells `dcast()` how to pivot the data: “city” goes on rows, and each “month” becomes a separate column.

- **Combining Aggregation and Reshaping**:  
  You might first aggregate data (e.g., compute mean temperature per city per month) and then reshape it for plotting. This flexibility helps you tell a clearer story with your data.

#### Demonstration: Visualising Melted Data

After melting the temperature data, you could plot monthly temperatures by city:

\\`\\`\\`r
ggplot2::ggplot(data = long_temp, ggplot2::aes(x = month, y = temperature, group = city, colour = city)) +
  ggplot2::geom_line() +
  ggplot2::geom_point(size = 3) +
  ggplot2::labs(title = "Monthly Temperatures by City",
                x = "Month",
                y = "Temperature (°C)")
\\`\\`\\`

This plot would not be as straightforward in a wide format. The melted format makes it natural to treat “month” as a single variable on the x-axis.

---

#### 3.8 Memory and Performance Considerations in Data Wrangling

*As datasets grow larger—think millions of rows or gigabytes of data—the efficiency of your code becomes increasingly important. Not all operations scale equally, and some naive approaches may cause R to run out of memory or take hours to complete. By understanding memory and performance considerations, you can write code that handles big data gracefully and avoids unnecessary bottlenecks.*[^2]

- **Why Performance Matters**:  
  Consider that a simple calculation that takes one second for a small dataset might take minutes or hours for a very large one. Efficient code ensures that your analyses remain feasible and your results timely, which is especially critical in production environments or real-time analytics.

- **data.table:: Efficiency**:  
  The `data.table::` package was designed with performance in mind. It avoids making unnecessary copies of your data, and many common operations (joins, aggregations, filtering) are implemented in highly optimised ways. By using `data.table::`, you already gain substantial performance benefits over less efficient approaches.

- **Memory Management**:  
  - **Use of In-Place Modifications**: Instead of creating new copies of data frames every time you transform them, `data.table::` allows in-place modifications. For example, `DT[ , new_col := f(old_col)]` creates a new column without copying the entire dataset.
  
  - **Subsetting and Slicing**: Load only the columns and rows you need. If you have a massive dataset, consider reading in a subset of columns (`select` argument in `fread()`) or filtering rows on the fly.
  
  - **Chunked Processing**: For extremely large files, consider processing data in chunks—read a portion, summarise it, then move on. This approach reduces peak memory usage.

- **Parallel Computing and Hardware Considerations**:  
  If your computations are still too slow, you might leverage parallel computing. Running computations in multiple cores of your CPU can cut down runtime. Also, investing in hardware upgrades (more RAM, faster SSDs) can help.

- **Benchmarking and Profiling**:  
  Use `system.time()` to measure how long a piece of code runs. For more detailed profiling, tools like `Rprof()` or `profvis` can highlight where the bottlenecks are. Identifying the slowest part of your workflow lets you focus on optimising just that section.

  For instance:
  
  \\`\\`\\`r
  start_time <- Sys.time()
  # Some long operation
  Sys.sleep(2)
  end_time <- Sys.time()
  print(end_time - start_time)
  \\`\\`\\`

  This code shows a trivial timing example, but in practice, you’d measure the time of your actual computations to understand their performance profile.

- **Mathematical Insight into Complexity**:  
  Consider that some operations scale linearly with the number of rows \( n \), while others scale quadratically \( O(n^2) \). As \( n \) grows, quadratic operations become intractable. Understanding the complexity of your algorithms can guide you to choose more efficient methods. For example, grouping and summarising often scale approximately linearly with \( n \), but certain types of merges or sorts might not. Minimising expensive operations (like sorting) or using data structures optimised for them can make a big difference.

#### Demonstration: Memory-Efficient Data Import

If you have a CSV file with many columns but you only need a few, specify those columns during import:

\\`\\`\\`r
# Hypothetical large dataset with 100 columns, but we only need 3:
needed_columns <- c("date", "sales", "region")
big_data <- data.table::fread("https://example.com/huge_data.csv", select = needed_columns)
print(dim(big_data))  # Fewer columns, memory saved

# Summarise after filtering rows to reduce memory usage
filtered_data <- big_data[region == "North"]
mean_sales_north <- filtered_data[ , mean(sales)]
print(mean_sales_north)
\\`\\`\\`

By selecting only needed columns and filtering rows early, you prevent reading and storing unnecessary data, improving performance and memory efficiency.

---

*By understanding how to aggregate and reshape data effectively, and by being mindful of performance and memory constraints, you ensure that your data processing workflow is both flexible and scalable. These skills prepare you to handle increasingly complex datasets and analyses as you progress through the statistical techniques in later chapters.*


[^1]: `melt()` and `dcast()` were inspired by similar functions in the reshape2/tidyr packages but `data.table::` provides them with extra speed and memory efficiency.  
[^2]: As data sizes explode in modern analytics (e.g., big data, streaming data), performance considerations become essential survival skills for any data scientist.  