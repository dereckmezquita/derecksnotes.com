---
title: "Introduction to Statistics with R: Ethical Considerations and Responsible Data Use  "
blurb: "Blue and white mean war"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Ethical Considerations and Responsible Data Use

## Data Privacy and Confidentiality

Protecting data privacy and confidentiality is crucial when working with sensitive or proprietary datasets. This involves ensuring that the identities of individuals represented in the data remain undisclosed, and that their personal information cannot be linked back to them. It also means adhering to relevant legal and ethical standards, which may vary by country, sector, and data type. In fields like healthcare, finance, or social sciences, maintaining trust and compliance with regulations (like the EU’s GDPR or other data protection laws) is non-negotiable.

### Anonymisation, De-Identification, and Secure Storage

- **Anonymisation vs. De-Identification:**  
  - **Anonymisation**: Removing all personally identifiable information (PII) so that individuals cannot be re-identified, even with additional data.  
  - **De-Identification**: A lighter approach that removes or modifies direct identifiers (like names, addresses, national IDs), but indirect identifiers might still be present. Under certain conditions, clever cross-referencing could re-identify individuals.  
  For example, if you have a health dataset of patients, anonymisation might mean removing names, dates of birth, exact addresses, and replacing them with broader categories (e.g., age groups, partial postcodes). De-identification might remove names but keep zip codes, which could still lead to re-identification if combined with other datasets.

- **Safe Harbour Principles:**  
  These principles provide guidance on what data to remove or transform. For health data, this might include removing names, phone numbers, full dates of birth, and exact geographic information to reduce the risk of re-identification.

- **Data Encryption and Access Controls:**  
  Even after anonymisation, store data securely:
  - Encrypt data at rest (on disk) and in transit (e.g., using HTTPS or SFTP).  
  - Restrict access to authorised personnel with role-based permissions.  
  - Keep audit logs of who accesses data and when.

- **Secure Storage and Backup:**  
  Use secure servers or cloud environments adhering to security standards. Regularly back up data and test recovery procedures. Consider `data.table::fread()` to load CSV files from a secure location, ensuring no unauthorised modifications have occurred.

**Comment:** If you need a public dataset with sensitive attributes, I can provide instructions to simulate or identify one from a public repository. Let’s assume we are using a public health dataset from a government open data portal that includes aggregated health outcomes by region, ensuring no direct identifiers exist.

### Working with Sensitive or Proprietary Datasets

- **Proprietary Datasets:**  
  Datasets owned by private companies or containing intellectual property often come with legal agreements and non-disclosure clauses. For instance, a marketing dataset from a private firm might detail purchasing habits of customers. Before analysis, review contracts and ensure compliance with their terms.

- **Data Use Agreements (DUAs) and IRB Approval:**  
  In academic or research settings, Institutional Review Boards (IRBs) or ethics committees may require approval before sensitive data usage. DUAs can specify permissible uses, duration of access, methods of data storage, and requirements for data destruction after project completion.

- **Aggregations and Perturbations:**  
  When sharing results, aggregate data to levels that prevent revealing individual patterns. For example, instead of providing raw counts of a rare disease in a small postcode, aggregate to a broader geographic unit. Introduce random noise to counts or round data to prevent exact matches that could reveal identities.

- **Example:**  
  Suppose we have sensitive health data on hospital admissions at a fine geographic level. We could aggregate admissions by month and region rather than showing patient-level entries. In R, you could do:

  \`\`\{r, echo=TRUE\}
  # Hypothetical aggregated hospital data (if no real dataset available, will simulate)
  if(!requireNamespace("data.table", quietly=TRUE)) {
    install.packages("data.table")
  }
  library(data.table)
  # Simulate aggregated data: region, month, admission_count
  set.seed(123)
  dt <- data.table(
    region = rep(paste0("Region_", 1:5), each=12),
    month = rep(month.abb[1:12], times=5),
    admission_count = rpois(60, lambda=50)
  )

  # Summarise data by quarter to reduce granularity:
  dt[, quarter := ceiling(match(month, month.abb)/3)]
  dt_quarterly <- dt[, .(admissions=sum(admission_count)), by=.(region, quarter)]
  dt_quarterly
  \`\`\`

  Here, we aggregated monthly admissions to quarterly counts, making it harder to identify particular patients and their admission timings.

- **Visualising Aggregated Data Securely:**  
  When visualising sensitive information, ensure your plots show aggregated trends without allowing “drilling down” to identify individual records.

  \`\`\{r, echo=TRUE\}
  if(!requireNamespace("ggplot2", quietly=TRUE)) {
    install.packages("ggplot2")
  }
  library(ggplot2)

  ggplot(dt_quarterly, aes(x=quarter, y=admissions, colour=region)) +
    geom_line() +
    labs(title="Hospital Admissions by Quarter and Region",
         x="Quarter", y="Total Admissions")
  \`\`\`

This line plot shows high-level trends rather than patient-level data.

---

**Key Takeaways:**
- Prioritise removing or masking identifiers and carefully control who can access sensitive data.
- Adhere to all applicable legal, ethical, and contractual requirements.
- Use aggregated results and statistical disclosure techniques (like adding noise or rounding) when sharing findings.
- Properly archive or destroy data after the project’s conclusion to maintain trust and comply with legal obligations.

By maintaining robust data privacy and confidentiality practices, you respect the individuals behind the data and uphold the integrity of your analyses.

[^1]: Consider regulatory frameworks like HIPAA in the US or GDPR in the EU for healthcare and personal data protection guidelines.

## Ethical Statistics and Research Integrity

Ensuring integrity in statistical practice is not merely a technical detail; it is central to maintaining the credibility of the scientific enterprise. Ethical statistics encompass principles and behaviours that prevent bias, promote transparency, and uphold trust in quantitative findings. This involves acknowledging uncertainty, adhering strictly to pre-planned analyses, and avoiding temptations such as selectively reporting results that “look good.”

### Avoiding P-Hacking, HARKing, and Researcher Degrees of Freedom

- **P-Hacking:**  
  P-hacking refers to the practice of manipulating data analysis methods (e.g., multiple testing, selective variable inclusion, post-hoc transformations) until a statistically significant result is found. For example, if a researcher tries many different models and only reports the one that yields a p-value < 0.05, that is p-hacking. This inflates the Type I error rate and presents misleading conclusions.

- **HARKing (Hypothesising After Results are Known):**  
  HARKing is stating a hypothesis after seeing the data and results, then presenting it as if it were the original hypothesis. This distorts the research narrative and can lead to spurious theories. For instance, after exploring a large dataset, a researcher finds that a certain variable correlates with an outcome. Stating post-hoc that this relationship was a priori hypothesised is dishonest.

- **Researcher Degrees of Freedom:**  
  These are the many choices a researcher can make (data cleaning decisions, inclusion of certain covariates, stopping rules for data collection) that, if exercised without transparency, can bias results. Each decision point can shift findings towards significance. Declaring these decisions in advance (via study protocols or pre-registration) and justifying them helps prevent cherry-picking.

**Example:** Suppose we have a dataset from a public finance dataset (if needed, I can find a suitable open dataset, but let’s assume a dataset containing monthly sales from different stores). A researcher might run multiple regression models, each time adding or removing predictors, until finding one that gives a “significant” p-value for a variable of interest. This is unethical if not reported transparently.

### Transparency, Open Science, and Pre-Registration

- **Pre-Registration of Analysis Plans:**  
  Before collecting or analysing data, pre-registering your hypotheses, planned methods, and primary outcomes can prevent HARKing and p-hacking. Platforms like the Open Science Framework (OSF) allow researchers to store protocols so that future readers can verify that the analysis followed the original plan.

- **Open Data and Open Code:**  
  Sharing datasets (when ethically permissible) and analysis scripts enables others to reproduce results, identify errors, and confirm conclusions. Tools like data.table::fread() help load data quickly and `ggplot2` assists in creating replicable plots. This open ecosystem fosters trust and supports the cumulative nature of science.

  For example, if a researcher uses `data.table::fread()` to load a CSV of financial data from a public repository, they can share both the original CSV and the `.Rmd` code so others can easily rerun analyses.

- **Version Control and Research Compendia:**  
  Using GitHub or GitLab to version control code and analyses further enhances transparency. One can track every change, highlight the exact code used, and connect code revisions to evolving research decisions.

### Professional Codes of Conduct (ASA, RSS)

- **American Statistical Association (ASA) Ethical Guidelines:**  
  The ASA provides guidelines emphasising honesty, integrity, and accountability. Statisticians should not knowingly provide incomplete or misleading information. They should respect privacy and be responsible stewards of data.

- **Royal Statistical Society (RSS) Code of Conduct:**  
  The RSS code similarly stresses objectivity, competence, and confidentiality. It guides statisticians to communicate statistical work clearly, avoid bias, and serve public interest.

**In Practice:**

- **Demonstration with a Real Dataset:**  
  Let’s consider a small demo using a publicly available dataset. If none are readily available in R, we can use the built-in `mtcars` dataset, acknowledging it’s not specifically a “sensitive” dataset. The principles remain the same though: we show how to declare analysis steps transparently and avoid selective reporting.

  \`\`\{r, echo=TRUE\}
  # Load a dataset
  # For demonstration, we use mtcars (public dataset included with R)
  # Suppose we are interested in relationship between mpg and horsepower (hp).
  # Avoid p-hacking: Pre-register that we'll fit a linear model of mpg ~ hp + wt.
  
  library(data.table)
  dt <- data.table(mtcars)
  
  # Pre-defined model (not chosen after seeing results)
  # We'll show the model and p-values but not try multiple transformations
  model <- lm(mpg ~ hp + wt, data=dt)
  summary(model)
  \`\`\`

  This code chunk simulates a scenario where we commit upfront to using hp (horsepower) and wt (weight) as predictors for mpg (miles per gallon). We do not run multiple variants searching for significance. Instead, we show the result as is, promoting integrity.

- **Disclosing Limitations and Uncertainties:**  
  In the report, the researcher would explain assumptions (linearity, normality of residuals), show diagnostic plots, and note any potential confounders not included due to prior rationale. If initial plans included these exact two predictors, no additional “fishing” for better results is done.

  For instance:

  \`\`\{r, echo=TRUE\}
  # Diagnostic plots
  par(mfrow=c(2,2))
  plot(model)
  \`\`\`

  Discussing these plots: If residuals look non-randomly distributed, we acknowledge it. If the significance is marginal, we report it honestly without suppressing the result.

**Takeaways:**

- Ethical statistical practice means resisting the temptation to p-hack or HARK. Instead, we:
  - Pre-plan analyses and stick to them.
  - Openly share data and code (when possible).
  - Follow professional guidelines from organisations like ASA and RSS.
  - Communicate findings honestly, including uncertainty and limitations.
  
By adhering to these principles, statisticians and researchers preserve the credibility of their work, contribute to a trustworthy scientific literature, and uphold the highest standards of research integrity.

[^1]: The ASA’s Ethical Guidelines for Statistical Practice and RSS Code of Conduct provide detailed frameworks for ethical decision-making in statistical analysis. They can be found on their respective association websites.


## Bias, Fairness, and Equity in Data Science

In the realm of data-driven decision-making, ensuring fairness and equity stands at the forefront of ethical and responsible data science. This means acknowledging that algorithms and statistical models, if trained or interpreted without care, can unintentionally perpetuate existing societal biases. Such biases can manifest in hiring algorithms favouring one demographic over another, loan approval models discriminating against certain income brackets, or healthcare screening tools providing poorer quality recommendations for underrepresented populations. Ensuring fairness and equity is not just a technical fix; it involves understanding social contexts, engaging with affected communities, and embedding inclusive practices throughout the data science pipeline.

### Identifying and Mitigating Algorithmic Bias

- **Sources of Bias:**  
  Bias can arise from many sources:
  - **Historical Bias:** If historical data used to train a model is itself a reflection of past discrimination or imbalance (e.g., fewer loans given to certain groups), then the model may learn and perpetuate these patterns.
  - **Sampling Bias:** Data not representing the population of interest fairly (e.g., oversampling certain demographics while underrepresenting others) can produce skewed insights.
  - **Measurement Bias:** Variables used as proxies may not capture the intended construct equally well across all groups. For instance, using credit history as a predictor of financial trustworthiness might inherently disadvantage those with less access to credit historically.

- **Fairness Metrics:**  
  Many metrics exist to assess fairness in algorithms. Examples include:
  - **Demographic Parity:** The model’s predictions should be independent of a protected attribute (such as race, gender, etc.).
  - **Equalised Odds:** Conditional on the true outcome, the model’s predictions should be equally accurate across groups.
  - **Predictive Parity:** The probability that positive predictions are correct should be the same across groups.

  These metrics help identify when a model’s performance or outcomes differ significantly between groups, alerting us to potential bias.

- **Remediation Techniques:**  
  Approaches to mitigate bias can include:
  - **Pre-Processing Methods:** Adjusting or reweighting training data to remove historical imbalances. For example, oversampling underrepresented groups or correcting labels.
  - **In-Processing Methods:** Incorporating constraints or penalties during model training to promote fair outcomes, such as adding fairness constraints to a loss function.
  - **Post-Processing Methods:** Adjusting model outputs after training (e.g., threshold shifting) to achieve a chosen fairness metric.

### Inclusive Data Practices and Stakeholder Engagement

- **Inclusive Data Collection:**  
  Ensuring that data collection efforts represent all relevant groups and contexts is key. For instance, a health dataset should not disproportionately contain data from one demographic if the model’s predictions are meant to apply broadly. Collecting more complete demographic information (where ethically permissible and in compliance with privacy laws) can help assess and address bias more effectively.

- **Stakeholder Engagement:**  
  Fairness is not purely a statistical concept; it involves values and priorities that vary by community. Engaging with stakeholders—such as patients in a healthcare setting, communities affected by policy decisions, or students in an educational assessment scenario—allows data scientists to understand what constitutes “fairness” from multiple perspectives. This can guide the choice of fairness metric and remediation strategy.

- **Contextual Understanding:**  
  A model that appears fair using one metric may be considered unfair under another, depending on societal context. For example, ensuring equal false positive rates for job applicants across groups might differ in moral priority compared to ensuring equal true positive rates. Understanding the domain context, historical injustices, and community expectations is essential.

**Demonstration Using Real Data:**

If we do not have a package that provides a dataset suitable for fairness analysis (e.g., a dataset with sensitive attributes and outcomes), please let me know. I will find a real-world dataset to demonstrate bias detection. For illustration, let’s assume we have a dataset of loan applications including demographic attributes. We’ll imagine a dataset with variables: `age`, `income`, `race`, `loan_approved` (a binary outcome), and so forth.

\`\`\`r
# Pseudocode demonstration (assuming we have a dataset 'loan_data.csv'):
# loan_data.csv hypothetically contains columns: race, income, loan_approved (0/1)
# We'll simulate loading and basic fairness checks.

library(data.table)
dt <- data.table::fread("loan_data.csv")  # Assume this dataset is available

# Check approval rates by race to identify potential bias:
approval_rates <- dt[, .(approval_rate = mean(loan_approved)), by=race]
print(approval_rates)

# Suppose we see that the approval rate for one racial group is significantly lower:
# This suggests potential bias in the historical data or the model that produced approvals.
\`\`\`

In this hypothetical scenario, if we found approval rates differ drastically by race (e.g., Group A has a 75% approval rate vs. Group B’s 40% approval rate), that signals a potential fairness issue. Next steps could include applying fairness metrics or adjusting the model inputs.

\`\`\`r
# Imagine we have a predicted model (logistic regression) for loan approval:
model <- glm(loan_approved ~ race + income, data=dt, family=binomial())

# Examine odds ratios:
summary(model)

# If 'race' is a significant predictor even after controlling for income,
# and if that aligns with known biases, we must consider fairness interventions.
\`\`\`

To address these issues, we might:  
- Try removing `race` from the model (if it's a protected attribute),  
- Use a fairness-aware algorithm or constraints that ensure demographic parity or equalised odds,  
- Engage with stakeholders to decide what fairness definition is most appropriate.

**Conclusion:**

Bias, fairness, and equity in data science involve more than technical fixes. They require a holistic approach:
- Identifying bias through careful examination of data and outcomes.
- Applying fairness metrics to quantify disparities.
- Mitigating bias through algorithmic strategies (pre-, in-, or post-processing).
- Ensuring inclusive data practices and consulting stakeholders to align technical measures with social values.

By combining statistical rigour with social awareness, data scientists can help ensure that models work equitably across diverse populations and do not reinforce historical injustices.

[^1]: For comprehensive guidance on fairness metrics and techniques, see the literature in fairness in machine learning, such as "Fairness in Machine Learning: NIPS 2017 Tutorial" by Feldman et al.