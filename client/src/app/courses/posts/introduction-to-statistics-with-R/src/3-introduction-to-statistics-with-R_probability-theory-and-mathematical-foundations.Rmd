---
title: "Introduction to Statistics with R: Probability Theory and Mathematical Foundations"
blurb: "Blue and white mean war"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Probability Theory and Mathematical Foundations

## Probability Spaces

### Sigma-Algebras, Measures, and Probability

Probability theory begins with a rigorous mathematical structure called a **probability space**, which formalises the notion of chance and randomness.

- **Elements of a Probability Space:**
    1. **Sample Space ($\Omega$):**  
       The set of all possible outcomes of an experiment or random process. For example, if you roll a six-sided die, $$\Omega = \{1, 2, 3, 4, 5, 6\}.$$

    2. **Sigma-Algebra ($\mathcal{F}$):**  
       A collection of subsets of $$\Omega$$ that includes the empty set and is closed under complementation and countable unions. This ensures we can talk consistently about events (which are subsets of $$\Omega$$) and their probabilities.  
       Intuitively, a sigma-algebra defines which groups of outcomes are "measurable" or well-defined events for which we want to assign probabilities.

    3. **Probability Measure ($P$):**  
       A function assigning a number in [0,1] to each event in the sigma-algebra, following these axioms:
       - $P(\emptyset) = 0$
       - $P(\Omega) = 1$
       - Countable additivity: For any sequence of disjoint events $A_1, A_2, \ldots$, 
         $$P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i).$$

Together, $(\Omega, \mathcal{F}, P)$ defines a probability space. Every probabilistic statement—like "The probability of rolling a 3 is 1/6" or "There's a 5% chance of rain tomorrow"—derives from this structure.

**Analogy:**  
Think of the sample space as a large box of all possible outcomes. The sigma-algebra defines a set of "labeled drawers" inside that box (events) to which we can attach probabilities. The probability measure is the way we fill these drawers with "weights" that sum to 1 across the entire box.

### Discrete vs. Continuous Random Variables

A **random variable** is a function mapping each outcome in the sample space $$\Omega$$ to a real number. Random variables translate abstract outcomes into concrete, numerical values that we can analyse statistically.

- **Discrete Random Variables:**
    - Take values from a countable set (e.g., integers).
    - Example: The number of heads in 10 coin tosses can be 0, 1, 2, ..., 10.
    - Probability is assigned directly to each possible value. For example, $P(X=3)$.

- **Continuous Random Variables:**
    - Take values from an uncountably infinite set (e.g., all real numbers in an interval).
    - Example: The height of a person could be any value in a range (like 150.732 cm).
    - We cannot assign a positive probability to an exact point. Instead, we define probabilities over intervals, using a probability density function (PDF).

**R Demonstration (Discrete vs. Continuous):**

\`\`\{r discrete-continuous-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)

# Discrete example: A fair die (values 1 to 6)
# Probability for each outcome is 1/6
die_outcomes <- 1:6
die_probs <- rep(1/6, 6)

# Continuous example: A Normal(0,1) random variable
# Probability of exact value is 0, we talk about intervals
p_interval <- pnorm(1, mean=0, sd=1) - pnorm(-1, mean=0, sd=1)

cat("Probability that a die roll is 3:", die_probs[3], "\n")
cat("Probability that a Normal(0,1) variable lies between -1 and 1:", p_interval, "\n")
\`\`\`

Here, the discrete case (die roll) assigns a probability mass to each outcome. The continuous case (normal distribution) deals with probabilities of ranges.

### Cumulative Distribution Functions (CDFs), PDFs, and PMFs

To handle random variables systematically, we use distribution functions. Three key concepts are essential:

1. **Cumulative Distribution Function (CDF):**
    - For any random variable $X$, the CDF is defined as $$F_X(x) = P(X \leq x).$$
    - $F_X(x)$ is non-decreasing, right-continuous, and satisfies $$F_X(-\infty)=0$$ and $$F_X(\infty)=1.$$
    - The CDF fully describes the distribution of $X$. We can find probabilities of intervals by differences in the CDF: $$P(a < X \leq b) = F_X(b)-F_X(a).$$

2. **Probability Mass Function (PMF) for Discrete Variables:**
    - For a discrete random variable, the PMF gives the probability that $X$ equals a specific value: $$p_X(x)=P(X=x).$$
    - Example: For a fair die, $p_X(3)=1/6$.

3. **Probability Density Function (PDF) for Continuous Variables:**
    - For a continuous random variable, the PDF $f_X(x)$ is related to the CDF by differentiation: $$F_X(x)=\int_{-\infty}^{x} f_X(t) dt.$$
    - The PDF is non-negative and integrates to 1. The probability of an interval is the area under the PDF over that interval.
    - Important: $f_X(x)$ is not a probability itself, but a density. The probability of a single point is 0. We must integrate over intervals to get probabilities.

**Analogy:**  
If the CDF is like a map that shows how probability accumulates as you move along the number line, the PMF (for discrete variables) or PDF (for continuous variables) is like the "landscape" of probability at each point. For a discrete variable, you have individual "spikes" (points) of probability. For a continuous variable, you have a smooth curve, and probabilities come from areas under that curve.

**R Demonstration (CDFs, PMFs, and PDFs):**

\`\`\{r distribution-functions-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(999)
library(ggplot2)
library(data.table)

# Discrete example: Binomial(n=10, p=0.3)
x_values <- 0:10
pmf_binomial <- dbinom(x_values, size=10, prob=0.3) # PMF
cdf_binomial <- pbinom(x_values, size=10, prob=0.3) # CDF

cat("PMF at x=3 for Binomial(10,0.3):", pmf_binomial[4], "\n") # P(X=3)
cat("CDF at x=3 for Binomial(10,0.3):", cdf_binomial[4], "\n") # P(X ≤ 3)

# Continuous example: Normal(0,1)
pdf_normal <- dnorm(seq(-3,3,by=0.5), mean=0, sd=1) # PDF values
cdf_normal <- pnorm(seq(-3,3,by=0.5), mean=0, sd=1) # CDF values

cat("P(X ≤ 1) for Normal(0,1):", pnorm(1,0,1), "\n")

# Plot binomial PMF
dt_pmf <- data.table(x=x_values, pmf=pmf_binomial)
ggplot(dt_pmf, aes(x=x, y=pmf)) +
  geom_bar(stat="identity", fill="steelblue", color="white") +
  labs(title="PMF of a Binomial(10,0.3)", x="x", y="P(X=x)")

\`\`\`

In the binomial example:
- The PMF assigns probability mass to each integer from 0 to 10.
- The CDF accumulates these probabilities up to a given value.

For the normal example:
- The PDF describes the shape of the distribution.
- The CDF gives cumulative probabilities. We find probabilities of intervals by subtracting CDF values.

---

**Key Takeaways:**

- **Probability Space:**  
  A probability space $(\Omega, \mathcal{F}, P)$ lays the foundations of probability, ensuring a rigorous framework for measuring uncertainty.

- **Random Variables (Discrete vs. Continuous):**  
  Discrete variables have countable outcomes with a PMF. Continuous variables have uncountably infinite outcomes described by a PDF. Probabilities for continuous variables arise from integrating the PDF over intervals.

- **CDF, PMF, and PDF:**  
  These functions describe how probabilities are distributed. The CDF gives cumulative probabilities, the PMF gives probabilities for discrete outcomes, and the PDF provides densities for continuous variables. Together, they provide a complete picture of a random variable’s behavior.

By understanding probability spaces, the nature of random variables, and the distribution functions that describe them, we establish the groundwork for all subsequent statistical inference and modeling. These concepts let us translate abstract uncertainty into concrete, measurable quantities.


## Core Probability Distributions

### Bernoulli, Binomial, and Multinomial Distributions

Some of the most fundamental probability distributions arise from counting successes or categorising outcomes.

- **Bernoulli Distribution:**
    - Describes a single trial with two possible outcomes, often called "success" ($X=1$) or "failure" ($X=0$).
    - Parameter: $p$, the probability of success.
    - PMF: $$P(X=1)=p,\quad P(X=0)=1-p.$$
    - Example: Flipping a coin once and labelling "heads" as success.

- **Binomial Distribution:**
    - Models the number of successes in $n$ independent Bernoulli trials, each with success probability $p$.
    - Parameters: $(n,p)$.
    - PMF: $$P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}, \quad k=0,1,\ldots,n.$$
    - Example: Number of heads in 10 coin tosses.

- **Multinomial Distribution:**
    - Generalises the binomial to more than two outcomes per trial.
    - Parameters: $(n, p_1,\ldots,p_m)$ where $p_1+\cdots+p_m=1$.
    - PMF: For counts $(k_1,\ldots,k_m)$ with $\sum k_i=n$:
      $$P(X_1=k_1,\ldots,X_m=k_m)=\frac{n!}{k_1!\cdots k_m!}p_1^{k_1}\cdots p_m^{k_m}.$$
    - Example: The distribution of outcomes when rolling a die $n$ times and counting how many times each face appears.

**R Demonstration (Binomial PMF):**

\`\`\{r binomial-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
# PMF of Binomial(10,0.3) at k=3
p_k3 <- dbinom(3, size=10, prob=0.3)
cat("P(X=3) for Binomial(10,0.3):", p_k3, "\n")

# Plot the PMF
x_vals <- 0:10
pmf_vals <- dbinom(x_vals, size=10, prob=0.3)
dt_bin <- data.table::data.table(x=x_vals, pmf=pmf_vals)

ggplot2::ggplot(dt_bin, ggplot2::aes(x=x, y=pmf)) +
  ggplot2::geom_bar(stat="identity", fill="steelblue", color="white") +
  ggplot2::labs(title="Binomial(10,0.3) PMF", x="k", y="P(X=k)")
\`\`\`

### Poisson and Negative Binomial Models

These distributions often arise in counting occurrences over time or space.

- **Poisson Distribution:**
    - Models the count of events occurring independently in a fixed interval with an average rate $\lambda$.
    - Parameter: $\lambda>0$ (mean and variance both equal $\lambda$).
    - PMF: $$P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!},\quad k=0,1,2,\ldots$$
    - Example: Number of emails received in an hour.

- **Negative Binomial Distribution:**
    - Models the number of failures before a specified number of successes in a sequence of independent Bernoulli trials, or equivalently, overdispersed count data.
    - Parameters: $(r, p)$ where $r$ is the number of successes to wait for, and $p$ is success probability.
    - PMF: $$P(X=k)=\binom{k+r-1}{k}(1-p)^k p^r,$$ where $k$ is the number of failures before the $r$-th success.
    - Also interpretable for counts with variance larger than the mean, useful for modelling overdispersed count data.

**R Demonstration (Poisson):**

\`\`\{r poisson-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Probability that a Poisson(2) random variable equals 3
p_poisson_3 <- dpois(3, lambda=2)
cat("P(X=3) for Poisson(2):", p_poisson_3, "\n")
\`\`\`

### Normal, t, Chi-Square, and F-Distributions

These continuous distributions form the backbone of classical inference.

- **Normal (Gaussian) Distribution:**
    - Parameters: mean $$\mu$$ and variance $$\sigma^2$$.
    - PDF: $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).$$
    - Central to the Central Limit Theorem and widely used in inference.

- **Student’s t-Distribution:**
    - Arises when estimating a mean with unknown variance from a small sample.
    - Parameter: degrees of freedom $\nu$.
    - As $\nu$ grows large, $t$-distribution approaches the normal distribution.

- **Chi-Square ($\chi^2$) Distribution:**
    - If $Z_1,\ldots,Z_\nu$ are independent standard normals, then $\sum Z_i^2$ is $\chi^2_\nu$.
    - Used in testing variances, constructing confidence intervals for variances, and forming ANOVA distributions.

- **F-Distribution:**
    - Ratio of two scaled chi-square variables: $(\chi^2_{\nu_1}/\nu_1)/(\chi^2_{\nu_2}/\nu_2)$.
    - Used in comparing variances (e.g., ANOVA, regression model comparisons).

**R Demonstration (Normal and t):**

\`\`\{r normal-t-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Probability P(X<1.96) for Normal(0,1)
p_norm <- pnorm(1.96, mean=0, sd=1)
cat("P(Z<1.96) for Z~N(0,1):", p_norm, "\n")

# Compare Normal and t-distribution with df=10
p_t <- pt(1.96, df=10)
cat("P(T<1.96) for T~t(10):", p_t, "\n")
\`\`\`

The t-distribution has heavier tails, giving a larger probability for values far from the mean compared to the normal distribution with the same mean and variance.

### Exponential Family of Distributions

The **exponential family** encompasses a wide range of distributions, including Normal, Binomial, Poisson, Gamma, and others. They share a common form:
$$f_X(x|\theta)=h(x)\exp(\eta(\theta)T(x)-A(\theta)),$$
where $T(x)$ is a sufficient statistic and $A(\theta)$ ensures normalization.

- **Why Exponential Families?**
    - They have convenient mathematical properties that simplify inference, often leading to sufficient statistics and conjugate priors in Bayesian analysis.
    - Common distributions in classical statistics (Normal, Bernoulli/Binomial, Poisson, Gamma) are all part of this family, making theoretical results and computations more tractable.

- **Sufficient Statistics and Simplified Inference:**
    - For exponential family distributions, a low-dimensional sufficient statistic captures all the information about a parameter from the sample.
    - This underlies many of the classical inference procedures and allows neat closed-form solutions for maximum likelihood estimation.

**R Demonstration (Exponential Family Concept):**  
While we won’t derive exponential family properties here, consider the Binomial distribution as an example. The sufficient statistic is the total count of successes. This single number encapsulates all relevant information about $p$ from the sample.

\`\`\{r expfam-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Example: Binomial n=10, p=0.3
x_vals <- 0:10
pmf_vals <- dbinom(x_vals, size=10, prob=0.3)
data.table::data.table(x=x_vals, pmf=pmf_vals)
\`\`\`

The sufficient statistic here is simply $X=\sum_{i=1}^{n} X_i$, the total number of successes.

---

**Key Takeaways:**

- **Bernoulli, Binomial, and Multinomial:**  
  Fundamental discrete distributions that model counts of successes (binary or multiple categories) in fixed numbers of trials.

- **Poisson and Negative Binomial:**  
  Key models for count data, where Poisson handles event counts over intervals and Negative Binomial deals with overdispersion or the number of failures before a fixed number of successes.

- **Normal, t, Chi-Square, and F-Distributions:**  
  Central to classical inference methods. The normal distribution underpins the central limit theorem, while t, chi-square, and F arise naturally in inference about means, variances, and linear models.

- **Exponential Family of Distributions:**  
  A unifying framework that includes many common distributions, simplifying inference and offering elegant theoretical properties.

These core probability distributions form the building blocks of statistical modelling and inference. Understanding their properties, parameters, and usage paves the way for selecting appropriate models in real-world data analysis. As you advance, you’ll leverage these distributions in hypothesis testing, regression, Bayesian updating, and beyond.

## Transformations and Expectations

### Expectation, Variance, and Higher Moments

The behaviour of random variables is often summarised using moments. Moments provide concise numerical summaries that capture aspects of a distribution’s location, spread, and shape.

- **Expectation (Mean):**  
  For a random variable $X$, the expectation (or mean) is defined as:
  $$E[X]=\begin{cases}
  \sum_x x p_X(x) & \text{(discrete)} \\[6pt]
  \int_{-\infty}^{\infty} x f_X(x)\,dx & \text{(continuous)}
  \end{cases}$$
  
  Intuitively, $E[X]$ represents the "long-run average" outcome if the random process were repeated many times.

- **Variance:**  
  Variance quantifies how spread out the random variable is around its mean:
  $$\text{Var}(X)=E[(X-E[X])^2]=E[X^2]- (E[X])^2.$$
  
  A small variance indicates that $X$ is typically close to its mean, while a large variance suggests that $X$ can deviate significantly from it.

- **Higher Moments (Skewness and Kurtosis):**  
  - **Skewness:** Measures asymmetry. If skewness is zero, the distribution is symmetric. Positive skewness indicates a longer right tail; negative skewness indicates a longer left tail.
  - **Kurtosis:** Measures the heaviness of the tails compared to the normal distribution. High kurtosis indicates fat tails and possibly more extreme outcomes.

**R Demonstration (Computing Moments):**

\`\`\{r moments-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
x <- rnorm(1000, mean=10, sd=2)
mean_x <- mean(x)
var_x <- var(x)
skew_x <- mean((x - mean_x)^3)/var_x^(3/2)
kurt_x <- mean((x - mean_x)^4)/var_x^2 - 3 # excess kurtosis

cat("Mean:", mean_x, "\nVariance:", var_x, "\nSkewness:", skew_x, "\nKurtosis:", kurt_x, "\n")
\`\`\`

### Moment Generating Functions (MGFs), Characteristic Functions

To handle moments systematically and to derive various properties, statisticians use functions like MGFs and characteristic functions.

- **Moment Generating Function (MGF):**
  Defined as $M_X(t)=E[e^{tX}]$ if it exists.  
  MGFs allow you to recover all moments by differentiation:
  $$E[X^k]=M_X^{(k)}(0),$$
  the $k$-th derivative of $M_X(t)$ evaluated at $0$.

- **Characteristic Function:**
  Defined as $\phi_X(t)=E[e^{itX}]$ where $i=\sqrt{-1}$.  
  Characteristic functions always exist (even when MGFs do not) and uniquely determine the distribution. They are invaluable in proving limit theorems and other theoretical results.

**Analogy:**  
Think of the MGF or characteristic function as a "fingerprint" of a random variable’s distribution. From these functions, you can derive properties like means, variances, and even identify the distribution.

### Transformations of Random Variables

We often deal with functions of random variables. If $Y=g(X)$, how do we find $Y$’s distribution?

- **Monotone Transformations:**  
  If $g$ is monotonic and continuous, we can find the PDF of $Y$ by a change of variables:
  $$f_Y(y)=f_X(g^{-1}(y))\left|\frac{d}{dy}(g^{-1}(y))\right|.$$

- **Non-Monotone Transformations:**
  For more complex transformations, we might need to split the range into regions where $g$ is invertible or use joint distributions and integrate out variables.

- **Examples:**
  - If $X$ is Normal($\mu,\sigma^2$), then $Z=\frac{X-\mu}{\sigma}$ is Standard Normal(0,1).
  - If $X$ is Uniform(0,1), then $Y=-\log(X)$ is Exponential(1).

**R Demonstration (Transformation):**

\`\`\{r transformation-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(321)
u <- runif(1000, 0, 1)
y <- -log(u) # If U~Unif(0,1), Y=-log(U) ~ Exp(1)
mean_y <- mean(y)
cat("Sample mean of Y:", mean_y, "\n") # close to 1, mean of Exp(1) is 1
\`\`\`

This example shows how a simple transformation can produce a well-known distribution from a uniform random variable.

### Order Statistics

Order statistics provide insight into the distribution of sorted random samples. If we have a sample $X_1,\ldots,X_n$ from a continuous distribution and sort them as $X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}$, these $X_{(k)}$’s are order statistics.

- **Properties:**
  - The $k$-th order statistic $X_{(k)}$ has a known distribution related to the Beta distribution if $X_i$ are i.i.d. Uniform(0,1).
  - More generally, distributions of order statistics are used in forming nonparametric confidence intervals (e.g., for medians), robustness studies, and extreme value analysis.

- **Example:**
  For i.i.d. Uniform(0,1) samples, $E[X_{(k)}]=\frac{k}{n+1}$. This provides a neat way to estimate quantiles.

**R Demonstration (Order Statistics):**

\`\`\{r order-statistics-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(999)
samples <- runif(1000, 0, 1)
sorted_samples <- sort(samples)
median_val <- median(sorted_samples)
cat("Sample median (order statistic):", median_val, "\n")
\`\`\`

Here the median is the $\frac{n+1}{2}$-th order statistic. For large samples, it approximates the 0.5 quantile of the Uniform(0,1) distribution, which should be near 0.5.

---

**Key Takeaways:**

- **Moments (Mean, Variance, Higher Moments):**  
  These summarise crucial features of distributions. The mean indicates location, variance describes spread, skewness and kurtosis reveal shape.

- **MGFs and Characteristic Functions:**  
  These functions encode all moments and provide a powerful tool for distribution identification, derivation of properties, and proving limit theorems.

- **Transformations of Random Variables:**  
  Changing variables is common. With monotonic transformations, we can derive the PDF of the transformed variable. With more complex transformations, careful methods are required.

- **Order Statistics:**  
  When we sort sample values, the resulting order statistics have known distributions and are integral to quantile estimation, nonparametric inference, and robustness analysis.

By mastering these concepts—moments, MGFs, transformations, and order statistics—you gain deeper insight into the structure and behaviour of random variables. This understanding prepares you for advanced topics like limit theorems, inference, and statistical modelling techniques that build on these foundational principles.

## Limit Theorems and Asymptotics

As sample sizes grow large, certain elegant results and approximations emerge, allowing us to make powerful statements about estimators, test statistics, and even complex transformations of data. These results—collectively known as **limit theorems**—bridge the gap between finite-sample behaviour and asymptotic (large-sample) properties, providing a foundation for many statistical inference procedures.

### Law of Large Numbers

The **Law of Large Numbers (LLN)** formalises the intuitive idea that as we average more and more observations, the sample mean approaches the true population mean.

- **Weak Law of Large Numbers:**
    - If $X_1, X_2, \ldots, X_n$ are i.i.d. random variables with finite expectation $E[X_i]=\mu$, then:
      $$\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{p} \mu \quad \text{as } n\to\infty.$$
    - "$\xrightarrow{p}$" means convergence in probability, i.e., for any $\epsilon>0$, $P(|\bar{X}_n - \mu|>\epsilon)\to 0$ as $n$ grows.

- **Intuition:**
    - Averaging a small sample may yield a value far from $\mu$, but as $n$ becomes large, the average stabilises around $\mu$.
    - This underpins the idea that sample means are good estimators of population means if we collect enough data.

**R Demonstration (LLN):**

\`\`\{r lln-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
n_values <- seq(100, 10000, by=500)
means <- sapply(n_values, function(n) {
  x <- rnorm(n, mean=5, sd=2)
  mean(x)
})

dt_lln <- data.table::data.table(n=n_values, sample_mean=means)
ggplot2::ggplot(dt_lln, ggplot2::aes(x=n, y=sample_mean)) +
  ggplot2::geom_line(color="blue") +
  ggplot2::geom_hline(yintercept=5, linetype="dashed", color="red") +
  ggplot2::labs(title="Illustration of the Law of Large Numbers",
                x="Sample Size (n)", 
                y="Sample Mean",
                subtitle="As n grows large, the sample mean approaches 5")
\`\`\`

As $n$ increases, the sample mean hovers closer to the true mean (5).

### Central Limit Theorem (Classical and Lindeberg Conditions)

The **Central Limit Theorem (CLT)** is one of the most important results in probability and statistics. It states that sums or averages of many independent, identically distributed (i.i.d.) random variables with finite mean and variance tend to follow a normal distribution, regardless of the original distribution’s shape.

- **Classical CLT:**
    - If $X_1,\ldots,X_n$ are i.i.d. with mean $\mu$ and variance $\sigma^2<\infty$, then:
      $$Z_n=\frac{\sum_{i=1}^{n}(X_i-\mu)}{\sigma\sqrt{n}}\xrightarrow{d}N(0,1) \quad \text{as } n\to\infty.$$
    - "$\xrightarrow{d}$" denotes convergence in distribution. In other words, $Z_n$ approaches a standard normal distribution.

- **Lindeberg CLT:**
    - A more general condition that relaxes the identical distribution assumption, requiring a certain "Lindeberg condition." This ensures that no single observation dominates the sum and that small parts of the data don’t carry disproportionate probability mass.
    - Under these conditions, even if $X_i$ are not identically distributed, their normalized sum still converges in distribution to a normal.

**R Demonstration (CLT):**

\`\`\{r clt-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(456)
n <- 1000
m <- 10000 # number of simulations
x_matrix <- matrix(rexp(n*m, rate=1), nrow=m, ncol=n) # Exponential(1) has mean=1, var=1
means <- rowMeans(x_matrix)
Z <- (means - 1)/(1/sqrt(n)) # standardize

dt_clt <- data.table::data.table(Z=Z)
ggplot2::ggplot(dt_clt, ggplot2::aes(x=Z)) +
  ggplot2::geom_histogram(aes(y=..density..), bins=50, fill="steelblue", color="white") +
  ggplot2::stat_function(fun=dnorm, args=list(mean=0, sd=1), color="red", size=1) +
  ggplot2::labs(title="Central Limit Theorem Demonstration",
                x="Standardized Means",
                y="Density",
                subtitle="As n=1000, the distribution of standardized means approaches Normal(0,1)")
\`\`\`

The histogram of standardized means (from exponential data) closely matches the red normal curve.

### Delta Method for Approximation

The **Delta Method** provides a way to approximate the distribution of a smooth transformation of an asymptotically normal estimator. If $\sqrt{n}(T_n-\theta)\xrightarrow{d}N(0,\sigma^2)$, then for a differentiable function $g$ with $g'(\theta)\neq 0$:
$$\sqrt{n}(g(T_n)-g(\theta))\xrightarrow{d}N(0,\sigma^2[g'(\theta)]^2).$$

- **Intuition:**
    - Start with an estimator $T_n$ that is asymptotically normal.
    - If we apply a smooth function $g$ to $T_n$, the transformed estimator $g(T_n)$ is also asymptotically normal.
    - The variance adjusts by the square of the derivative of $g$ at the true parameter $\theta$.

- **Example:**
    - Suppose $T_n$ estimates a mean $\mu$. Then $\sqrt{n}(T_n-\mu)\to N(0,\sigma^2)$.
    - For $g(\mu)=\log(\mu)$, the asymptotic variance of $\log(T_n)$ is $\sigma^2/\mu^2$.

**R Demonstration (Delta Method Concept):**

\`\`\{r delta-method-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Example (not numerically shown): If T_n ~ Normal(mu, sigma^2/n) for large n,
# g(mu)=log(mu), g'(mu)=1/mu.
# Asymptotic variance of log(T_n) ~ sigma^2/(n*mu^2).
# This is conceptual only.
cat("Delta Method conceptual demonstration\n")
\`\`\`

### Advanced Asymptotics (Slutsky’s Theorem, Continuous Mapping Theorem)

These theorems allow us to extend convergence results from basic forms to more complex transformations and combinations of random variables.

- **Slutsky’s Theorem:**
    - If $X_n\xrightarrow{d}X$ and $Y_n\xrightarrow{p}c$, a constant, then:
      - $X_n+Y_n\xrightarrow{d}X+c$
      - $X_nY_n\xrightarrow{d}Xc$
    - This theorem helps combine convergences. For example, if an estimator converges in distribution and another converges in probability to a constant, their sum/product/quotient behaves predictably.

- **Continuous Mapping Theorem:**
    - If $X_n\xrightarrow{d}X$ and $g$ is a continuous function, then $g(X_n)\xrightarrow{d}g(X)$.
    - Extends convergence results: once we know $X_n$ converges in distribution, any continuous transformation of $X_n$ also converges in distribution to the transformed limit.

**Analogy:**
- Think of Slutsky’s and the Continuous Mapping Theorem as "bridge builders." They let you move from convergence results about simple random variables to convergence results about complex expressions involving those variables. If you know how $X_n$ behaves asymptotically, you can deduce how $g(X_n)$ or $X_n+Y_n$ behaves without starting from scratch.

**R Demonstration (Concept):**
  
\`\`\{r advanced-asymptotics-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Suppose X_n ~ N(0,1) and we know X_n -> N(0,1)
# Let g(x)=x^2. g is continuous.
# By Continuous Mapping Theorem, X_n^2 converges in distribution to X^2.
# This is conceptual; no numeric demonstration needed.
cat("Continuous Mapping Theorem conceptual demonstration\n")
\`\`\`

---

**Key Takeaways:**

- **Law of Large Numbers:**  
  Sample averages converge to the true mean as the sample size grows large.

- **Central Limit Theorem:**  
  Normal approximation emerges for sums/averages of i.i.d. random variables, enabling normal-based inference from a broad class of distributions.

- **Delta Method:**  
  For asymptotically normal estimators, smooth transformations remain asymptotically normal with adjusted variance.

- **Advanced Asymptotics (Slutsky’s and Continuous Mapping Theorem):**  
  These theorems extend convergence results to combinations and transformations, ensuring that once you know how a sequence converges, you can easily deduce the convergence of more complex functions of that sequence.

These asymptotic results form the backbone of many inference techniques. They justify using normal-based confidence intervals and hypothesis tests, underpin the bootstrap and other resampling methods, and guide the development of advanced statistical modeling approaches as sample sizes become large.