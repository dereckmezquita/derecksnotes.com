---
title: "Introduction to Statistics with R: Introduction and Overview"
blurb: "File war tea"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Introduction and Overview

## Setting the Stage

### Motivation for Studying Statistics

Statistics occupies a crucial niche in the modern world, acting as the bridge between raw, noisy data and meaningful insights. In an era where data is ubiquitous—streaming from sensors, collected from surveys, generated by simulations—understanding the principles of statistics allows us to extract patterns, quantify uncertainties, and ultimately make evidence-based decisions.

- **From Data to Knowledge:**
  Raw data can be chaotic and overwhelming. Statistical techniques provide the tools to summarise this data into comprehensible measures like means, medians, and standard deviations. More importantly, they help us understand whether observed patterns are genuine signals or mere coincidences.

- **Quantifying Uncertainty:**
  The world is inherently uncertain. Two identical experiments rarely yield exactly the same results, and the underlying reasons may be complex and multifaceted. Statistics provides a rigorous language—probabilities, confidence intervals, p-values—to express this uncertainty. This ensures that conclusions are not only drawn but also accompanied by estimates of how confidently they are held.

- **Driving Decision-Making in Diverse Domains:**
  Whether evaluating the effectiveness of a new drug, optimising a financial portfolio, assessing environmental risks, or improving business strategies, statistics is the backbone of rational decision-making. It ensures that our inferences are not just intuitive leaps but are grounded in quantitative reasoning.

**Analogy:** Imagine you have a blurry photograph (data). Statistics is like a set of lenses and filters that sharpen the image, reveal hidden details, and help you understand what you are seeing. Without statistical methods, you might jump to misleading conclusions or fail to notice critical patterns.

[^1]: For instance, large-scale clinical trials rely extensively on statistical methods to determine the efficacy and safety of treatments before they reach the market.

\`\`\{r motivation-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
daily_sales <- data.table::data.table(day=1:30, sales=rpois(30, lambda=20))
sales_mean <- mean(daily_sales$sales)
sales_sd <- sd(daily_sales$sales)

cat("Average Daily Sales:", sales_mean, "\n")
cat("Standard Deviation of Sales:", sales_sd, "\n")

# Visualisation to understand distribution of sales
ggplot2::ggplot(daily_sales, ggplot2::aes(x=sales)) +
  ggplot2::geom_histogram(binwidth=1, fill="steelblue", color="white") +
  ggplot2::labs(title="Distribution of Daily Sales", x="Sales", y="Frequency")
\`\`\`

In this simple demonstration, summarising and visualising a dataset quickly reveals its central tendencies and variability—an essential first step in statistical thinking.

### Interdisciplinary Nature of Statistical Methods

One of the greatest strengths of statistics is its universality. It is not confined to a single field or type of data. Instead, statistical reasoning weaves through every branch of science, industry, and policy.

- **Medical and Biological Sciences:**
  Determining if a new vaccine reduces infection rates, analysing genomic data to find disease markers, evaluating the outcomes of a treatment strategy—all rely on statistical inference to separate real effects from random variation.

- **Physical and Environmental Sciences:**
  Assessing climate trends, modelling seismic risks, or interpreting results from high-energy physics experiments—statistics provides the framework to handle noisy and complex data, ensuring conclusions are robust rather than speculative.

- **Social Sciences and Humanities:**
  Understanding social phenomena like voting patterns, cultural shifts, or economic indicators involves sampling large populations, dealing with messy observational data, and drawing cautious conclusions. Statistics is the language that allows researchers in these domains to test theories and validate findings rigorously.

- **Business, Finance, and Industry:**
  Companies use statistical models to forecast demand, assess financial risks, optimise manufacturing quality, and test new market strategies. The competitiveness of modern industries often hinges on the subtle statistical insights derived from their data.

By mastering statistical principles, you gain a toolkit that applies to virtually any domain where uncertainty and complexity abound. It enables interdisciplinary collaboration because different fields can use the same statistical techniques to answer their unique questions.

\`\`\{r interdisciplinary-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Demonstration: basic summary of two distinct datasets
# For medical data: patient weights (in kg)
patient_data <- data.table::data.table(patient_id=1:10, weight=rnorm(10, mean=70, sd=5))
cat("Mean patient weight (medical domain):", mean(patient_data$weight), "\n")

# For environmental data: daily temperatures (in °C)
temp_data <- data.table::data.table(day=1:10, temperature=rnorm(10, mean=15, sd=3))
cat("Mean daily temperature (environmental domain):", mean(temp_data$temperature), "\n")
\`\`\`

Regardless of context—medical or environmental—the fundamental statistical operations (calculating means, examining distributions) remain the same.

### Statistical Thinking and the Scientific Method

The scientific method—observe, hypothesise, experiment, analyse, and conclude—relies critically on statistical thinking:

1. **Observation:**
   Collecting data is only the beginning. You might gather measurements, record events, or conduct a survey. However, raw data often contains noise and uncertainty.

2. **Hypothesis Formation:**
   Based on observations, you form a hypothesis: a tentative explanation or prediction. For example, "This new fertilizer increases plant height."

3. **Data Collection and Experimentation:**
   You design experiments or observational studies to test the hypothesis. Randomisation, replication, and control groups ensure that the data collected can truly reflect cause-and-effect relationships when interpreted statistically.

4. **Analysis and Interpretation (The Statistical Part):**
   Statistical methods help determine if the observed patterns are significant or due to random fluctuations. Hypothesis testing, confidence intervals, Bayesian inference, or model-based approaches all serve to quantify the evidence.

5. **Conclusion and Iteration:**
   Based on the statistical analysis, you either gain confidence in the hypothesis, reject it, or refine your understanding. The cycle repeats, steadily improving the reliability of scientific knowledge.

Statistics ensures that scientific claims are not just guesses, but statements accompanied by measures of certainty. It guards against being misled by coincidences or outliers, providing a framework to discern real effects from the noise of randomness.

\`\`\{r scientific-method-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(42)
# Heights without fertilizer (control)
control <- rnorm(30, mean=50, sd=5)
# Heights with fertilizer (treatment)
treatment <- rnorm(30, mean=52, sd=5)

t_test_result <- t.test(treatment, control, var.equal=TRUE)
cat("t-test p-value:", t_test_result$p.value, "\n")
\`\`\`

If the p-value is sufficiently small, it suggests that the difference in mean heights is unlikely due to mere chance. This aligns perfectly with the scientific method: we had a hypothesis that the fertilizer increases height, we collected data, performed a statistical test, and now can make an informed conclusion.

---

**Key Takeaways:**

- **Motivation for Studying Statistics:**
  Statistics turns messy data into clear insights. It quantifies uncertainty and ensures that our conclusions are more than hunches.

- **Interdisciplinary Nature:**
  No matter your field—medicine, physics, social sciences, or business—statistical reasoning is a universal tool that enhances your ability to interpret data and make sound decisions.

- **Statistical Thinking and the Scientific Method:**
  Science advances through carefully designed experiments and honest assessment of uncertainty. Statistics provides the structure to confirm or refute hypotheses rigorously, ensuring that scientific progress rests on a solid, quantitative foundation.

By setting this stage, we establish why statistics is indispensable and how it fits into the grand scheme of knowledge generation. We now stand ready to delve deeper into the fundamentals, exploring probability theory, modelling techniques, and the rich tapestry of methods that will transform us into confident data analysts and researchers.


## Course Aims and Objectives

### Intended Audience (Graduate, Doctoral, Postdoc)

This course is designed for learners at the graduate, doctoral, and postdoctoral levels who seek a deep, research-oriented understanding of statistical principles and methods. The content is tailored to those who:

- **Already Possess Foundational Knowledge:**
  You should be comfortable with basic probability, introductory statistics, and elementary R skills. While we will review some fundamentals, the course moves rapidly into advanced territories.

- **Engage in Research and Data Analysis:**
  Graduate students preparing theses, doctoral candidates looking to rigorously model complex data, and postdoctoral researchers aiming to publish in top-tier journals will all benefit. The course addresses the sophisticated statistical challenges researchers routinely face: complex data structures, high-dimensional settings, evolving computational tools, and intricate inferential problems.

- **Work Across Disciplines:**
  Whether your domain is biomedical research, economics, engineering, social sciences, environmental studies, or another data-driven field, this course provides universal tools for rigorous statistical inference. By mastering these advanced methods, you will be empowered to tackle highly diverse data problems and communicate your findings to multidisciplinary audiences.

### Skills and Competencies to be Developed

Upon completing this course, you will have honed a suite of both theoretical and practical skills. Expect to develop competence in:

- **Theoretical Underpinnings of Statistics:**
  Understand the axiomatic foundations of probability, explore limit theorems and asymptotic behaviour, and dissect the assumptions behind statistical models. You will not just use methods but also know why they work.

- **Advanced Statistical Modelling and Inference:**
  From classic linear models to generalised linear models, hierarchical models, nonparametric and semiparametric methods, and cutting-edge Bayesian computation techniques, you will learn how to select, fit, interpret, and critically evaluate a range of sophisticated statistical models.

- **Computational Proficiency in R:**
  Hone your R skills, placing special emphasis on efficient data manipulation with `data.table`, effective visualisation with `ggplot2`, and reproducible workflows using parameterised reports and version control. The course emphasises code optimisation, memory management, and parallel computing strategies.

- **Critical Thinking and Communication:**
  Develop the ability to critically assess model assumptions, diagnose estimation problems, and interpret results in the context of real-world questions. Learn to present findings with clarity, justify methodological choices, and convey uncertainty transparently to both technical and non-technical audiences.

**Analogy:**
Think of your current analytical toolkit as a small backpack with basic tools. This course will transform that backpack into a full mountaineer’s kit—equipped with advanced gear, detailed maps (theory), navigation instruments (computational skills), and the experience needed to traverse complex analytical terrains.

### Assessment Structure and Final Project Expectations

**Assessments will be multi-faceted, ensuring both depth and breadth:**

1. **Regular Problem Sets:**
   - **Theoretical Exercises:**
        Expect proofs and derivations that solidify your understanding of asymptotic results, estimator properties, and the behaviour of complex distributions.
 
    - **Applied Tasks:**
        Use R for simulation studies, model fitting, and exploratory analyses. For instance, you may simulate data from a given distribution, fit a regression model, and assess convergence diagnostics. Another example might involve performing Bayesian updating using MCMC methods and comparing posterior summaries.

2. **Midterm Examination:**
   - **Breadth Coverage:**
        The midterm will test intermediate concepts—probability foundations, basic inference techniques, and initial modelling approaches. It may combine short-answer theoretical questions with simple R-based analysis tasks.

3. **Final Examination:**
   - **Integrative and Challenging:**
        The final exam expects you to integrate multiple course components. For example, you might need to derive asymptotic distributions of estimators, apply the delta method to transform parameters, or interpret output from a complex hierarchical model. There may be short-answer questions, as well as data analysis problems to be solved under exam conditions.

4. **Final Project:**
   - **Research-Like Analysis:**
        The culminating experience is a comprehensive project simulating a real research scenario. You will:
     - Identify a research question from your domain of interest.
     - Obtain a suitable dataset (either publicly available or simulated).
     - Perform data cleaning and exploratory analysis using `data.table` for fast, efficient handling of datasets.
     - Fit appropriate models—maybe a generalised linear model, a Bayesian hierarchical model, or a high-dimensional regularised model—depending on your question.
     - Validate model assumptions, conduct diagnostics, and evaluate model fit.
     - Use simulation or resampling methods (e.g., bootstrapping) to gauge uncertainty.
     - Produce a polished R Markdown or Quarto report, integrating text, code (with backslash-escaped backticks), figures, and interpretive paragraphs.
     - Communicate your findings as if you were preparing a manuscript section or a conference presentation.

   - **Peer Review and Iteration:**
        You may receive peer feedback, encouraging you to refine your work. This simulates academic peer review or industry team collaboration, pushing you to polish both methodology and communication.

**R Demonstration (Project-Style Example):**

\`\`\{r project-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(100)

# Example: Suppose we are interested in modelling the relationship between temperature and crop yield.
# We simulate a dataset:
n <- 200
temp <- runif(n, 10, 30) # Temperature from 10°C to 30°C
yield <- 50 + 2*temp + rnorm(n, 0, 5) # Linear relationship with noise

data_proj <- data.table::data.table(temp=temp, yield=yield)

# Fit a linear model to estimate the effect of temperature on yield
model <- lm(yield ~ temp, data=data_proj)
summary(model)

# Visualise the relationship
ggplot2::ggplot(data_proj, ggplot2::aes(x=temp, y=yield)) +
  ggplot2::geom_point(color="blue") +
  ggplot2::geom_smooth(method="lm", color="red") +
  ggplot2::labs(title="Crop Yield vs Temperature", x="Temperature (°C)", y="Yield (kg)")
\`\`\`

In this brief demonstration:
- We simulate data akin to a real research scenario.
- Fit a model (linear regression).
- Assess the results and produce a diagnostic plot.

Your final project would be more involved, perhaps requiring model selection, diagnostics for residual patterns, Bayesian reanalysis, or the application of advanced inference techniques.

---

**Key Takeaways:**
- The course is aimed at advanced learners who need robust, research-level statistical knowledge.
- You will develop both deep theoretical insights and hands-on computational expertise.
- Assessments are designed to ensure mastery: from problem sets fostering conceptual understanding, through exams testing integrative knowledge, to a final project simulating genuine research analysis.
- By the end, you will be well-equipped to navigate complex statistical landscapes, conduct cutting-edge analyses, and communicate your findings with authority and clarity.

## Why Use R?

### The Philosophy Behind R

R is more than just a programming language; it embodies a philosophy rooted in openness, adaptability, and community-driven development. Originally conceived as a dialect of the S language, R has evolved into a powerful, flexible environment for statistical computing and data analysis. At its core, R embraces the following ideals:

- **By Statisticians, for Statisticians:**
  R was developed by statisticians, ensuring that data analysis tasks feel natural and intuitive. Tasks such as fitting models, exploring distributions, and performing simulations are central, not afterthoughts.

- **Open Source and Customisable:**
  R is available at no cost and is open source. You can inspect its code, modify it, and extend it. This freedom fuels innovation, allowing researchers to implement cutting-edge methods and share them as packages.

- **Integration with Statistical Theory:**
  R’s design encourages users to think statistically. Complex statistical concepts—from simple linear regression to advanced Bayesian models—are implemented in a manner consistent with their theoretical underpinnings.

**Analogy:** Imagine a bespoke craftsman’s workshop: every tool you need for statistical inquiry is at your fingertips, and if you can’t find the perfect tool, you’re free to craft your own. That’s R—a customisable workshop for data-driven research.

### R as an Ecosystem for Statistical Computing

R is not merely a language; it’s a vast and ever-growing ecosystem of packages, tools, and frameworks. Some of the key components include:

- **CRAN (The Comprehensive R Archive Network):**
  Thousands of packages cover an astonishing range of methods, from classical models (linear regression, ANOVA) to the most advanced machine learning techniques, causal inference methods, and beyond. New research ideas often appear first as R packages.

- **Specialised Domain Packages:**
  Whether you’re working in genomics (`data.table::fread()` combined with bioinformatics packages), finance (time-series forecasting methods), or social sciences (survey analysis), you’ll find well-tested solutions ready to implement.

- **Graphics and Visualisation:**
  Tools like `ggplot2` allow for elegant, layered plots based on the Grammar of Graphics. This encourages clear, reproducible, and aesthetically pleasing data visualisations.

- **Seamless Integration:**
  R interfaces easily with other languages (C/C++, Python) and platforms (databases, cloud environments), ensuring that you can optimise performance or tap into specialised ecosystems without leaving the comfort of R’s environment.

**R Demonstration (Basic Data Manipulation and Plot):**
Below is a simple demonstration illustrating how R’s ecosystem supports quick data manipulation and visualisation. Note the use of `data.table` for fast data handling and `ggplot2` for polished plots.

\`\`\{r why-use-r-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Simulate some data: Suppose we have monthly sales figures
set.seed(456)
sales_data <- data.table::data.table(
  month=1:12,
  sales=rpois(12, lambda=20)
)

# Compute summary statistics using data.table:
avg_sales <- mean(sales_data$sales)
sd_sales <- sd(sales_data$sales)

cat("Average Monthly Sales:", avg_sales, "\n")
cat("Standard Deviation of Monthly Sales:", sd_sales, "\n")

# Visualise sales across months using ggplot2
ggplot2::ggplot(sales_data, ggplot2::aes(x=month, y=sales)) +
  ggplot2::geom_line(color="blue") +
  ggplot2::geom_point(color="red") +
  ggplot2::labs(
    title="Monthly Sales Over a Year",
    x="Month",
    y="Sales"
  )
\`\`\`

This brief snippet shows how natural it is to load data, summarise it, and create a professional-quality plot—all in a few lines.

### Reproducibility, Open Science, and the R Community

R’s openness and extensive community support form the bedrock of reproducible research and open science. The language and its tools encourage best practices that ensure your analyses are transparent, sharable, and verifiable.

- **Literate Programming Tools:**
  RMarkdown and Quarto integrate code, output, and narrative in a single document. This allows you to generate dynamic reports where figures, tables, and results are automatically updated whenever the data or code changes.

- **Version Control and Collaboration:**
  R projects integrate well with Git and GitHub, promoting collaborative workflows, version-tracking, and peer review of analytical code.

- **Package Ecosystem for Reproducibility:**
  Tools like `packrat` or `renv` help freeze package dependencies, ensuring that analyses can be reproduced even after package versions evolve.

- **Vibrant Community and Knowledge Sharing:**
  The R community thrives on knowledge exchange. Through mailing lists, user groups, R-Ladies chapters, conferences (like useR!), and forums, users at all skill levels share advice, troubleshoot problems, and collectively push the language forward. This communal spirit fosters continuous improvement and keeps R at the cutting edge of statistical innovation.

**Analogy:** Think of R as a collaborative laboratory bench in an open-access research hub. Everyone contributes their latest instruments (packages), shares notes (vignettes, tutorials), and supports each other in refining experimental methods (statistical techniques). This communal environment accelerates scientific progress and ensures no analysis is locked behind proprietary barriers.

**R Demonstration (Reproducible Reporting Example):**
In a real research scenario, you might combine code, output, and commentary in a single RMarkdown file. Below is a conceptual snippet (not fully executed here) showing how you could dynamically generate a summary report:

\`\`\{r reproducibility-demo, echo=TRUE\}
# Example code chunk in an RMarkdown file
summary_stats <- data.table::data.table(
  Statistic=c("Mean","Median","SD"),
  Value=c(mean(sales_data$sales), median(sales_data$sales), sd(sales_data$sales))
)

summary_stats
\`\`\`

By knitting this RMarkdown document, you produce a report with tables, plots, and text all integrated. Any update to the data or code instantly reflects in the final rendered document, bolstering trust in the reproducibility of your analysis.

---

**Key Takeaways:**

- **The Philosophy Behind R:**
  R is designed with statisticians’ needs in mind, promoting intuitive data analysis and welcoming innovation through its open-source nature.

- **R as an Ecosystem:**
  Beyond a language, R is an expansive ecosystem of packages, tools, and frameworks supporting every aspect of modern data science—from basic summaries to high-end machine learning and Bayesian computation.

- **Reproducibility, Open Science, and Community:**
  The R community embraces best practices for reproducible research. Open access to packages, forums, and literate programming tools ensure that your analyses are transparent, verifiable, and easy to share. This shared ethos accelerates scientific discovery and fosters a spirit of cooperation and continuous improvement.

By choosing R as your analytical platform, you join a global community dedicated to rigorous, open, and reproducible statistical practice. The stage is set for us to explore how to install, configure, and leverage R’s full potential in this course and beyond.

## Preparatory Work

### Installing and Configuring R and RStudio

Before diving into statistical analyses, it is essential to set up a stable computing environment. R provides the computational backbone for this course, and RStudio offers a user-friendly Integrated Development Environment (IDE) for working with R code, data, visualisations, and reports.

- **Installing R:**
    1. Visit [CRAN](https://cran.r-project.org/) and select the appropriate binary for your operating system (Windows, macOS, or Linux).
    2. Follow the installation prompts. By default, R installs in a standard location. Once installed, you can run R from a terminal or command prompt.

- **Installing RStudio:**
    1. Download RStudio Desktop (free version) from the [RStudio website](https://posit.co/download/rstudio-desktop/).
    2. Run the installer and open RStudio. You’ll see a console pane, a script editor, a workspace/environment pane, and a files/plots/help pane.
  
  - **Basic Configuration:**
    - In RStudio, go to *Tools > Global Options* to adjust preferences.
    - Set a CRAN mirror close to your geographic location for faster package installations.
    - Adjust the Appearance, Code, and Editor settings to suit your preferences. Increase the font size if needed.
    - Install essential packages: you’ll frequently use `data.table` for data manipulation and `ggplot2` for plotting.

**R Demonstration (Initial Setup):**

\`\`\{r setup-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Installing a package (if not installed)
if(!"data.table" %in% installed.packages()[,"Package"]) {
    install.packages("data.table")
}

if(!"ggplot2" %in% installed.packages()[,"Package"]) {
    install.packages("ggplot2")
}

# Test loading:
library(data.table)
library(ggplot2)

cat("R and RStudio are successfully configured!\n")
\`\`\`

### Overview of RMarkdown and Quarto

Reproducible research thrives on tools that integrate code, text, and results in one document. RMarkdown and Quarto facilitate this by allowing you to write documents that contain:

- **Code Chunks:**
  Embed R code inside your document. When rendered, the code runs and its results (tables, figures, computed values) appear in the final output.

- **Narrative and Interpretation:**
  Interleave explanatory text, equations, images, and references with code, ensuring that your thought process, methods, and conclusions are clearly documented alongside the computations.

- **Multiple Output Formats:**
  Knit or render RMarkdown and Quarto documents to various output formats: HTML, PDF, Word, or interactive dashboards. This flexibility ensures your reports can adapt to different audiences—supervisors, collaborators, or even non-technical stakeholders.

**R Demonstration (RMarkdown Conceptual Example):**

\`\`\{r rmarkdown-demo, echo=TRUE\}
# This code chunk might appear in a .Rmd file:
data_example <- data.table(x=1:10, y=rnorm(10, mean=5, sd=2))
mean_val <- mean(data_example$y)

mean_val
\`\`\`

When you knit the `.Rmd` file, the computed `mean_val` and any figures you produce will be embedded in the output, ensuring a self-contained, reproducible analysis.

- **Quarto:**
  A next-generation system similar to RMarkdown, Quarto supports multiple programming languages and advanced publishing options. It offers enhanced flexibility for producing books, websites, presentations, and blogs, integrating seamlessly with R and other languages like Python.

### File Structures, Project Management, and Version Control Basics

As projects grow complex—spanning multiple scripts, data files, and reports—organisation and version control become crucial. Clear directory structures and version control (e.g., Git) ensure that you can:

- **Reproduce Past Results:**
  You can easily retrace the steps of your analysis, run older versions of scripts, or revert to a known stable state.

- **Collaborate Efficiently:**
  Multiple team members can work simultaneously without overwriting each other’s changes. Version control systems track who changed what and when, making collaboration transparent and manageable.

- **Maintain Data Integrity:**
  Keeping raw data separate from intermediate processed data and final results prevents accidental overwrites and ensures clarity about what’s original and what’s derived.

**Recommended Directory Structure:**

Consider a structure like this for a research project: