---
title: "Introduction to Statistics with R: Regression and Modelling"
blurb: "Blue and white mean war"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Regression and Modelling

## Linear Models

Linear models form a cornerstone of statistical modelling, linking a response variable to one or more predictors through linear relationships. This framework underpins countless analyses, from simple trend estimation to complex multi-factor studies. Mastering linear models and their assumptions, diagnostics, and remedies for multicollinearity or influential points is crucial for reliable inference.

### Simple Linear Regression

**Concept:**
- Model a response $Y$ as a linear function of a single predictor $X$:
  $$Y=\beta_0 + \beta_1 X + \varepsilon,$$
  where $\varepsilon$ is a random error with mean 0 and constant variance $\sigma^2$.
- Aim: Estimate $\beta_0$ (intercept) and $\beta_1$ (slope).
- **Least Squares Estimation:**
  - Choose $\hat{\beta}_0, \hat{\beta}_1$ to minimise $\sum (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2$.
- Interpreting $\hat{\beta}_1$: The average change in $Y$ per unit increase in $X$.

**Assumptions:**
- Linearity: The relationship between $X$ and $Y$ is linear.
- Independence: Observations are independent.
- Homoscedasticity: Constant variance of errors.
- Normality: Errors are normally distributed.

When these hold, least squares estimates are unbiased, consistent, and efficient.

**R Demonstration (Simple Linear Regression):**

\`\`\{r simple-lm-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
library(ggplot2)
library(data.table)

x <- runif(50, 0, 10)
y <- 2 + 0.5*x + rnorm(50, sd=1)
dt_lin <- data.table::data.table(x=x, y=y)

lm_res <- lm(y ~ x, data=dt_lin)
summary(lm_res)
\`\`\`

Check the summary to see estimates, standard errors, t-values, and p-values for $\hat{\beta}_0$ and $\hat{\beta}_1$.

### Multiple Regression, Model Assumptions, Diagnostics

**Multiple Linear Regression:**
- Extend simple regression to multiple predictors:
  $$Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon.$$
- Interpret $\beta_j$ as the effect of $X_j$ on $Y$, controlling for other variables.

**Assumptions (same as simple regression, extended):**
- Linearity in parameters.
- Independence of errors.
- Homoscedasticity: Errors have constant variance.
- Normality of errors (for inference).
- No perfect multicollinearity: Predictors should not be perfectly correlated.

**Diagnostics:**
- **Residual vs Fitted Plot:** Check non-linearity or heteroscedasticity.
- **Normal Q-Q Plot:** Assess normality of residuals.
- **Scale-Location Plot:** Check constant variance.
- **Cook’s Distance Plot:** Identify influential observations that unduly affect estimates.

**R Demonstration (Multiple Regression Diagnostics):**

\`\`\{r multiple-lm-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(456)
X1 <- runif(100, 0, 10)
X2 <- runif(100, 0, 5)
Y <- 5 + 1.5*X1 + (-0.8)*X2 + rnorm(100, sd=2)
dt_mul <- data.table::data.table(X1=X1, X2=X2, Y=Y)

lm_mul <- lm(Y ~ X1 + X2, data=dt_mul)
summary(lm_mul)

# Diagnostics
par(mfrow=c(2,2))
plot(lm_mul) # Residual plots, Q-Q plot, Scale-Location, and Cook's distance
par(mfrow=c(1,1))
\`\`\`

Check these plots to ensure assumptions are not severely violated. If residual plots show patterns, consider transformations or adding non-linear terms. If residual variance changes with fitted values, think about weighted regression or robust methods.

### Dealing with Multicollinearity, Influential Points

**Multicollinearity:**
- Occurs when predictors are highly correlated.
- Implications:
  - Coefficients may become unstable and sensitive to small data changes.
  - Standard errors inflate, making it hard to identify significant predictors.

**Detection:**
- Check correlation matrix of predictors.
- Compute Variance Inflation Factors (VIFs): VIF > 5 or 10 often signals problematic multicollinearity.

**Solutions:**
- Drop redundant variables or combine them (e.g., principal component scores).
- Use regularisation methods (Ridge, Lasso) to stabilise estimates.

**Influential Points:**
- Observations that disproportionately affect parameter estimates.
- Identified via Cook’s distance, leverage, or DFBETAS.
- Consider whether to remove, investigate measurement error, or use robust methods.

**R Demonstration (VIF Computation):**

\`\`\{r vif-demo, echo=TRUE, message=FALSE, warning=FALSE\}
library(car) # provides vif()
vif_values <- car::vif(lm_mul)
vif_values
\`\`\`

If any VIF is large, you may consider addressing multicollinearity.

---

**Key Takeaways:**

- **Simple Linear Regression:**  
  Models one predictor and a response linearly, yielding interpretable slope and intercept estimates.

- **Multiple Regression and Diagnostics:**  
  Extends to multiple predictors. Assumptions ensure valid inference. Diagnostics (residual plots, Q-Q plots, Cook’s distance) reveal when assumptions fail or when individual points exert undue influence.

- **Addressing Issues (Multicollinearity, Influential Points):**  
  Check VIFs, consider dropping or combining variables, or switch to regularised methods. Investigate influential observations before discarding them—sometimes they are genuine outliers representing important phenomena.

Linear models remain the workhorses of statistical practice due to their interpretability, computational simplicity, and direct linkage to classical inference. By carefully diagnosing assumption violations and addressing issues like multicollinearity and outliers, you ensure reliable, meaningful results from these foundational tools.

## Generalised Linear Models (GLMs)

Linear models are powerful, but not all response variables are continuous and normally distributed. **Generalised Linear Models (GLMs)** extend linear models to handle a wide range of response distributions and relationships via link functions.

### Logistic Regression, Poisson Regression

**Logistic Regression:**
- Used for binary response variables (e.g., success/failure, yes/no).
- Model the probability of success $p_i=P(Y_i=1)$ as a function of predictors $X_i$:
  $$\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1X_{i1} + \cdots + \beta_pX_{ip},$$
  where the logit link ensures $p_i \in (0,1)$.
- Interpretation: Coefficients represent log-odds changes associated with a unit change in the predictor.

**Poisson Regression:**
- Used for count data (nonnegative integers).
- Model the expected count:
  $$\log(\mu_i) = \beta_0 + \beta_1X_{i1} + \cdots + \beta_pX_{ip},$$
  where $\mu_i=E[Y_i]$ is the mean count, and the log link ensures positivity.
- Useful for modelling rates (counts per unit time or area).

### Link Functions, Deviance, and Overdispersion

**Link Functions:**
- Connect the linear predictor $\eta = X\beta$ to the mean of the response $\mu$.
- Common links:
  - Identity: $\mu=\eta$ (like linear regression)
  - Logit for binary outcomes
  - Log for counts
  - Probit, cloglog, complementary log-log also exist for binary data
- Choosing a link that suits the response distribution’s nature is crucial.

**Deviance:**
- A measure of model fit similar to the residual sum of squares in linear regression.
- Compares the fitted model to the saturated model (a model with a parameter for every observation).
- Smaller deviance indicates a better fit.

**Overdispersion:**
- Occurs when the data show more variability than the model’s assumed distribution can accommodate.
- Common in Poisson regression, where variance equals mean. Real data often have variance > mean.
- Solutions:
  - Quasi-likelihood methods (adjust standard errors)
  - Use a Negative Binomial model if counts are overdispersed
  - Add random effects (generalised linear mixed models)

### Model Selection (AIC, BIC) and Stepwise Methods

**AIC (Akaike Information Criterion)** and **BIC (Bayesian Information Criterion)**:
- Criteria for model comparison, balancing fit and complexity.
- AIC = $-2\log(L) + 2k$, BIC = $-2\log(L) + k\log(n)$, where $L$ is the likelihood and $k$ is number of parameters.
- Lower AIC/BIC suggests a better trade-off of fit to complexity.
- BIC penalises complexity more heavily than AIC.

**Stepwise Methods:**
- Add or remove predictors iteratively based on criteria (AIC or BIC).
- Forward selection: Start with no predictors, add them one at a time if they improve criteria.
- Backward elimination: Start with all predictors, remove them one at a time.
- Stepwise: Combines forward and backward steps.

**Caution:**
- Stepwise methods can lead to overfitting.
- Better to use domain knowledge, regularisation, or information criteria directly without blind stepwise procedures.

**R Demonstration (Logistic Regression):**

\`\`\{r logistic-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
library(ggplot2)
library(data.table)

# Simulate binary outcome data
x <- runif(100, 0, 2)
log_odds <- -1 + 2*x
p <- exp(log_odds)/(1+exp(log_odds))
y <- rbinom(100, size=1, prob=p)

dt_glm <- data.table::data.table(x=x, y=y)

# Fit logistic regression
logit_model <- glm(y ~ x, data=dt_glm, family=binomial(link="logit"))
summary(logit_model)
\`\`\`

Check the summary to see estimated coefficients, their significance, and model fit measures. You might compare this model to one without $x$ using AIC or perform a likelihood ratio test.

---

**Key Takeaways:**

- **GLMs (Generalised Linear Models):**  
  Expand linear models to handle non-normal response distributions (binary, counts, etc.) by applying link functions to relate linear predictors to response means.

- **Logistic and Poisson Regression:**  
  Logistic regression handles binary outcomes using a logit link, while Poisson regression models counts with a log link. Both are widely used in medical studies, insurance claims analysis, ecology, and more.

- **Deviance and Overdispersion:**  
  Deviance measures model fit; overdispersion signals that the assumed distribution understates variability. Addressing overdispersion is crucial for correct inference (quasi-likelihood, negative binomial, or random effects).

- **Model Selection (AIC/BIC) and Stepwise Methods:**  
  Information criteria guide choice among competing models. Stepwise methods add or remove predictors to find a good model, but should be used cautiously.

By mastering GLMs, you gain flexibility to tackle a broad range of outcomes—beyond continuous normal responses—and confidently select and refine models using principled criteria. This expands your modelling toolbox for real-world problems where normal assumptions do not hold.

## Nonlinear and Nonparametric Regression

Not all relationships are linear. In many scenarios, response variables depend on predictors in more complex, curved ways. Nonlinear and nonparametric regression methods allow you to capture these relationships without forcing a straight line. They can reveal subtle patterns, interactions, and structures that linear models miss.

### Polynomial Regression, Spline Regression

**Polynomial Regression:**
- Extend a linear model by including polynomial terms of a predictor:
  $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \beta_d X^d + \varepsilon.$$
- Flexibility increases as you add higher-degree terms, but beware of overfitting and unwieldy oscillations.
- Best for capturing simple curves; high-degree polynomials can become unstable.

**Spline Regression:**
- Splines are piecewise polynomials joined smoothly at "knots."
- Offer more local flexibility than a single high-degree polynomial.
- Natural splines or cubic splines often used:
  - Allow capturing complex shapes while maintaining smoothness.
  - Knots placed at strategic points (e.g., percentiles of $X$) to allow localised curvature.

Compared to polynomials, splines provide controlled flexibility, usually resulting in more stable and interpretable fits.

**R Demonstration (Cubic Spline with `splines`):**

\`\`\{r spline-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
library(data.table)
library(ggplot2)
library(splines)

x <- seq(0,10,length.out=100)
y <- 2 + 0.5*x - 0.2*x^2 + rnorm(100, sd=2) # a slightly curved relationship
dt_poly <- data.table::data.table(x=x, y=y)

# Fit a spline model
lm_spline <- lm(y ~ ns(x, df=5), data=dt_poly) # natural spline with df=5
dt_poly[, yhat := predict(lm_spline, newdata=dt_poly)]

ggplot(dt_poly, aes(x=x)) +
  geom_point(aes(y=y), colour="blue", alpha=0.5) +
  geom_line(aes(y=yhat), colour="red", size=1.2) +
  labs(title="Spline Regression", x="X", y="Y")
\`\`\`

### Kernel Smoothing, Local Regression (LOESS)

**Kernel Smoothing:**
- Nonparametric technique that estimates the regression function locally.
- Assign more weight to points near $x$ of interest.
- Result is a smooth curve that adapts to local patterns without assuming a global polynomial form.

**LOESS (Local Regression Smoothing):**
- A popular kernel-based method.
- For each prediction point, LOESS fits a simple polynomial to a neighbourhood of data points, weighted by proximity.
- No global formula; each region gets a locally appropriate curve.
- Great for exploratory analysis, revealing intricate relationships that vary across the predictor domain.

**R Demonstration (LOESS):**

\`\`\{r loess-demo, echo=TRUE, message=FALSE, warning=FALSE\}
loess_fit <- loess(y ~ x, data=dt_poly, span=0.3) # span controls smoothness
dt_poly[, y_loess := predict(loess_fit)]

ggplot(dt_poly, aes(x=x)) +
  geom_point(aes(y=y), colour="blue", alpha=0.5) +
  geom_line(aes(y=y_loess), colour="green", size=1.2) +
  labs(title="LOESS Smoothing", x="X", y="Y")
\`\`\`

Adjusting `span` trades off smoothness and fidelity to local variation.

### Generalised Additive Models (GAMs)

**GAMs** provide a powerful framework:
- Extend GLMs by allowing linear predictors to be replaced by smooth functions of predictors:
  $$Y = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p) + \varepsilon,$$
  where each $f_j$ is an unknown smooth function estimated from the data.
- Offer flexibility like LOESS or splines, but within a regression framework that can handle different error distributions (like GLMs).
- Easy interpretation: each $f_j(X_j)$ is a smooth effect of $X_j$ on $Y$, while other variables are held constant.

**Advantages of GAMs:**
- Balance flexibility (nonlinear fits) with interpretability (each predictor’s effect is shown as a smooth curve).
- Can incorporate penalisation to avoid overfitting.
- Useful across many fields: from ecology (species response curves) to economics (nonlinear income effects).

**R Demonstration (GAM using `mgcv`):**

\`\`\{r gam-demo, echo=TRUE, message=FALSE, warning=FALSE\}
library(mgcv) # mgcv fits GAMs

gam_fit <- gam(y ~ s(x, k=5), data=dt_poly) # s(x) fits a smooth function of x
summary(gam_fit)
dt_poly[, y_gam := predict(gam_fit)]

ggplot(dt_poly, aes(x=x)) +
  geom_point(aes(y=y), colour="blue", alpha=0.5) +
  geom_line(aes(y=y_gam), colour="purple", size=1.2) +
  labs(title="GAM Fit", x="X", y="Y")
\`\`\`

Check summary and the plot. The GAM chooses a smooth shape automatically, guided by data and smoothing penalties.

---

**Key Takeaways:**

- **Polynomial and Spline Regression:**  
  Polynomials add flexibility but can become unstable. Splines focus flexibility where needed, producing smooth, interpretable curves without wild oscillations.

- **Kernel Smoothing and LOESS:**  
  Nonparametric methods that adapt to data locally, capturing complex patterns without a global formula. Great for exploratory insight.

- **Generalised Additive Models (GAMs):**  
  Combine the interpretability of regression models with the flexibility of smooth functions. They extend the GLM idea, fitting a separate smooth curve for each predictor, offering a structured and automated approach to nonlinear relationships.

In essence, these methods liberate you from the confines of straight lines, enabling you to model real-world phenomena more accurately. Whether you choose polynomials, splines, LOESS, or GAMs depends on the complexity of your data, your modelling goals, and how you balance flexibility with interpretability.

## Mixed-Effects and Hierarchical Models

In many real-world problems, data arise from multi-level structures. For example, students nested within classrooms, or repeated measurements collected from the same subject over time. **Mixed-effects and hierarchical models** acknowledge that observations within the same group or subject are correlated, and model these complex structures elegantly using random effects.

### Random Intercepts and Random Slopes

**Random Intercepts:**
- Allow different groups (e.g., classrooms, individuals) to have their own baseline level.
- Example: In a study of students nested within schools, each school could have its own intercept for the average test score, capturing unobserved factors that make some schools systematically higher or lower scoring.

**Random Slopes:**
- Extend the idea further: not only intercepts but also slopes can vary across groups.
- Example: Students’ improvement over time may differ by school. One school’s students improve faster (steeper slope) than another.

**Model Structure:**
- A simple two-level model:
  $$Y_{ij}=\beta_0 + u_{0j} + \beta_1X_{ij} + u_{1j}X_{ij} + \varepsilon_{ij},$$
  where:
  - $u_{0j}$ is the random intercept for group $j$,
  - $u_{1j}$ is the random slope for group $j$,
  - $\varepsilon_{ij}$ are residual errors.

Both $u_{0j}$ and $u_{1j}$ are random effects, typically assumed to follow Normal distributions:
$$u_{0j}\sim N(0,\sigma_{u0}^2), \quad u_{1j}\sim N(0,\sigma_{u1}^2).$$

### Multilevel and Longitudinal Data Analysis

Multilevel models handle data with natural groupings or hierarchical structures:

- **Two-level structure:** Students (level-1) nested in schools (level-2).
- **Longitudinal/Repeated Measures:** Observations collected over time from the same subject introduce correlation that standard models cannot ignore. Mixed models incorporate random effects to model subject-specific intercepts and possibly slopes over time.
- Handling these structures avoids inflated Type I errors or biased estimates caused by ignoring within-group correlation.

**Benefits:**
- Separate within-group from between-group variation.
- Model complex dependency structures without resorting to ad-hoc solutions.
- Easily incorporate time-varying covariates, random slopes, and non-linear trajectories with minimal modifications.

### Generalised Linear Mixed Models (GLMMs)

When the response is not normally distributed (binary, count, etc.), combine the idea of GLMs with random effects to form a **Generalised Linear Mixed Model (GLMM)**:

- Similar to GLMs, but now with random intercepts/slopes.
- Example: Logistic regression for binary responses within subjects or Poisson regression for counts across groups.
- Maximum likelihood or restricted maximum likelihood (REML) methods are used for fitting, and MCMC or approximations for complex models.

**R Demonstration (Linear Mixed Model using `lme4`):**

\`\`\{r lmm-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
library(data.table)
library(ggplot2)
library(lme4)

# Simulate data: Suppose 10 groups (e.g., schools), each with 20 observations
# Overall intercept=5, random intercept sd=2, slope=0.5, random slope sd=0.3
ng <- 10
n_each <- 20
group <- rep(1:ng, each=n_each)
x <- runif(ng*n_each, 0, 10)

u0 <- rnorm(ng, 0, 2)[group]  # random intercept per group
u1 <- rnorm(ng, 0, 0.3)[group] # random slope per group
y <- 5 + u0 + (0.5 + u1)*x + rnorm(ng*n_each, sd=1)

dt_mix <- data.table::data.table(y=y, x=x, group=factor(group))

# Fit a mixed model: random intercept and slope
lmm <- lmer(y ~ x + (x|group), data=dt_mix) 
summary(lmm)
\`\`\`

Check the summary for estimates of fixed effects (overall intercept, slope) and random effects (variance of intercepts and slopes).

---

**Key Takeaways:**

- **Random Intercepts/Slopes:**  
  Mixed models incorporate random effects to handle hierarchical or repeated-measures data, acknowledging correlated observations within groups or subjects.

- **Multilevel and Longitudinal Data:**  
  Mixed models are ideal for data with multiple levels or time-dependent measurements. They separate variation at different levels, improving inference and interpretation.

- **Generalised Linear Mixed Models:**  
  Extend mixed models to non-normal responses by adding link functions and non-Gaussian distributions. Perfect for binary, count, or other complex data structures.

Mixed-effects and hierarchical models align closely with the data’s structure, yield better estimates, and offer nuanced insights into group-level effects and individual trajectories. They are indispensable in fields like education, medicine, ecology, and social sciences, where multi-level structures and repeated measurements are the norm.

## High-Dimensional and Regularised Models

Modern data sets often contain a large number of predictors—possibly even more variables than observations. Traditional regression methods can fail or become unstable in these high-dimensional settings. **Regularisation** and **dimension reduction** methods help prevent overfitting, control complexity, and highlight the most important variables.

### Ridge, Lasso, and Elastic Net Regularisation

When $p$ (the number of predictors) is large, ordinary least squares (OLS) estimates become unstable or even inestimable. Regularisation methods add a penalty to the regression coefficients, shrinking them towards zero to reduce variance and improve predictive accuracy.

- **Ridge Regression:**
    - Minimises the residual sum of squares with an $$L_2$$ penalty:
      $$\sum_{i}(y_i - \hat{y}_i)^2 + \lambda\sum_{j}\beta_j^2,$$
      where $\lambda \geq 0$ is a tuning parameter.
    - Shrinks coefficients but never exactly to zero.
    - Useful when many predictors are moderately correlated.

- **Lasso (Least Absolute Shrinkage and Selection Operator):**
    - Uses an $$L_1$$ penalty:
      $$\sum_{i}(y_i - \hat{y}_i)^2 + \lambda\sum_{j}|\beta_j|.$$
    - Encourages sparsity—some coefficients become exactly zero, performing variable selection automatically.
    - Ideal when you want a simpler model with fewer predictors.

- **Elastic Net:**
    - Combines $$L_1$$ and $$L_2$$ penalties:
      $$\sum_{i}(y_i - \hat{y}_i)^2 + \lambda\left(\alpha\sum_{j}|\beta_j| + (1-\alpha)\sum_{j}\beta_j^2\right).$$
    - Balances the strengths of ridge and lasso.
    - Useful when $p > n$ and predictors are highly correlated.

**Choosing $\lambda$ and $\alpha$:**
- Tune parameters via cross-validation.
- Minimising prediction error on held-out data selects appropriate penalty strengths.

**R Demonstration (Lasso using `glmnet`):**

\`\`\{r lasso-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
library(glmnet)
library(data.table)

# Simulate data with many predictors:
n <- 100
p <- 50
X <- matrix(rnorm(n*p), n, p)
beta <- c(1.5, -2, 0.5, rep(0, p-3)) # only first 3 predictors matter
y <- X %*% beta + rnorm(n, sd=1)

# Fit lasso
lasso_model <- glmnet::glmnet(X, y, alpha=1)
plot(lasso_model, xvar="lambda")
\`\`\`

Check the coefficient paths as $\lambda$ changes. Use cross-validation (`cv.glmnet`) to find the best $\lambda$.

### Dimension Reduction Techniques (PCA for Regression, Partial Least Squares)

When you have many predictors, another strategy is to reduce their dimensionality before regression:

- **PCA for Regression:**
    - Principal Component Analysis (PCA) transforms predictors into orthogonal directions capturing maximum variance.
    - You can regress on a few principal components instead of the full set of correlated predictors.
    - Reduces dimensionality and multicollinearity but may lose interpretability.

- **Partial Least Squares (PLS):**
    - Simultaneously considers predictors and response when finding directions.
    - Aims to find latent components that explain both predictor variance and the response.
    - Useful when predictors are highly correlated and you want a balance between predictive accuracy and interpretability.

Both PCA and PLS help control complexity and improve prediction when $p$ is large or predictors are collinear.

**R Demonstration (PCA for Regression):**

\`\`\{r pca-demo, echo=TRUE, message=FALSE, warning=FALSE\}
library(pls) # for pls and pcr functions

pcr_model <- pcr(y ~ X, scale=TRUE, validation="CV") # PCA regression using pls package
summary(pcr_model)
\`\`\`

Check how many components minimise cross-validation error.

### Stability Selection and Variable Importance

**Stability Selection:**
- Rather than relying on one run of a variable selection method (like lasso), stability selection involves repeated subsampling or bootstrapping to see how often each variable is chosen.
- Ensures that the final set of predictors is not an artefact of a particular sample.
- Produces a more reliable set of important variables.

**Variable Importance Measures:**
- Many methods produce metrics (e.g., coefficients, loadings, or importance scores) indicating which variables matter most.
- Random forests, boosted trees, or neural networks often yield variable importance metrics helpful in understanding model structure.
- In a high-dimensional setting, these importance measures guide interpretation and subsequent data collection or modelling decisions.

---

**Key Takeaways:**

- **Ridge, Lasso, Elastic Net:**  
  Regularisation methods that shrink or eliminate coefficients to prevent overfitting. Lasso selects variables, ridge shrinks all coefficients, elastic net is a hybrid approach.
  
- **Dimension Reduction (PCA, PLS):**  
  Instead of dealing with many correlated variables, project them onto fewer components. PCA is unsupervised (uses only predictors), while PLS uses both predictors and response.

- **Stability Selection and Variable Importance:**  
  Ensure that chosen predictors are robust to sampling variability. Importance measures and stability selection strengthen confidence in the selected model’s variables.

In a world of big data and complex predictor sets, these tools are essential. Regularisation stabilises models, dimension reduction clarifies structure, and stability selection ensures reproducibility and reliability. Together, they equip you to handle high-dimensional challenges with nuance and rigour.