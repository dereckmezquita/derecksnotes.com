---
title: "Introduction to Statistics with R: Foundations of Statistical Reasoning"
blurb: "Blue and white mean war"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Foundations of Statistical Reasoning

## Historical and Philosophical Context

### Early Foundations: From Gauss to Fisher

The roots of modern statistics stretch back centuries, evolving from rudimentary counting methods to sophisticated probabilistic reasoning and inferential techniques. A few key figures and eras set the stage for today’s expansive field:

- **Carl Friedrich Gauss (1777–1855):**
    - Gauss introduced the concept of the normal distribution, originally to model measurement errors in astronomy.
    - He developed the method of least squares, providing a mathematical foundation for regression analysis. This idea—minimising the sum of squared residuals—remains a cornerstone of linear modeling.

- **Adolphe Quetelet (1796–1874) and Social Physics:**
    - Quetelet applied statistical reasoning to social data, showing that human traits (like height) often follow stable distributions.
    - His work hinted that even complex phenomena could be studied probabilistically, planting the seeds for statistical methods in the social sciences.

- **Francis Galton (1822–1911):**
    - Galton’s studies on heredity introduced concepts of correlation and regression toward the mean.
    - He pioneered techniques to quantify relationships between variables, setting the stage for linear regression and correlation analysis.

- **Karl Pearson (1857–1936) and the Chi-Square Test:**
    - Pearson established the foundations of hypothesis testing and introduced the chi-square test for goodness-of-fit.
    - He built upon Galton’s work, refining methods to measure relationships between variables and developing early forms of modern statistical theory.

- **Ronald A. Fisher (1890–1962):**
    - Fisher revolutionised statistics by rigorously defining principles of estimation, hypothesis testing, and experimental design.
    - He introduced the concept of maximum likelihood estimation, formalised ANOVA, and fostered the practice of statistically designed experiments.
    - Fisher’s emphasis on probability and formal inference frameworks turned statistics into a well-founded discipline, bridging theory and application.

**Analogy:** If we imagine statistics as a grand cathedral of knowledge, Gauss laid the first stones, Pearson crafted elaborate structures of inference, and Fisher installed the stained-glass windows that let mathematical rigor and clarity shine through.

### The Frequentist Paradigm vs. Bayesian Paradigm

Two broad philosophical approaches dominate modern statistical inference, each offering a distinct way to reason about uncertainty:

- **Frequentist Paradigm:**
    - **Core Idea:** Probability is viewed as a long-run frequency. Parameters are fixed but unknown, and data is seen as repeatable outcomes of experiments.
    - **Inference Process:**  
      Estimators are chosen for their properties under repeated sampling. Confidence intervals express how frequently they contain the true parameter in hypothetical repetitions.
    - **Significance and Hypothesis Testing:**
      P-values measure how surprising observed data would be if a null hypothesis were true. This approach avoids assigning probabilities to parameters directly.

- **Bayesian Paradigm:**
    - **Core Idea:** Probability is a measure of belief or uncertainty about a parameter. Parameters are random and have prior distributions.
    - **Inference Process:**  
      Bayes’ theorem combines the prior belief about a parameter with observed data (likelihood) to produce a posterior distribution. From this posterior, we can directly quantify probabilities about parameters.
    - **Credible Intervals and Decision-Theory:**
      Bayesian intervals (credible intervals) have a direct probability interpretation: there is, for example, a 95% probability that the parameter lies within the interval.

**Analogy:**  
Consider estimating the mean height of a population. A frequentist treats the population mean as fixed and tries to design procedures with good long-run properties. A Bayesian starts with a belief distribution about the mean, updates it with observed data, and makes probability statements about the mean itself.

**R Demonstration (Conceptual Bayesian vs. Frequentist):**

\`\`\{r paradigm-demo, echo=TRUE, message=FALSE, warning=FALSE\}
# Demonstration: Simulating data from a Normal(10, 2^2)
set.seed(123)
data_sample <- rnorm(100, mean=10, sd=2)

# Frequentist perspective: estimate mean and CI
freq_mean <- mean(data_sample)
freq_ci <- freq_mean + c(-1,1)*1.96*(2/sqrt(100))

cat("Frequentist 95% CI for the mean:", freq_ci, "\n")

# Bayesian perspective: assume Normal(0, 10^2) prior for mean, known variance=4
# Posterior for mean (normal prior and known variance) is also normal.
# Posterior mean: weighted combination of prior and data
# We'll skip exact formula details, just conceptual:

prior_mean <- 0; prior_var <- 100
data_mean <- freq_mean; data_var <- (2^2)/100
posterior_var <- 1/(1/prior_var + 1/data_var)
posterior_mean <- posterior_var*(prior_mean/prior_var + data_mean/data_var)
posterior_sd <- sqrt(posterior_var)

cat("Bayesian posterior mean:", posterior_mean, "\n")
cat("Bayesian 95% credible interval:", 
    posterior_mean + c(-1,1)*1.96*posterior_sd, "\n")
\`\`\`

This example is simplified, but it illustrates that the frequentist confidence interval arises from a repeated-sampling concept, while the Bayesian credible interval comes from directly updating a prior belief with observed data.

### The Rise of Computational Statistics

As the 20th century progressed, computational advancements profoundly influenced statistics:

- **Before Computers:**  
  Classical inference methods often relied on closed-form solutions and simplifications. Estimation, testing, and model fitting were constrained by what could be done by hand or simple calculators.

- **Digital Computing Revolution:**  
  With the advent of digital computers, statisticians could simulate complex processes, fit models without closed-form solutions, and explore high-dimensional data. This sparked innovation in areas like:
  - **Monte Carlo Methods:** Simulating random variables to approximate probabilities, integrals, and expectations that cannot be solved analytically.
  - **Bootstrap and Resampling:** Drawing multiple samples from observed data to estimate variability, confidence intervals, and biases, free from stringent parametric assumptions.
  - **MCMC for Bayesian Analysis:** Complex Bayesian posterior distributions became tractable through iterative simulation, enabling Bayesian methods to flourish.
  
- **Modern Era of Big Data and Machine Learning:**
  - High-performance computing, parallel processing, and cloud infrastructures allow the analysis of massive datasets.
  - Machine learning algorithms leverage computational power to build flexible, data-driven models like random forests, gradient boosting machines, and deep neural networks.
  - This has transformed statistics into a key player in data science, supporting fields as diverse as genomics, climate modelling, and real-time analytics.

**R Demonstration (Simulation and Bootstrap Concept):**
  
\`\`\{r computational-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(999)
data_sample2 <- rnorm(100, mean=5, sd=3)
# Compute bootstrap estimate of the mean's standard error
B <- 1000
boot_means <- numeric(B)
for(i in 1:B) {
    boot_sample <- sample(data_sample2, size=100, replace=TRUE)
    boot_means[i] <- mean(boot_sample)
}

boot_se <- sd(boot_means)
cat("Bootstrap estimate of mean's std. error:", boot_se, "\n")

# Plot bootstrap distribution
ggplot2::ggplot(data.table::data.table(boot_means=boot_means), ggplot2::aes(x=boot_means)) +
  ggplot2::geom_histogram(binwidth=0.1, fill="steelblue", color="white") +
  ggplot2::labs(title="Bootstrap Distribution of the Mean", x="Bootstrap Means", y="Frequency")
\`\`\`

In this snippet, we simulate data, apply a bootstrap procedure, and visualise the bootstrap means. Decades ago, such computations would have been impractical. Today, they are standard practice, demonstrating the transformative power of computation in statistics.

---

**Key Takeaways:**

- **Early Foundations:**  
  The work of Gauss, Pearson, Fisher, and others laid the theoretical and methodological bedrock for statistical methods. They established principles that guide how we measure uncertainty, estimate parameters, and test hypotheses.

- **Frequentist vs. Bayesian Paradigms:**  
  Two philosophical schools offer distinct interpretations of probability and inference. Understanding their differences helps you select the most appropriate methods and interpret results confidently.

- **Computational Revolution:**  
  Advancements in computing unleashed new statistical methods—resampling techniques, MCMC, and machine learning—allowing analysts to tackle problems unimaginable a century ago. This shift continues today, shaping modern data science and the future of statistical inquiry.

With this historical and philosophical context, we better appreciate the richness of statistical methods, why certain approaches exist, and how computation has vastly expanded our analytical toolkit. This perspective informs not only which methods we choose but also how we communicate and justify our analytical decisions.

## Defining Populations, Parameters, and Estimates

### Population vs. Sample Concepts

At the heart of statistical reasoning is the distinction between a **population** and a **sample**. Understanding this difference is crucial because almost all statistical inference involves using samples (what we observe) to learn about populations (what we care about but cannot fully observe).

- **Population:**
    - The population is the entire collection of units (people, organisms, objects, events) of interest. It could be infinite (e.g., all possible outcomes of a random experiment) or exceedingly large (all citizens of a country, all cells in a culture).
    - Characteristics of the population, such as the true mean height of all oak trees in a forest or the true proportion of a genetic variant in a species, are called **parameters**.
    - Often, we cannot measure or observe the entire population, either due to cost, time, or feasibility constraints.

- **Sample:**
    - A sample is a subset of the population that we actually observe. By carefully selecting a sample, we attempt to ensure it represents the population well.
    - From the sample data, we calculate **statistics**, such as the sample mean or sample proportion. These statistics serve as estimates of the corresponding population parameters.
    - The logic of statistical inference lies in extending findings from the sample to infer properties of the population, while explicitly accounting for uncertainty due to sampling variability.

**Analogy:** If the population is a huge forest, the sample is a handful of trees you examine. By measuring those trees, you infer how tall the entire forest might be. You must acknowledge that different samples (handfuls of trees) would yield slightly different measurements.

**R Demonstration (Sampling Concept):**

\`\`\{r population-sample-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(123)
# Simulating a population of 1 million individuals with incomes ~ Normal(50000, 10000^2)
population <- rnorm(10^6, mean=50000, sd=10000)

# Draw a sample of size 1000
sample_data <- sample(population, size=1000, replace=FALSE)

# Compare population mean and sample mean
true_mean <- mean(population)
sample_mean <- mean(sample_data)

cat("True population mean:", true_mean, "\n")
cat("Sample mean:", sample_mean, "\n")
\`\`\`

In this demonstration:
- We have a simulated population (for illustration, though real populations are often not fully known).
- The sample mean approximates the true population mean. The difference reflects sampling variability.

### Parameters and Estimators

**Parameters** are numerical characteristics of populations. Examples include:
- Population mean ($$\mu$$): The average of a particular measurement over the entire population.
- Population variance ($$\sigma^2$$): A measure of how spread out values are.
- Population proportion ($$p$$): The fraction of the population that possesses a certain attribute.

We rarely know parameters exactly, as we often cannot measure the entire population. Instead, we rely on **estimators**, which are rules or formulas applied to sample data to produce **estimates**.

- **Estimators:**
    - An estimator is a statistical procedure (a function of the sample data) used to guess a population parameter.
    - For example, the sample mean $$\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i$$ is an estimator for the population mean $$\mu$$.
    - The sample proportion $$\hat{p}$$ is an estimator for the population proportion $$p$$.

- **Properties of Good Estimators:**
    - **Unbiasedness:** An estimator is unbiased if its expected value equals the true parameter. For example, the sample mean is an unbiased estimator of the population mean.
    - **Consistency:** As the sample size grows large ($$n \to \infty$$), a consistent estimator converges (in probability) to the true parameter.
    - **Efficiency:** Among unbiased estimators, the one with the smallest variance is preferred because it tends to produce estimates closer to the true parameter.

**R Demonstration (Estimators):**

\`\`\{r estimator-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(42)
# Simulate a small sample from a Normal(10, 2^2) population
sample_small <- rnorm(30, mean=10, sd=2)

# Estimate the mean (parameter: mean=10)
est_mean <- mean(sample_small)
cat("Sample estimate of mean:", est_mean, "\n")

# With larger sample sizes, the estimate should get closer to 10
sample_large <- rnorm(10000, mean=10, sd=2)
cat("Large sample estimate of mean:", mean(sample_large), "\n")
\`\`\`

In this code:
- With a small sample (n=30), the estimate may differ noticeably from the true mean (10).
- With a large sample (n=10,000), the sample mean is much closer to the true parameter, illustrating consistency.

### Sampling Techniques: Simple Random, Stratified, Cluster

The quality of statistical inference heavily depends on how the sample is drawn. Poor sampling can introduce bias, rendering estimates unrepresentative of the population. Several common sampling methods include:

1. **Simple Random Sampling (SRS):**
    - **Definition:** Every member of the population has an equal chance of being selected, and each set of individuals is equally likely to form the sample.
    - **Advantages:** Straightforward and unbiased in theory.
    - **Disadvantages:** Might be logistically challenging for large, dispersed populations. Potentially less efficient if the population is heterogeneous.

2. **Stratified Sampling:**
    - **Definition:** The population is divided into subgroups (strata) based on a characteristic (e.g., age groups, geographic regions). A random sample is drawn from each stratum, often proportionally to its size.
    - **Why Stratify?**  
      Improves precision if each stratum is more internally homogeneous. Ensures representation of key subgroups that might be missed by simple random sampling.
    - **Analogy:** If you want a sample of a country’s population, you might stratify by region (north, south, east, west) to ensure each part of the country is represented.

3. **Cluster Sampling:**
    - **Definition:** The population is divided into clusters (e.g., schools, hospitals, city blocks). A random selection of clusters is chosen, and then all (or a random subset) of units within those clusters are sampled.
    - **Advantages:** Can be more cost-effective and practical if the population is geographically dispersed.
    - **Disadvantages:** Clusters may not be as internally diverse, increasing the variance of estimates unless properly accounted for.

**R Demonstration (Sampling Methods Concept):**

\`\`\{r sampling-demo, echo=TRUE, message=FALSE, warning=FALSE\}
set.seed(202)
# Suppose a population of 10,000 individuals split into two strata:
pop_strata <- data.table::data.table(
  id=1:10000,
  strata=rep(c("A","B"), each=5000),
  value=rnorm(10000, mean=50, sd=10)
)

# Simple Random Sample of size 500
srs_sample <- pop_strata[sample(.N, 500)]

# Stratified sample: pick 250 from A and 250 from B
A_sample <- pop_strata[strata=="A"][sample(.N, 250)]
B_sample <- pop_strata[strata=="B"][sample(.N, 250)]
strat_sample <- rbind(A_sample, B_sample)

cat("SRS Sample mean:", mean(srs_sample$value), "\n")
cat("Stratified Sample mean:", mean(strat_sample$value), "\n")
\`\`\`

In this demonstration:
- The simple random sample picks individuals irrespective of their strata.  
- The stratified sample ensures equal representation from strata A and B. If the strata differ in mean or variance, stratification can reduce sampling error and improve precision.

---

**Key Takeaways:**

- **Population vs. Sample:**  
  We almost never have full knowledge of a population’s parameters. Instead, we rely on samples—selected subsets from the population—to estimate these parameters.

- **Parameters and Estimators:**  
  Parameters (like the population mean $$\mu$$) are unknown constants. Estimators are sample-derived formulas that attempt to approximate these parameters. Good estimators are unbiased, consistent, and efficient, ensuring reliable inferences.

- **Sampling Techniques:**  
  Proper sampling is crucial. Simple random sampling offers simplicity and fairness, but stratified sampling can increase efficiency by ensuring representation of all subgroups. Cluster sampling can make data collection more practical in geographically dispersed populations, at the potential cost of higher variance if clusters are not internally diverse.

By understanding these fundamental concepts, we set the stage for valid, reliable statistical inference. With well-chosen samples and sound estimators, we can confidently move forward into the realms of probability, hypothesis testing, regression, and beyond.

## Variability, Uncertainty, and Randomness

### Sources of Variation in Data

Data rarely come in neat, predictable patterns. In nearly all empirical settings—biological measurements, economic indicators, physical experiments—variability is the norm. Understanding where this variation originates is central to statistical analysis.

- **Biological and Physical Variation:**  
  Even genetically identical plants grown under controlled conditions exhibit differences in height due to micro-environmental factors, slight nutritional differences, or random biological processes. In physics, measurement noise and instrument precision introduce variability.

- **Measurement Error:**  
  No measuring instrument is perfect. Scales, thermometers, or sensors introduce small random errors. Two measurements of the same quantity may differ slightly.

- **Sampling Variability:**  
  Drawing different samples from a population typically yields different results. For example, if you measure the average height of 100 randomly chosen people and then measure another 100, the two averages will differ, even if drawn from the same population.

- **Complex Interactions:**  
  In social sciences or epidemiology, human behaviours, environmental factors, and genetic predispositions interact in complicated ways, producing highly variable outcomes.

**Analogy:**  
Imagine an impressionist painting: the brushstrokes vary in direction, length, and colour intensity. Similarly, data points vary due to numerous subtle factors that create a rich, textured landscape rather than a uniform pattern.

### Uncertainty Quantification

Variability leads directly to uncertainty. When we try to estimate a population mean, predict future sales, or assess the effect of a drug, we must acknowledge that our conclusions are not certain. Instead, they are supported by evidence that includes a margin of error or a range of likely outcomes.

- **Confidence Intervals and Margin of Error:**  
  A confidence interval for a parameter (like a mean) provides a range of values consistent with the observed sample data, accounting for sampling variability. For instance, a 95% confidence interval for the mean might read as (48.5, 51.5), telling us that under repeated sampling, 95% of such intervals would contain the true mean.

- **Probabilistic Statements:**  
  Instead of saying "The average height is exactly 170 cm," we say "The average height is about 170 cm, with a 95% confidence interval from 168 to 172 cm." This communicates uncertainty quantitatively.

- **Uncertainty in Predictions and Decisions:**  
  Whether we’re forecasting tomorrow’s temperature or evaluating the reliability of a machine component, uncertainty quantification ensures that decisions consider the likelihood of various outcomes, not just a single best guess.

### The Role of Probability in Statistics

Probability theory provides the mathematical language that describes uncertainty and variation. Without probability, we would have no formal way to quantify how surprising an outcome is, how likely it is that a parameter estimate differs from the truth by a certain amount, or how confident we can be in our predictions.

- **Probability as a Model for Variation:**  
  By assigning probabilities to events, random variables, and parameter estimates, we convert randomness into a structured framework. Probability distributions model how data might vary, allowing us to derive expectations, variances, and other characteristics of estimators.

- **Link Between Data and Underlying Processes:**  
  Probability connects observed data to unseen parameters. For example, if we assume data are sampled from a normal distribution with unknown mean $$\mu$$, probability theory helps us infer $$\mu$$ and quantify how uncertain that inference is.

- **Foundation of Inference and Prediction:**  
  Hypothesis testing, confidence intervals, Bayesian updating, and predictive intervals all rest on probabilistic arguments. Probability ensures that conclusions are not just intuitive or anecdotal—they are supported by a rigorous framework that accounts for randomness.

**R Demonstration (Variation and Uncertainty):**

Below is an example illustrating how randomness in data translates into uncertainty about parameters. We will:

1. Simulate data from a known distribution.
2. Estimate a parameter (the mean).
3. Construct a confidence interval to reflect uncertainty about this estimate.

\\`\\`\\`{r variation-uncertainty-demo, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(123)
library(data.table)
library(ggplot2)

# Suppose our population is N(50, 10^2)
# We draw a sample of size n=100
n <- 100
sample_data <- rnorm(n, mean=50, sd=10)

# Estimate the mean:
est_mean <- mean(sample_data)

# Theoretical standard error of the mean: sigma/sqrt(n) = 10/sqrt(100) = 1
# 95% confidence interval: est_mean ± 1.96 * std.error
std_error <- 10/sqrt(n)
ci <- est_mean + c(-1,1)*1.96*std_error

cat("Sample mean estimate:", est_mean, "\n")
cat("95% CI for the mean:", ci, "\n")

# Visualise the sample distribution
dt <- data.table(value=sample_data)
ggplot(dt, aes(x=value)) +
  geom_histogram(binwidth=2, fill="steelblue", color="white") +
  geom_vline(xintercept=est_mean, color="red", linetype="dashed") +
  geom_vline(xintercept=ci[1], color="green", linetype="dotted") +
  geom_vline(xintercept=ci[2], color="green", linetype="dotted") +
  labs(title="Sample Distribution with Mean and CI",
       x="Value", y="Count",
       subtitle=paste0("Mean=", round(est_mean,2), 
                       ", 95% CI=[", round(ci[1],2), ", ", round(ci[2],2), "]"))
\\`\\`\\`

In this demonstration:

- We see that the sample data vary around the true mean of 50.
- The sample mean (est_mean) may differ from 50 due to random chance.
- The confidence interval captures the uncertainty: it’s a range that likely contains the true mean. Different samples would yield different intervals, reflecting sampling variability.

---

**Key Takeaways:**

- **Sources of Variation:**  
  Data vary due to inherent natural differences, measurement error, complex underlying processes, and sampling randomness.

- **Uncertainty Quantification:**  
  We use statistical tools like confidence intervals, standard errors, and credible intervals to express how certain or uncertain we are about parameters, predictions, and conclusions.

- **Role of Probability:**  
  Probability is the mathematical framework that underpins statistical inference. It allows us to model uncertainty, estimate parameters, test hypotheses, and make informed decisions despite the randomness and variability inherent in real-world data.

By appreciating variability, quantifying uncertainty, and leveraging probability, we set the foundation for rigorous statistical reasoning. These concepts form the bedrock upon which all statistical inference, modelling, and prediction are built.