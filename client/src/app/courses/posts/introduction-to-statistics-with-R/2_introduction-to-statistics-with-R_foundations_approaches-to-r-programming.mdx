---
title: "Introduction to Statistics with R"
subtitle: "Chapter 2: Approaches to R Programming"
blurb: "File war tea"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output:
  html_document:
    keep_md: true
---



## Table of contents

### A Brief History of R

*In this section, we take a journey back in time to understand the origins of R, exploring how it evolved from a niche statistical environment into a global cornerstone for data science and quantitative analysis.*

- **Genesis of R**: The R language was born in the mid 1990s at the University of Auckland, New Zealand, created by Ross Ihaka and Robert Gentleman. Inspired by the S language (developed at AT&T's Bell Laboratories), R sought to combine flexibility, extensibility, and a user friendly syntax. From these origins, R has grown into a freely available, open source language supported by a passionate global community.

- **Open-Source Philosophy**: R's open source nature ensures that anyone can inspect its source code, propose improvements, and contribute packages. This open environment fuels innovation and collaboration.

- **Academic and Industry Adoption**: Initially adopted by statisticians and researchers in academia, R slowly made its way into industries: finance, pharmaceuticals, genomics, climate science, and more. Its mathematical capabilities, combined with a rich ecosystem of packages, make it a versatile tool. Over time, R's use cases expanded from classical statistical analysis to advanced machine learning, Bayesian modelling, and big data analytics.

- **Community Growth**: Key to R's meteoric rise is its community. Enthusiasts frequently share code snippets, answer questions on Q & A platforms, host user groups, and organise conferences. This collective learning environment means that if you encounter a statistical challenge, likely someone has faced it before and documented their solution. 
  - R-bloggers, Stack Overflow, and GitHub are just a few of the many platforms where R users share knowledge and collaborate.
  - CRAN has been central to R's growth. Anyone can share their code with the world, this has driven innovation and made R a powerful tool for data analysis.

### Demonstration: A Historical Glance Through Code

Here we look at the number of R packages available on CRAN over time[^1]. R's growth is fueled not only by the core language but also by the ecosystem that has sprung up around it.

[^1]: Credit to [@daroczig](https://gist.github.com/daroczig) and Henrik Bengtsson for scraping together the data.


``` r
# data source: https://gist.github.com/daroczig/3cf06d6db4be2bbe3368
r_packages_dt <- data.table::fread("./data/r-packages-statistics-historical.csv")
today_n_packages <- r_packages_dt[nrow(r_packages_dt), index]

ggplot2::ggplot(r_packages_dt, ggplot2::aes(as.Date(first_release), index)) +
    ggplot2::geom_line(size = 2) +
    ggplot2::scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
    ggplot2::scale_y_continuous(n.breaks = 15) +
    ggplot2::labs(
      title = "Number of public R packages on CRAN over time",
      subtitle = stringr::str_interp('Packages as of today: ${today_n_packages}'),
      x = NULL,
      y = NULL
    ) +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(angle = 45, hjust = 1)
    )
```

```
## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
```

<Figure src="/courses/2_introduction-to-statistics-with-R_foundations_approaches-to-r-programming_files/figure-html/unnamed-chunk-1-1.png">
	
</Figure>

In the plot we see how the number of packages really has grown exponentially over time. This reflects the maturity of the language as well as the diverse needs and interests of the R community. Packages cover a wide range of utilities, applications and fields of study such as finance, bioinformatics, and machine learning.

Of these 19866 packages, I've contributed one myself!

- [interface](https://github.com/dereckmezquita/interface)
  - https://dereckmezquita.github.io/interface/
  - https://cran.r-project.org/web/packages/interface/index.html

---

### Ecosystem Overview: CRAN, Bioconductor, GitHub

- **CRAN (Comprehensive R Archive Network)**: CRAN as you've understood is the main repository for R packages. Each package on CRAN follows strict submission guidelines, ensuring quality and compatibility.
- **Bioconductor**: Bioconductor is a specialised repository for bioinformatics packages. It is a curated collection of tools, frameworks, and data structures that cater to the needs of researchers in the life sciences. Being that bioinformatics deals with complex datasets and structures, the Bioconductor repository has adopted by convention the use of S4 classes and methods. This is a object oriented (OOP) paradigm that allows for more complex data structures, functions, methods and tools to be developed and maintained.
- **GitHub and Other Code Repositories**: Not all innovations first appear in CRAN or Bioconductor. Anyone can create a package and share it on GitHub. To install a github package you can use the `remotes` package.


``` r
# to install a package from CRAN
install.packages("data.table")

# to install a package from Bioconductor
BiocManager::install("DESeq2")

# to install a package from GitHub
remotes::install_github("dereckmezquita/interface")
```

## 2.3 R Programming Philosophies

*As you embark on this journey into R programming, it is essential to understand the philosophical underpinnings that guide how we write and structure code, why certain tools are preferred over others, and how these choices impact the efficiency, readability, and reliability of your analyses. Think of this as the architectural blueprint of a grand mansion you are about to build: before laying bricks, you must decide on the framework, the materials, and the style.*

- **Why Philosophies Matter**:  
  Programming philosophies shape how you approach a problem. Do you rely on a few core tools, mastering them thoroughly, or do you continually reach for new packages? Do you prefer transparent, “bare-bones” solutions or embrace extensive frameworks that provide many conveniences at the cost of hiding underlying complexity? These choices resemble deciding whether to cook from scratch or use pre-packaged meals. Both approaches feed you, but one offers more control, while the other offers more convenience.

- **Mathematical Framing**:  
  Suppose you have a set of data transformations $ T_1, T_2, \ldots, T_k $ you need to apply to a dataset $ D $. Your goal might be to produce an output $ O $ such that:
  
  $$
  O = (T_k \circ T_{k-1} \circ \cdots \circ T_1)(D)
  $$
  
  The philosophy you adopt affects how easily you compose and understand these transformations. A minimalistic philosophy (like Base R plus `data.table::`) may mean more manual coding, but offers maximum clarity. An all-in-one framework might speed initial development but can obscure these transformations' internals.

- **Storytelling with Code**:  
  Consider that each line of code not only manipulates data but also tells a story—about your reasoning, methodology, and decision-making. A coherent philosophy helps ensure your code's narrative is clear, purposeful, and maintainable over time.

---

### 2 Base R as a Foundational Tool

*Base R is the original building block upon which the entire R ecosystem rests. Before reaching for additional packages, it is wise to understand what Base R offers. Think of Base R as the strong, unadorned foundation of a house. You can live in it as is—basic but functional—or build upon it with extensions.*

- **Core Data Structures**:  
  Base R provides vectors, matrices, arrays, lists, and data frames. Understanding these structures is crucial. For instance, a data frame can hold heterogeneous data types, making it suitable for many statistical analyses. A vector of numeric values $ x_1, x_2, \ldots, x_n $ forms the bedrock of computations like mean:
  
  $$
  \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i
  $$
  
  This simple formula, computed easily in Base R with `mean(x)`, underpins more complex statistical reasoning.

- **Built-in Functions and Flexibility**:  
  Base R includes built-in functions for subsetting, sorting, and basic statistical tasks. For example:
  

``` r
# Using Base R to compute a simple mean
x_values <- rnorm(100, mean = 5, sd = 2)
basic_mean <- mean(x_values)
print(basic_mean)
```

```
## [1] 5.056877
```
  
  While this seems trivial, it demonstrates how no additional packages are needed for fundamental computations. Base R also provides plotting functions (`plot()`, `hist()`) to quickly visualise data. Although we will often use `ggplot2::` for more sophisticated visuals, Base R plots are reliable fallbacks.

- **Simplicity and Transparency**:  
  R's Base functions are often simpler than those in complex frameworks. While they may lack syntactic sugar and advanced abstractions, this simplicity ensures transparency. When you use Base R, you see each step of data manipulation, improving your understanding and enabling fine-grained control.

*Think of Base R as a well-equipped kitchen with standard tools—a stove, oven, knives, and basic pots. You can cook a wide variety of dishes, albeit with more manual effort. As you grow comfortable, you might add specialised utensils (`data.table::`), but you never lose the solidity and versatility that Base R provides.*

---

### 2 data.table for High-Performance Data Manipulation

*While Base R is powerful, it may become unwieldy for large datasets or complex data manipulations. Enter `data.table::`—a specialised tool designed for speed, efficiency, and elegance in data handling. If Base R is your foundational kitchen, `data.table::` is the set of high-end chef's knives and cookware that make slicing, dicing, and stirring data a breeze.*

- **Performance-Driven Design**:  
  `data.table::` excels at handling large datasets that might slow down Base R operations. It provides an internally optimised structure that can handle millions of rows without significant slowdowns. For example, consider a dataset with $ 10^7 $ rows. A Base R operation that runs in seconds might run in a fraction of a second with `data.table::`, thanks to efficient indexing and memory usage.

- **Intuitive Syntax**:  
  `data.table::` uses a `[i, j, by]` syntax, reminiscent of SQL queries. This structure transforms data wrangling into a legible process. Suppose you have a dataset of sales transactions and you want to compute total revenue per region:
  

``` r
# Hypothetical data: sales dataset
sales_data <- data.table::data.table(
  region = rep(c("North", "South", "East", "West"), each = 2500),
  sales = rnorm(10000, mean = 500, sd = 50)
)

# Compute total sales by region
regional_sales <- sales_data[ , .(total_sales = sum(sales)), by = region]
print(regional_sales)
```

```
##    region total_sales
##    <char>       <num>
## 1:  North     1251468
## 2:  South     1254761
## 3:   East     1252753
## 4:   West     1248764
```
  
  This single line inside `data.table::` shows exactly what you are doing: taking `sales_data`, grouping it by `region`, and summing `sales`.

- **Parallel to Mathematical Expressions**:  
  Consider the operation:
  
$$
\text{total_sales_by_region} = \sum_{i \in \text{region}} \text{sales}_i
$$

`data.table::` aligns closely with such a mathematical idea. It reduces the cognitive load required to map mathematical notation onto code. The directness of this syntax makes it easier to reason about your transformations.

- **Memory Efficiency and Scaling**: For very large datasets, efficient memory use is crucial. `data.table::` avoids making unnecessary copies of data, reducing memory overhead. This efficiency means you can handle bigger problems on the same hardware[^3].

[^3]: For more on memory efficiency and performance benchmarks, consider reading the [`data.table::` vignettes](https://rdatatable.gitlab.io/data.table/) or and the benchmarks at [duckdblabs.github.io/db-benchmark/](https://duckdblabs.github.io/db-benchmark/).

*In essence, `data.table::` refines your data-handling toolkit. If Base R is your reliable family car, `data.table::` is a sports car built for speed and precision.*

---

### 2 A Note on Tidyverse and Why We Focus on data.table Instead

*R's ecosystem is diverse. The tidyverse is another popular set of packages that aims to simplify data manipulation and analysis. Its approach, however, differs significantly from the minimalist yet powerful world of Base R and `data.table::`. We want to clarify why this book emphasises `data.table::` over the tidyverse.*

- **Philosophical Differences**:  
  The tidyverse provides a “grammar of data manipulation,” encouraging a functional and pipeline-oriented style. This can be appealing—like cooking with pre-measured spice blends that ensure consistent flavour. But it also layers abstractions that may obscure what happens under the hood.  
  With `data.table::`, you often write more explicit and compact code. It may initially feel less “elegant” than tidyverse pipelines, but it reduces hidden complexity. Instead of a chain of pipes `%>%`, you have a straightforward `[ , .() , by= ]` structure that leaves fewer surprises lurking beneath.

- **Performance Considerations**:  
  While the tidyverse prioritises readability and a uniform grammar, `data.table::` consistently outperforms it in terms of speed, especially on large datasets. If your work involves massive computations—like processing tens of millions of rows—the performance difference is non-trivial.

- **Direct Control and Transparency**:  
  `data.table::` does not hide copies of data or silently change column types. It expects you to be explicit and in control. This transparency can prevent subtle bugs and performance bottlenecks. The tidyverse's philosophy encourages a more “magical” style, automatically handling many details. This can be convenient, but for deep learning and precise control, `data.table::` fosters better craftsmanship.

- **Mathematical Aptness**:  
  Many statistical or data transformations translate neatly into `data.table::` operations. If you want to go from concept (mathematical formula) to code, `data.table::` supports a style that keeps the mental overhead low. For instance, if you think of computing group means:
  
  $$
  \bar{x}_g = \frac{\sum_{i \in g}x_i}{|g|}
  $$
  
  With `data.table::`, you write something like `[ , mean(x), by=g]`, which almost reads like the math itself.

- **Learning Curve and Knowledge Depth**:  
  While the tidyverse is easy to pick up initially, `data.table::` promotes a deeper understanding of R's internals. As you hone these skills, you become a more versatile and knowledgeable programmer, capable of handling challenges that might stump a tidyverse-only user.

*In a world of multiple paths, we have chosen one that emphasises performance, transparency, and closeness to the mathematical essence of data manipulation. Consider it the path of the artisan baker who shapes each loaf by hand rather than relying on pre-mixed dough. Both approaches yield bread, but one gives you more insight and mastery over the process.*

---

### Demonstration: Contrasting Approaches

To illustrate the difference, imagine we have a dataset of daily temperatures collected from a meteorological station. Suppose we have access to a CSV online:

- **Setup**:


``` r
temp_data <- data.table::fread("./data/3878388.csv")

# Assume the data has columns: date, temperature, month
# We want the mean temperature per month

# monthly_temp <- temp_data[ , .(mean_temp = mean(temperature, na.rm = TRUE)), by = month]
# print(monthly_temp)

# # Visualise with ggplot2::
# ggplot2::ggplot(data = monthly_temp, ggplot2::aes(x = month, y = mean_temp)) +
#   ggplot2::geom_line(colour = "blue") +
#   ggplot2::geom_point(size = 3, colour = "red") +
#   ggplot2::labs(
#     title = "Average Monthly Temperature",
#     x = "Month",
#     y = "Mean Temperature (°C)"
#   )
```

This snippet shows how straightforward and efficient it is with `data.table::`. In a tidyverse approach, you might pipeline multiple steps and rely on a different mental model. While tidyverse code can look “clean,” it may hide complexities and cost performance.

*By embracing Base R and `data.table::`, you gain a firm grounding in essential concepts and computational thinking. As a result, you become adept at reading, understanding, and optimising code, which is the very spirit of programming philosophies we wish to foster in this book.*


## 2.4 Coding Practices: Style, Organisation, and Project Structure

*Imagine walking into a well-organised kitchen—ingredients labelled, utensils neatly placed, and a logical flow from chopping block to stove. Programming is no different. Good coding practices ensure that your R code is maintainable, readable, and efficient. Style, organisation, and a solid project structure free you from confusion, allowing you to focus on the core statistical tasks.*

- **Readability and Style Guidelines**:  
  - **Consistent Naming Conventions**: Use meaningful variable names, like `mean_sepal_length` rather than `msl`. This helps others (and your future self) understand what the code is doing.  
  - **Indentation and Spacing**: Indent code blocks and space around operators. For instance:

``` r
x <- 1:10
mean_x <- mean(x)
```
This is more readable than `x<-1:10;mean_x=mean(x)`.
  
- **Commenting**: Add comments to clarify complex logic or important reasoning steps. However, avoid over-commenting trivial code. Think of comments as signposts, not wallpaper.

- **Organisation of Scripts**:  
  - **Modular Structure**: Split code into smaller scripts or functions. Instead of one massive file, organise your code by functionality—data loading, data cleaning, analysis, visualisation.  
  - **Functions for Repetition**: If you find yourself repeating the same block of code multiple times, convert it into a function. For example, if you frequently compute group means:

``` r
group_means <- function(DT, group_col, value_col) {
  DT[ , .(mean_value = mean(get(value_col), na.rm = TRUE)), by = group_col]
}
```
    This encapsulates logic and avoids copy-paste errors.

- **Project Structure**:  
  - **Dedicated Directories**: Keep raw data in a `data/` folder, scripts in `R/` or `src/`, and output (plots, tables) in `output/`.  
  - **Version Control**: Use Git for tracking changes. This helps you revert to previous states and work collaboratively with others.  
  - **Configuration and Parameters**: Store parameters in a separate file, so changing a dataset path or a model parameter does not require searching through multiple code files.

- **Mathematical Tie-In**:  
  Good style does not just make code pretty; it mirrors mathematical clarity. Just as you present equations step-by-step when explaining complex formulas:
  
  $$
  \mu = \frac{1}{n}\sum_{i=1}^{n}x_i
  $$
  
  If you wrote this formula in a messy way, it would confuse readers. Similarly, messy code confuses analysts.

*In short, proper coding practices are the foundation of sustainable, understandable, and error-resistant work. When your code is well-structured, you can confidently build complex statistical analyses without losing your way.*


### Demonstration: Styled Code Snippet

Below is an example of a small script that reads a dataset, computes a summary, and plots a result—all while following good coding conventions:


``` r
# # Load data from a public source
# car_data <- data.table::data.table(datasets::mtcars)

# # Compute mean mileage by brand
# mileage_summary <- car_data[ , .(avg_mileage = mean(mileage, na.rm = TRUE)), by = brand]

# # Plot results with ggplot2::
# ggplot2::ggplot(data = mileage_summary, ggplot2::aes(x = brand, y = avg_mileage)) +
#   ggplot2::geom_col(fill = "steelblue") +
#   ggplot2::coord_flip() +
#   ggplot2::labs(title = "Average Mileage by Brand",
#                 x = "Car Brand",
#                 y = "Average Mileage (MPG)")
```


---

## 2.5 File Input/Output and Working Directories

*Data is the raw ingredient of any analysis. Just as a chef retrieves ingredients from the pantry and places finished dishes on serving plates, you will frequently read data into R and write results out. Understanding how to handle file input/output (I/O) and working directories ensures you always know where your “ingredients” and “final products” are located.*

- **Working Directory Basics**:  
  The working directory is the current folder from which R reads files and to which it writes outputs. Think of it as the kitchen counter you're currently using.  
  - **Checking and Setting**:  

``` r
getwd()  # Check current directory
setwd("path/to/your/project")  # Set a new working directory
```
  
  Ensuring you are in the correct directory means you can use relative paths (like `data/mydata.csv`), which helps maintain portability across machines.

- **Reading Data**:  
  Data often comes in CSV, Excel, or database formats. Using `data.table::fread()` is efficient for CSVs:

``` r
# sales_data <- data.table::fread("data/sales_transactions.csv")
```
  
If you deal with large files, `fread()` is faster than `read.csv()` and handles automatic type inference well. For Excel files, consider `readxl::read_excel()` if you must—though we have to remind ourselves to call it explicitly:


``` r
# excel_data <- readxl::read_excel("data/financial_report.xlsx")
```

- **Writing Data and Results**:  
After processing, you might save summaries or predictions. You can write CSV files using:

``` r
# data.table::fwrite(sales_data, "output/sales_summary.csv")
```

This ensures a record of your processed data for future reference. If you produce plots, you can save them as images:


``` r
# ggplot2::ggsave("output/mileage_plot.png", width = 6, height = 4)
```

- **Mathematical and Logical Considerations**:  
  File I/O can be viewed as $ D_{in} \rightarrow T \rightarrow D_{out} $, where $ D_{in} $ is the input data, $ T $ transformations, and $ D_{out} $ the output. Maintaining a logical directory structure makes this flow straightforward. Just as knowing the source of your data $ D_{in} $ ensures you can replicate analyses, controlling where $ D_{out} $ goes ensures future reproducibility.

- **Keeping Track of Multiple Datasets**:  
  If you have multiple data sources—like demographic data, survey responses, and sales transactions—store them in well-named subfolders. Clear organisation prevents confusion and accidental overwriting of files. It's like having different shelves for spices, grains, and produce in your kitchen.

*By mastering file I/O and working directories, you ensure that data always flows smoothly from raw sources to final outputs, just as ingredients move gracefully from pantry to plate.*


### Demonstration: Combining Multiple Inputs

Suppose you have sales data split by month. You can read each month's file, combine them, and then write out a combined dataset:


``` r
# # Assume monthly files like Jan.csv, Feb.csv, etc.
# files <- c("data/Jan.csv", "data/Feb.csv", "data/Mar.csv")

# # Read and combine
# all_sales <- data.table::rbindlist(lapply(files, data.table::fread))

# # Output a combined CSV
# data.table::fwrite(all_sales, "output/all_sales_combined.csv")

# # Quick plot of monthly totals:
# monthly_totals <- all_sales[ , .(total_sales = sum(sales)), by = month]
# ggplot2::ggplot(data = monthly_totals, ggplot2::aes(x = month, y = total_sales)) +
#   ggplot2::geom_line(colour = "blue") +
#   ggplot2::geom_point(size = 3, colour = "red") +
#   ggplot2::labs(title = "Monthly Total Sales",
#                 x = "Month",
#                 y = "Total Sales")
```


---

## 2.6 Reproducible Research with R Markdown and Quarto

*Reproducibility is the hallmark of good science. Just as a recipe should allow another chef to recreate a dish, your statistical analysis should allow others (and your future self) to reproduce the same results. Tools like R Markdown and Quarto turn static code and results into dynamic documents that integrate code, narrative, and output in a single file.*

- **What Are R Markdown and Quarto?**:  
  - **R Markdown**: A file format (`.Rmd`) that weaves together prose, code, and results. When “knitted,” it generates reports, HTML pages, or PDFs that show your code, outputs (tables, plots), and explanation.  
  - **Quarto**: A next-generation tool building on the success of R Markdown, Quarto supports multiple languages (not just R) and offers extended capabilities for reproducible workflows and publishing.
  
  Both serve to make your analyses transparent. Instead of a separate Word document and code script, you have a unified, executable document.

- **Mathematical Integration**:  
  In these documents, you can write equations in LaTeX-like syntax:
  
  $$
  \hat{\beta} = (X^\top X)^{-1}X^\top y
  $$
  
  This formula might represent the least squares estimate in a linear model. Displaying it alongside code that computes `lm()` results ensures readers see both theory and implementation together.

- **Code Blocks and Results**: In R Markdown or Quarto, code blocks are designated sections that execute R code when knitted. For instance, after explaining how to compute mean monthly sales, you can show the actual code and produce the resulting table and plot inline. This fosters trust that what's shown in the text is actually what the code produces.[^4]

[^4]: Ensuring the code that produced the results is visible or at least accessible enhances scientific transparency and fosters collaboration.


- **Version Control and Automation**:  
  By combining code and prose, you eliminate the risk of out-of-date results. If the underlying data changes, just re-run (knit) the document, and all figures, tables, and conclusions update automatically. This is the equivalent of always having a fresh loaf of bread ready when someone asks for a taste—no stale, old summaries.

- **Exporting and Sharing**:  
  R Markdown and Quarto can produce multiple output formats—HTML for interactive viewing, PDF for academic publication, Word for corporate reports. Share these documents with collaborators, managers, or journals, ensuring everyone sees exactly what you did.

*In short, reproducibility tools let your analysis tell a complete story: from raw data, through code and mathematics, to final interpretations. It ensures your work can be trusted, extended, and learned from by others.*


### Demonstration: A Simple R Markdown/Quarto Workflow

You might have a `.Rmd` or `.qmd` file with the following structure:


```` r
# ---
# title: "Monthly Sales Report"
# author: "Your Name"
# date: "`r Sys.Date()`"
# output: html_document
# ---

# # Introduction

# This report summarises the monthly sales from January to March, providing mean values and a visual representation.

# ```{r}
# monthly_data <- data.table::fread("output/all_sales_combined.csv")
# monthly_avg <- monthly_data[ , .(avg_sales = mean(sales)), by = month]
# print(monthly_avg)

# ggplot2::ggplot(data = monthly_avg, ggplot2::aes(x = month, y = avg_sales)) +
#   ggplot2::geom_col(fill = "lightblue") +
#   ggplot2::labs(title = "Average Monthly Sales",
#                 x = "Month",
#                 y = "Average Sales")
````
  
# Conclusion

As shown above, the average sales vary by month, indicating potential seasonal trends. Further analysis could incorporate additional variables like marketing spend or regional breakdown.
```

When you knit this file, it generates a self-contained report showing your code, output, and narrative in one place. If you change the data file or computations, re-knitting updates all results instantly.

---

*By adopting strong coding practices, handling file I/O with care, and harnessing reproducible workflows, you lay the groundwork for analyses that are not only accurate and meaningful, but also trusted, understood, and easily shared.*  
