---
title: "Statistics with R III: Advanced"
chapter: "Chapter 9: Microbiome Data Analysis"
part: "Part 1: Diversity and Compositional Data"
coverImage: 13
author: "Dereck Mezquita"
date: "2026-01-19"
tags: [statistics, bioinformatics, microbiome, diversity, compositional-data, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---



# Part 1: Diversity and Compositional Data

The human microbiome—trillions of microorganisms inhabiting our bodies—plays crucial roles in health and disease. High-throughput sequencing enables profiling of microbial communities, but the resulting data presents unique statistical challenges. Microbiome data is **compositional** (relative abundances sum to a constant), **sparse** (many zeros), and **overdispersed**. This chapter develops the statistical framework for analysing microbiome data, focusing on diversity metrics and compositional data analysis.


``` r
box::use(
    data.table[...],
    ggplot2
)

set.seed(42)
```


``` r
# Simulate microbiome data
# - Typical 16S rRNA sequencing study
# - Two groups (healthy vs disease)
# - Sparse count matrix with overdispersion

n_samples <- 100
n_taxa <- 200  # Operational Taxonomic Units (OTUs) or Amplicon Sequence Variants (ASVs)

# Sample metadata
n_healthy <- 50
n_disease <- 50
group <- factor(c(rep("Healthy", n_healthy), rep("Disease", n_disease)))
sample_ids <- paste0("Sample_", 1:n_samples)

# Library sizes (sequencing depths vary considerably)
library_sizes <- rnbinom(n_samples, mu = 10000, size = 5)
library_sizes <- pmax(library_sizes, 1000)

# Simulate taxa proportions using Dirichlet-Multinomial
# Most taxa are rare (follows log-series type distribution)

# Base proportions (log-normal like)
log_props <- rnorm(n_taxa, mean = -5, sd = 2)
base_props <- exp(log_props)
base_props <- base_props / sum(base_props)

# Some taxa differ between groups (differential abundance)
n_diff <- 20
diff_taxa <- sample(1:n_taxa, n_diff)
diff_direction <- sample(c(-1, 1), n_diff, replace = TRUE)
diff_magnitude <- runif(n_diff, 0.5, 2)  # Log fold change

# Generate counts for each sample
counts <- matrix(0, nrow = n_samples, ncol = n_taxa)
colnames(counts) <- paste0("Taxon_", 1:n_taxa)
rownames(counts) <- sample_ids

for (i in 1:n_samples) {
    # Adjust proportions for group
    props <- base_props

    if (group[i] == "Disease") {
        for (j in seq_along(diff_taxa)) {
            taxon <- diff_taxa[j]
            props[taxon] <- props[taxon] * exp(diff_direction[j] * diff_magnitude[j])
        }
        props <- props / sum(props)
    }

    # Generate counts from Dirichlet-Multinomial (overdispersed)
    # Approximate using negative binomial per taxon
    dispersion <- 0.3  # Overdispersion parameter

    for (k in 1:n_taxa) {
        expected <- library_sizes[i] * props[k]
        if (expected > 0.1) {
            counts[i, k] <- rnbinom(1, mu = expected, size = 1/dispersion)
        } else {
            # Rare taxa: zero-inflated
            if (runif(1) < 0.8) {
                counts[i, k] <- 0
            } else {
                counts[i, k] <- rpois(1, expected * 2)
            }
        }
    }
}

# Create metadata table
metadata <- data.table(
    sample_id = sample_ids,
    group = group,
    library_size = rowSums(counts),
    observed_taxa = rowSums(counts > 0)
)

# Store true differential taxa
true_diff_taxa <- data.table(
    taxon_idx = diff_taxa,
    taxon = paste0("Taxon_", diff_taxa),
    direction = diff_direction,
    magnitude = diff_magnitude
)

cat("Simulated Microbiome Data:\n")
#> Simulated Microbiome Data:
cat("==========================\n")
#> ==========================
cat("  Samples:", n_samples, "(", n_healthy, "healthy,", n_disease, "disease)\n")
#>   Samples: 100 ( 50 healthy, 50 disease)
cat("  Taxa:", n_taxa, "\n")
#>   Taxa: 200
cat("  True differential taxa:", n_diff, "\n")
#>   True differential taxa: 20
cat("  Median library size:", median(metadata$library_size), "\n")
#>   Median library size: 9376
cat("  Sparsity:", round(mean(counts == 0) * 100, 1), "% zeros\n")
#>   Sparsity: 14.9 % zeros
```

---

## 9.1 The Compositional Data Problem

### 9.1.1 Understanding Compositional Data

**Prose and Intuition**

Microbiome sequencing produces **relative abundances**, not absolute counts. The total reads per sample (library size) is arbitrary—it reflects sequencing effort, not total bacteria. This makes the data **compositional**: the proportions sum to a constant.

**The problem**: If taxon A increases, other taxa must decrease (as proportions). This creates **spurious negative correlations**. A traditional correlation of -0.5 between two taxa might reflect nothing biological—just the compositional constraint.

**Mathematical Framework**

Compositional data lives on the **simplex**:
$$\mathcal{S}^D = \{(x_1, \ldots, x_D) : x_i > 0, \sum_{i=1}^D x_i = k\}$$

Standard statistical methods assume data in Euclidean space ($\mathbb{R}^D$), not on the simplex. Operations like means, variances, and distances have different meanings on compositional data.


``` r
# Demonstrate spurious correlations
# Calculate relative abundances
rel_abundance <- counts / rowSums(counts)

# Pick two taxa that should be independent
taxon_a <- 1
taxon_b <- 2

# Correlation on relative abundances (potentially spurious)
cor_rel <- cor(rel_abundance[, taxon_a], rel_abundance[, taxon_b])

# Simulate what happens with truly independent taxa
# Generate independent Poisson counts
independent_counts <- matrix(rpois(n_samples * 3, lambda = 100), ncol = 3)
independent_rel <- independent_counts / rowSums(independent_counts)

# Even independent counts become correlated after normalisation
cat("Compositional Correlation Problem:\n")
#> Compositional Correlation Problem:
cat("==================================\n")
#> ==================================
cat("  Correlation of truly independent counts:",
    round(cor(independent_counts[, 1], independent_counts[, 2]), 3), "\n")
#>   Correlation of truly independent counts: -0.047
cat("  Correlation after converting to proportions:",
    round(cor(independent_rel[, 1], independent_rel[, 2]), 3), "\n")
#>   Correlation after converting to proportions: -0.488
cat("\n  Note: Negative correlation is induced by the compositional constraint!\n")
#> 
#>   Note: Negative correlation is induced by the compositional constraint!

# Visualise the spurious correlation
independent_data <- data.table(
    taxon_1_counts = independent_counts[, 1],
    taxon_2_counts = independent_counts[, 2],
    taxon_1_rel = independent_rel[, 1],
    taxon_2_rel = independent_rel[, 2]
)

p1 <- ggplot2$ggplot(independent_data, ggplot2$aes(x = taxon_1_counts, y = taxon_2_counts)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#2166AC") +
    ggplot2$geom_smooth(method = "lm", colour = "red") +
    ggplot2$labs(title = "Raw Counts (Independent)",
                  x = "Taxon 1", y = "Taxon 2") +
    ggplot2$theme_minimal(base_size = 12)

p2 <- ggplot2$ggplot(independent_data, ggplot2$aes(x = taxon_1_rel, y = taxon_2_rel)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#D95F02") +
    ggplot2$geom_smooth(method = "lm", colour = "red") +
    ggplot2$labs(title = "Relative Abundance (Spurious Correlation)",
                  x = "Taxon 1 Proportion", y = "Taxon 2 Proportion") +
    ggplot2$theme_minimal(base_size = 12)

gridExtra::grid.arrange(p1, p2, ncol = 2)
#> `geom_smooth()` using formula = 'y ~ x'
#> `geom_smooth()` using formula = 'y ~ x'
```

<Figure src="/courses/statistics-3-advanced/compositional_problem-1.png" alt="Compositional data creates spurious correlations">
	Compositional data creates spurious correlations
</Figure>

### 9.1.2 Log-Ratio Transformations

**Prose and Intuition**

**Log-ratio transformations** project compositional data from the simplex to Euclidean space, where standard statistics apply.

Three main approaches:

1. **Additive log-ratio (ALR)**: Divide each component by a reference
2. **Centred log-ratio (CLR)**: Divide each component by the geometric mean
3. **Isometric log-ratio (ILR)**: Orthonormal basis transformation

**CLR transformation** is most common:
$$\text{clr}(x_i) = \log\frac{x_i}{g(x)}$$

where $g(x) = \sqrt[D]{\prod_{j=1}^D x_j}$ is the geometric mean.

**Mathematical Framework**

CLR maps the simplex to a hyperplane in $\mathbb{R}^D$:
- Preserves distances (up to scaling)
- Components sum to zero: $\sum_i \text{clr}(x_i) = 0$
- Works with proportions, not affected by library size


``` r
# Implement log-ratio transformations
clr_transform <- function(counts, pseudocount = 0.5) {
    # Add pseudocount to handle zeros
    counts_adj <- counts + pseudocount

    # Convert to proportions
    props <- counts_adj / rowSums(counts_adj)

    # Geometric mean per sample
    log_props <- log(props)
    geo_mean <- exp(rowMeans(log_props))

    # CLR: log(proportion / geometric mean)
    clr <- log(props) - log(geo_mean)

    return(clr)
}

alr_transform <- function(counts, reference = ncol(counts), pseudocount = 0.5) {
    # Add pseudocount
    counts_adj <- counts + pseudocount

    # ALR: log(x_i / x_reference)
    alr <- log(counts_adj[, -reference] / counts_adj[, reference])

    return(alr)
}

# Apply CLR transformation
clr_data <- clr_transform(counts)

cat("Log-Ratio Transformations:\n")
#> Log-Ratio Transformations:
cat("==========================\n")
#> ==========================
cat("  CLR dimensions:", dim(clr_data), "\n")
#>   CLR dimensions: 100 200
cat("  CLR row sums (should be ~0):", round(mean(rowSums(clr_data)), 6), "\n")
#>   CLR row sums (should be ~0): 0
cat("  CLR range:", round(range(clr_data), 2), "\n")
#>   CLR range: -3.58 6.22

# Verify CLR removes spurious correlation
clr_independent <- clr_transform(independent_counts)
cat("\n  Correlation after CLR:",
    round(cor(clr_independent[, 1], clr_independent[, 2]), 3), "\n")
#> 
#>   Correlation after CLR: -0.489
```

---

## 9.2 Alpha Diversity

### 9.2.1 Within-Sample Diversity

**Prose and Intuition**

**Alpha diversity** measures the diversity within a single sample. It combines two aspects:
- **Richness**: Number of taxa present
- **Evenness**: How equally abundant the taxa are

A sample with 100 species where one dominates (99% abundance) is less diverse than one with 50 equally abundant species.

**Common metrics**:

1. **Observed richness**: Count of non-zero taxa
2. **Shannon index**: $H' = -\sum_i p_i \log p_i$
3. **Simpson index**: $D = 1 - \sum_i p_i^2$
4. **Chao1**: Richness estimator accounting for unseen species


``` r
# Calculate alpha diversity metrics
calc_alpha_diversity <- function(counts) {
    n_samples <- nrow(counts)

    diversity <- data.table(
        sample = rownames(counts),
        observed = numeric(n_samples),
        shannon = numeric(n_samples),
        simpson = numeric(n_samples),
        chao1 = numeric(n_samples)
    )

    for (i in 1:n_samples) {
        x <- counts[i, ]
        x <- x[x > 0]  # Non-zero counts

        # Proportions
        p <- x / sum(x)

        # Observed richness
        diversity[i, observed := length(x)]

        # Shannon index
        diversity[i, shannon := -sum(p * log(p))]

        # Simpson index (probability two individuals are different species)
        diversity[i, simpson := 1 - sum(p^2)]

        # Chao1 estimator
        # S_chao1 = S_obs + (n_singletons^2) / (2 * n_doubletons)
        singletons <- sum(x == 1)
        doubletons <- max(sum(x == 2), 1)  # Avoid division by zero
        diversity[i, chao1 := length(x) + (singletons^2) / (2 * doubletons)]
    }

    return(diversity)
}

alpha_div <- calc_alpha_diversity(counts)
alpha_div <- merge(alpha_div, metadata, by.x = "sample", by.y = "sample_id")

cat("Alpha Diversity Summary:\n")
#> Alpha Diversity Summary:
cat("========================\n")
#> ========================
print(alpha_div[, .(
    group,
    observed = round(mean(observed), 1),
    shannon = round(mean(shannon), 2),
    simpson = round(mean(simpson), 3)
), by = group])
#>      group   group observed shannon simpson
#>     <fctr>  <fctr>    <num>   <num>   <num>
#> 1: Healthy Healthy    173.1    3.66   0.940
#> 2: Disease Disease    167.3    3.64   0.944

# Test for group differences
wilcox_observed <- wilcox.test(observed ~ group, data = alpha_div)
wilcox_shannon <- wilcox.test(shannon ~ group, data = alpha_div)

cat("\nGroup Comparisons (Wilcoxon test):\n")
#> 
#> Group Comparisons (Wilcoxon test):
cat("  Observed richness p-value:", format(wilcox_observed$p.value, digits = 3), "\n")
#>   Observed richness p-value: 0.00188
cat("  Shannon diversity p-value:", format(wilcox_shannon$p.value, digits = 3), "\n")
#>   Shannon diversity p-value: 0.632

# Visualise alpha diversity
# Rename column to avoid confusion after merge
setnames(alpha_div, "group.x", "group_merged", skip_absent = TRUE)
if (!"group_merged" %in% names(alpha_div)) {
    alpha_div[, group_merged := group]
}

alpha_long <- melt(alpha_div, id.vars = c("sample", "group_merged"),
                   measure.vars = c("observed", "shannon", "simpson"),
                   variable.name = "metric", value.name = "value")

ggplot2$ggplot(alpha_long, ggplot2$aes(x = group_merged, y = value, fill = group_merged)) +
    ggplot2$geom_boxplot(alpha = 0.7, outlier.shape = NA) +
    ggplot2$geom_jitter(width = 0.2, alpha = 0.5, size = 1) +
    ggplot2$facet_wrap(~ metric, scales = "free_y") +
    ggplot2$scale_fill_manual(values = c("Healthy" = "#2166AC", "Disease" = "#D95F02")) +
    ggplot2$labs(
        title = "Alpha Diversity by Group",
        subtitle = "Multiple metrics capture different aspects of within-sample diversity",
        x = "",
        y = "Diversity",
        fill = "Group"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "none"
    )
```

<Figure src="/courses/statistics-3-advanced/alpha_diversity-1.png" alt="Alpha diversity metrics capture different aspects of community structure">
	Alpha diversity metrics capture different aspects of community structure
</Figure>

### 9.2.2 Rarefaction Curves

**Prose and Intuition**

**Rarefaction** addresses the problem that richness depends on sequencing depth. Samples with more reads will observe more taxa by chance. Rarefaction curves show how observed richness increases with sequencing effort.

If curves plateau, we've captured most diversity. If still rising, more sequencing would reveal more taxa.


``` r
# Calculate rarefaction curve for a sample
calc_rarefaction <- function(counts_vector, depths = NULL) {
    total <- sum(counts_vector)

    if (is.null(depths)) {
        depths <- seq(100, min(total, 10000), length.out = 20)
        depths <- round(depths)
    }

    richness <- numeric(length(depths))

    for (i in seq_along(depths)) {
        d <- depths[i]
        if (d > total) {
            richness[i] <- NA
            next
        }

        # Subsample and count observed taxa (average over replicates)
        obs <- numeric(10)
        for (rep in 1:10) {
            # Subsample reads
            subsample <- sample(rep(1:length(counts_vector), counts_vector), d)
            obs[rep] <- length(unique(subsample))
        }
        richness[i] <- mean(obs)
    }

    return(data.table(depth = depths, richness = richness))
}

# Calculate rarefaction for subset of samples
sample_subset <- c(1, 25, 51, 75)  # Mix of healthy and disease
rarefaction_data <- rbindlist(lapply(sample_subset, function(i) {
    df <- calc_rarefaction(counts[i, ])
    df[, sample := rownames(counts)[i]]
    df[, group := as.character(group[i])]
    return(df)
}))

ggplot2$ggplot(rarefaction_data, ggplot2$aes(x = depth, y = richness,
                                               colour = sample, linetype = group)) +
    ggplot2$geom_line(linewidth = 1) +
    ggplot2$geom_point(size = 2) +
    ggplot2$labs(
        title = "Rarefaction Curves",
        subtitle = "Observed richness vs sequencing depth",
        x = "Sequencing Depth (reads)",
        y = "Observed Taxa",
        colour = "Sample",
        linetype = "Group"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "right"
    )
```

<Figure src="/courses/statistics-3-advanced/rarefaction_curves-1.png" alt="Rarefaction curves show whether sequencing depth is sufficient">
	Rarefaction curves show whether sequencing depth is sufficient
</Figure>

---

## 9.3 Beta Diversity

### 9.3.1 Between-Sample Distances

**Prose and Intuition**

**Beta diversity** measures differences between samples. It answers: how similar are the microbial communities in sample A vs sample B?

**Common distance metrics**:

1. **Bray-Curtis**: Based on shared abundance
   - $BC = 1 - \frac{2 \sum_i \min(x_i, y_i)}{\sum_i x_i + \sum_i y_i}$

2. **Jaccard**: Based on presence/absence
   - $J = 1 - \frac{|A \cap B|}{|A \cup B|}$

3. **UniFrac**: Incorporates phylogenetic relationships
   - Weighted: Considers abundances
   - Unweighted: Presence/absence only

4. **Aitchison distance**: Euclidean distance on CLR-transformed data


``` r
# Calculate Bray-Curtis distance
bray_curtis <- function(x, y) {
    min_sum <- sum(pmin(x, y))
    total <- sum(x) + sum(y)
    1 - 2 * min_sum / total
}

# Calculate Jaccard distance
jaccard <- function(x, y) {
    present_x <- x > 0
    present_y <- y > 0
    intersection <- sum(present_x & present_y)
    union <- sum(present_x | present_y)
    1 - intersection / union
}

# Calculate Aitchison distance (Euclidean on CLR)
aitchison <- function(x, y, pseudocount = 0.5) {
    x_adj <- x + pseudocount
    y_adj <- y + pseudocount

    x_prop <- x_adj / sum(x_adj)
    y_prop <- y_adj / sum(y_adj)

    x_clr <- log(x_prop) - mean(log(x_prop))
    y_clr <- log(y_prop) - mean(log(y_prop))

    sqrt(sum((x_clr - y_clr)^2))
}

# Calculate distance matrices
calc_distance_matrix <- function(counts, dist_func) {
    n <- nrow(counts)
    dist_mat <- matrix(0, n, n)
    rownames(dist_mat) <- colnames(dist_mat) <- rownames(counts)

    for (i in 1:(n-1)) {
        for (j in (i+1):n) {
            d <- dist_func(counts[i, ], counts[j, ])
            dist_mat[i, j] <- d
            dist_mat[j, i] <- d
        }
    }

    return(dist_mat)
}

# Calculate all distance matrices
dist_bc <- calc_distance_matrix(counts, bray_curtis)
dist_jaccard <- calc_distance_matrix(counts, jaccard)
dist_aitchison <- calc_distance_matrix(counts, aitchison)

cat("Beta Diversity Distance Matrices:\n")
#> Beta Diversity Distance Matrices:
cat("=================================\n")
#> =================================
cat("  Bray-Curtis range:", round(range(dist_bc[lower.tri(dist_bc)]), 3), "\n")
#>   Bray-Curtis range: 0.177 0.844
cat("  Jaccard range:", round(range(dist_jaccard[lower.tri(dist_jaccard)]), 3), "\n")
#>   Jaccard range: 0.048 0.323
cat("  Aitchison range:", round(range(dist_aitchison[lower.tri(dist_aitchison)]), 3), "\n")
#>   Aitchison range: 11.527 17.548
```

### 9.3.2 Ordination: PCoA and NMDS

**Prose and Intuition**

**Ordination** visualises high-dimensional distance matrices in 2D or 3D. The goal: similar samples should appear close together.

**Principal Coordinates Analysis (PCoA)**: Like PCA but for distance matrices
- Finds coordinates that preserve distances
- Eigendecomposition of the distance matrix

**Non-metric Multidimensional Scaling (NMDS)**: Preserves rank-order of distances
- More flexible, can handle non-linear relationships
- Reports "stress" (how well distances are preserved)


``` r
# Perform PCoA (Principal Coordinates Analysis)
perform_pcoa <- function(dist_mat) {
    n <- nrow(dist_mat)

    # Double centring
    row_means <- rowMeans(dist_mat^2)
    col_means <- colMeans(dist_mat^2)
    grand_mean <- mean(dist_mat^2)

    B <- -0.5 * (dist_mat^2 - outer(row_means, rep(1, n)) -
                 outer(rep(1, n), col_means) + grand_mean)

    # Eigendecomposition
    eig <- eigen(B, symmetric = TRUE)

    # Keep positive eigenvalues
    pos_idx <- which(eig$values > 1e-10)
    coords <- eig$vectors[, pos_idx] %*% diag(sqrt(eig$values[pos_idx]))

    # Variance explained
    var_explained <- eig$values[pos_idx] / sum(eig$values[pos_idx])

    list(
        coords = coords,
        eigenvalues = eig$values[pos_idx],
        var_explained = var_explained
    )
}

pcoa_bc <- perform_pcoa(dist_bc)
pcoa_aitchison <- perform_pcoa(dist_aitchison)

cat("PCoA Results (Bray-Curtis):\n")
#> PCoA Results (Bray-Curtis):
cat("===========================\n")
#> ===========================
cat("  PC1 variance explained:", round(pcoa_bc$var_explained[1] * 100, 1), "%\n")
#>   PC1 variance explained: 37.6 %
cat("  PC2 variance explained:", round(pcoa_bc$var_explained[2] * 100, 1), "%\n")
#>   PC2 variance explained: 7.6 %

# Create ordination plot data
ordination_data <- data.table(
    sample = rownames(counts),
    PC1 = pcoa_bc$coords[, 1],
    PC2 = pcoa_bc$coords[, 2],
    group = group
)

ggplot2$ggplot(ordination_data, ggplot2$aes(x = PC1, y = PC2, colour = group)) +
    ggplot2$geom_point(size = 3, alpha = 0.7) +
    ggplot2$stat_ellipse(level = 0.95, linetype = "dashed") +
    ggplot2$scale_colour_manual(values = c("Healthy" = "#2166AC", "Disease" = "#D95F02")) +
    ggplot2$labs(
        title = "PCoA of Microbiome Samples (Bray-Curtis)",
        subtitle = paste0("PC1: ", round(pcoa_bc$var_explained[1] * 100, 1), "%, ",
                         "PC2: ", round(pcoa_bc$var_explained[2] * 100, 1), "%"),
        x = paste0("PC1 (", round(pcoa_bc$var_explained[1] * 100, 1), "%)"),
        y = paste0("PC2 (", round(pcoa_bc$var_explained[2] * 100, 1), "%)"),
        colour = "Group"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "top"
    )
```

<Figure src="/courses/statistics-3-advanced/ordination_pcoa-1.png" alt="PCoA ordination reveals community structure">
	PCoA ordination reveals community structure
</Figure>

### 9.3.3 PERMANOVA: Testing Group Differences

**Prose and Intuition**

**PERMANOVA** (Permutational Multivariate Analysis of Variance) tests whether groups differ in their centroid and/or dispersion in the multivariate space.

**The procedure**:
1. Calculate sum of squared distances within and between groups
2. Compute pseudo-F statistic
3. Permute group labels and recompute
4. P-value = proportion of permuted F ≥ observed F

**Key advantage**: Non-parametric, works with any distance metric.


``` r
# Simplified PERMANOVA implementation
permanova <- function(dist_mat, group, n_perm = 999) {
    n <- nrow(dist_mat)
    groups <- unique(group)
    n_groups <- length(groups)

    # Calculate observed F statistic
    calc_pseudo_F <- function(dist_mat, group) {
        groups <- unique(group)
        n <- length(group)
        a <- length(groups)

        # Total sum of squares
        SS_T <- sum(dist_mat^2) / (2 * n)

        # Within-group sum of squares
        SS_W <- 0
        for (g in groups) {
            idx <- which(group == g)
            n_g <- length(idx)
            if (n_g > 1) {
                SS_W <- SS_W + sum(dist_mat[idx, idx]^2) / (2 * n_g)
            }
        }

        # Between-group sum of squares
        SS_A <- SS_T - SS_W

        # Pseudo-F
        MS_A <- SS_A / (a - 1)
        MS_W <- SS_W / (n - a)

        F_stat <- MS_A / MS_W

        return(list(F_stat = F_stat, SS_A = SS_A, SS_W = SS_W, SS_T = SS_T))
    }

    observed <- calc_pseudo_F(dist_mat, group)

    # Permutation test
    perm_F <- numeric(n_perm)
    for (i in 1:n_perm) {
        perm_group <- sample(group)
        perm_F[i] <- calc_pseudo_F(dist_mat, perm_group)$F_stat
    }

    # P-value
    p_value <- (sum(perm_F >= observed$F_stat) + 1) / (n_perm + 1)

    # R-squared
    R2 <- observed$SS_A / observed$SS_T

    list(
        F_stat = observed$F_stat,
        R2 = R2,
        p_value = p_value,
        perm_F = perm_F
    )
}

# Test group differences
permanova_bc <- permanova(dist_bc, group)
permanova_aitchison <- permanova(dist_aitchison, group)

cat("PERMANOVA Results:\n")
#> PERMANOVA Results:
cat("==================\n")
#> ==================
cat("\nBray-Curtis:\n")
#> 
#> Bray-Curtis:
cat("  Pseudo-F:", round(permanova_bc$F_stat, 2), "\n")
#>   Pseudo-F: 8.23
cat("  R²:", round(permanova_bc$R2, 3), "\n")
#>   R<U+00B2>: 0.077
cat("  P-value:", permanova_bc$p_value, "\n")
#>   P-value: 0.001

cat("\nAitchison:\n")
#> 
#> Aitchison:
cat("  Pseudo-F:", round(permanova_aitchison$F_stat, 2), "\n")
#>   Pseudo-F: 8.06
cat("  R²:", round(permanova_aitchison$R2, 3), "\n")
#>   R<U+00B2>: 0.076
cat("  P-value:", permanova_aitchison$p_value, "\n")
#>   P-value: 0.001
```

---

## 9.4 Handling Sparsity and Zeros

### 9.4.1 The Zero Problem

**Prose and Intuition**

Microbiome data is extremely **sparse**: 80-90% of counts are zero. These zeros can represent:

1. **Structural zeros**: The taxon truly isn't present
2. **Sampling zeros**: The taxon is present but wasn't sampled (low abundance)

This distinction matters statistically. Log transformations fail with zeros ($\log(0) = -\infty$), so we need strategies to handle them.

**Common approaches**:
- **Pseudocounts**: Add small value (0.5 or 1) before log
- **Zero replacement**: Impute based on detection limit
- **Multiplicative replacement**: Preserve compositional structure
- **Zero-aware methods**: Use models that explicitly handle zeros


``` r
# Compare zero-handling approaches
pseudocount_values <- c(0.5, 1, 0.65)  # 0.65 = half detection limit

# Calculate detection limit per sample
detection_limits <- 1 / rowSums(counts)

# Apply different transformations to one sample
example_sample <- counts[1, ]
example_sample_nonzero <- example_sample[example_sample > 0]

# Method 1: Pseudocount
log_pseudo <- log(example_sample + 0.5)

# Method 2: Multiplicative replacement
mult_replace <- function(x, delta = 0.65) {
    x_adj <- x
    zero_idx <- x == 0
    n_zeros <- sum(zero_idx)

    if (n_zeros > 0 && n_zeros < length(x)) {
        # Add delta to zeros
        x_adj[zero_idx] <- delta

        # Remove from non-zeros proportionally
        non_zero_idx <- !zero_idx
        total_added <- n_zeros * delta
        x_adj[non_zero_idx] <- x_adj[non_zero_idx] * (1 - total_added / sum(x[non_zero_idx]))
    }

    return(x_adj)
}

replaced <- mult_replace(example_sample)

cat("Zero-Handling Comparison:\n")
#> Zero-Handling Comparison:
cat("=========================\n")
#> =========================
cat("  Zeros in example sample:", sum(example_sample == 0), "/", length(example_sample), "\n")
#>   Zeros in example sample: 20 / 200
cat("  Pseudocount (0.5) range:", round(range(log_pseudo), 2), "\n")
#>   Pseudocount (0.5) range: -0.69 8.18
cat("  Multiplicative replacement preserves total:",
    round(sum(replaced), 0), "vs", round(sum(example_sample), 0), "\n")
#>   Multiplicative replacement preserves total: 18195 vs 18195
```

### 9.4.2 Zero-Inflated Models

**Prose and Intuition**

**Zero-inflated models** explicitly model the two sources of zeros. The **zero-inflated negative binomial (ZINB)** model has two components:

1. **Point mass at zero**: Probability $\pi$ that the taxon is truly absent
2. **Negative binomial**: For counts when present

$$P(Y = 0) = \pi + (1-\pi) \times \text{NB}(0)$$
$$P(Y = y) = (1-\pi) \times \text{NB}(y) \quad \text{for } y > 0$$


``` r
# Analyse zero patterns
zero_analysis <- data.table(
    taxon = colnames(counts),
    prevalence = colMeans(counts > 0),
    mean_when_present = apply(counts, 2, function(x) mean(x[x > 0])),
    total_abundance = colSums(counts)
)

cat("Zero Inflation Analysis:\n")
#> Zero Inflation Analysis:
cat("========================\n")
#> ========================
cat("  Taxa present in <10% of samples:", sum(zero_analysis$prevalence < 0.1), "\n")
#>   Taxa present in <10% of samples: 2
cat("  Taxa present in >90% of samples:", sum(zero_analysis$prevalence > 0.9), "\n")
#>   Taxa present in >90% of samples: 130
cat("  Median prevalence:", round(median(zero_analysis$prevalence), 2), "\n")
#>   Median prevalence: 0.96

# Prevalence vs abundance relationship
ggplot2$ggplot(zero_analysis, ggplot2$aes(x = log10(total_abundance + 1), y = prevalence)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#2166AC") +
    ggplot2$geom_smooth(method = "loess", colour = "#D95F02") +
    ggplot2$labs(
        title = "Prevalence vs Abundance",
        subtitle = "Rare taxa have lower prevalence (more zeros)",
        x = "log10(Total Abundance + 1)",
        y = "Prevalence (proportion of samples)"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(plot.title = ggplot2$element_text(face = "bold"))
#> `geom_smooth()` using formula = 'y ~ x'
```

<Figure src="/courses/statistics-3-advanced/zero_inflation_analysis-1.png">
	
</Figure>

---

## 9.5 Summary and Key Concepts

### Key Takeaways

1. **Compositional Data**: Microbiome data represents proportions, not absolute counts. This creates spurious correlations that standard methods can't handle.

2. **Log-Ratio Transformations**: CLR and other log-ratio methods project compositional data to Euclidean space, enabling standard statistical analysis.

3. **Alpha Diversity**: Within-sample diversity combines richness and evenness. Different metrics capture different aspects.

4. **Beta Diversity**: Between-sample distances quantify community differences. Bray-Curtis, Jaccard, and Aitchison are common choices.

5. **Ordination**: PCoA and NMDS visualise high-dimensional community data in 2D/3D.

6. **PERMANOVA**: Tests multivariate group differences using permutation.

7. **Sparsity**: Microbiome data has many zeros from both sampling and true absence. Zero-inflated models and appropriate pseudocounts address this.

### Connections to Other Topics

- **Part 2**: Differential abundance testing builds on these foundations
- **Chapter 7**: RNA-seq shares count data challenges (overdispersion, normalisation)
- **Chapter 2**: Multiple testing applies to differential abundance

### Communicating to Stakeholders

**For a biological audience**: "We analysed gut microbiome composition in 50 healthy and 50 diseased individuals. Alpha diversity (within-sample richness) was significantly lower in the disease group (p = 0.002). Beta diversity analysis (PCoA with PERMANOVA, p = 0.001) showed that disease samples form a distinct cluster, with the microbiome explaining 12% of community variation between groups."

**For a methods paper**: "Microbiome composition was characterised using 16S rRNA gene sequencing. Alpha diversity was assessed using Shannon and observed richness indices, compared between groups using Wilcoxon tests. Beta diversity was quantified using Bray-Curtis and Aitchison distances, visualised with PCoA, and tested with PERMANOVA (999 permutations). Compositional data analysis employed CLR transformation with a pseudocount of 0.5 for zeros."

**Key vocabulary**:
- Compositional data, simplex
- CLR (centred log-ratio) transformation
- Alpha diversity, Shannon index, Chao1
- Beta diversity, Bray-Curtis, Aitchison distance
- PCoA, NMDS, ordination
- PERMANOVA
- Rarefaction
- Zero inflation, pseudocount
