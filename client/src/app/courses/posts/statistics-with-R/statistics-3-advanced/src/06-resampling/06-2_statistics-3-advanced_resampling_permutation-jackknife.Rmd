---
title: "Statistics with R III: Advanced"
chapter: "Chapter 6: Resampling Methods"
part: "Part 2: Permutation Tests and Jackknife"
coverImage: 13
author: "Dereck Mezquita"
date: "`r Sys.Date()`"
tags: [statistics, resampling, permutation-tests, jackknife, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7, comment = "#>", warning = FALSE, collapse = FALSE, results = 'hold')
```

# Part 2: Permutation Tests and Jackknife

While the bootstrap estimates sampling distributions, **permutation tests** provide exact hypothesis tests under the null. The **jackknife** predates the bootstrap and offers bias estimation and influence analysis. This chapter completes our resampling toolkit with these complementary methods.

```{r packages, message=FALSE, warning=FALSE}
box::use(
    data.table[...],
    ggplot2
)

set.seed(42)
```

```{r load_data, message=FALSE}
# Load breast cancer dataset
breast_cancer <- fread("../../../data/bioinformatics/breast_cancer_wisconsin.csv")
breast_cancer[, diagnosis := factor(diagnosis, levels = c("B", "M"),
                                     labels = c("Benign", "Malignant"))]

cat("Dataset Summary\n")
cat("===============\n")
cat("  Total samples:", nrow(breast_cancer), "\n")
cat("  Benign:", sum(breast_cancer$diagnosis == "Benign"), "\n")
cat("  Malignant:", sum(breast_cancer$diagnosis == "Malignant"), "\n")
```

---

## Table of Contents

## 6.8 Permutation Tests

### 6.8.1 The Permutation Principle

**Prose and Intuition**

**Permutation tests** (also called randomisation tests or exact tests) test hypotheses by asking: "If the null hypothesis were true, how likely is the observed test statistic?"

Under the null (no difference between groups), group labels are exchangeable—we could shuffle them without changing the distribution. The permutation test:
1. Computes the observed test statistic
2. Generates the null distribution by permuting group labels
3. Calculates the p-value as the proportion of permutations at least as extreme as observed

**Key insight**: The permutation distribution is the **exact** null distribution under the hypothesis of exchangeability.

**Mathematical Framework**

For a two-sample test comparing groups A and B:

**Null hypothesis**: $H_0: F_A = F_B$ (same distribution)

Under $H_0$, any assignment of observations to groups is equally likely. With $n_A + n_B = n$ observations, there are $\binom{n}{n_A}$ possible assignments.

The **permutation p-value**:
$$p = \frac{\#\{T^* \geq T_{obs}\}}{\text{total permutations}}$$

```{r permutation_principle, fig.cap="Permutation test: generating the null distribution by shuffling labels"}
# Two-sample permutation test: compare mean_radius between diagnoses
benign <- breast_cancer[diagnosis == "Benign"]$mean_radius
malignant <- breast_cancer[diagnosis == "Malignant"]$mean_radius

# Observed test statistic
obs_diff <- mean(malignant) - mean(benign)

# Pool all data
pooled <- c(benign, malignant)
n_benign <- length(benign)
n_malignant <- length(malignant)
n_total <- n_benign + n_malignant

# Permutation test
n_perm <- 10000
perm_diffs <- numeric(n_perm)

for (p in 1:n_perm) {
    # Shuffle labels
    shuffled <- sample(pooled)
    perm_benign <- shuffled[1:n_benign]
    perm_malignant <- shuffled[(n_benign + 1):n_total]

    perm_diffs[p] <- mean(perm_malignant) - mean(perm_benign)
}

# P-value (two-sided)
p_value <- mean(abs(perm_diffs) >= abs(obs_diff))

cat("Permutation Test: Mean Radius by Diagnosis\n")
cat("==========================================\n")
cat("  Mean (Benign):", round(mean(benign), 3), "\n")
cat("  Mean (Malignant):", round(mean(malignant), 3), "\n")
cat("  Observed difference:", round(obs_diff, 3), "\n")
cat("  Permutation p-value:", format.pval(p_value, digits = 4), "\n")

# Compare to t-test
t_result <- t.test(malignant, benign)
cat("  T-test p-value:", format.pval(t_result$p.value, digits = 4), "\n")

# Visualise null distribution
perm_data <- data.table(difference = perm_diffs)

ggplot2$ggplot(perm_data, ggplot2$aes(x = difference)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..),
                            bins = 50, fill = "grey70", alpha = 0.8) +
    ggplot2$geom_density(colour = "#2166AC", linewidth = 1) +
    ggplot2$geom_vline(xintercept = obs_diff, colour = "#D95F02",
                        linewidth = 1.2, linetype = "solid") +
    ggplot2$geom_vline(xintercept = -obs_diff, colour = "#D95F02",
                        linewidth = 1.2, linetype = "dashed") +
    ggplot2$annotate("text", x = obs_diff + 0.3, y = 0.5,
                      label = paste("Observed =", round(obs_diff, 2)),
                      colour = "#D95F02", fontface = "bold", hjust = 0) +
    ggplot2$labs(
        title = "Permutation Null Distribution",
        subtitle = paste0("p-value = ", format.pval(p_value, digits = 4),
                         " (", n_perm, " permutations)"),
        x = "Difference in Means (Malignant - Benign)",
        y = "Density"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(plot.title = ggplot2$element_text(face = "bold"))
```

### 6.8.2 Permutation Test for Correlation

**Prose and Intuition**

To test whether two variables are associated, we can permute one variable while keeping the other fixed. Under the null of no association, the pairing is arbitrary.

```{r permutation_correlation, fig.cap="Permutation test for correlation coefficient"}
# Test correlation between mean_radius and mean_texture
x <- breast_cancer$mean_radius
y <- breast_cancer$mean_texture

# Observed correlation
obs_cor <- cor(x, y)

# Permutation test (permute y)
n_perm <- 10000
perm_cors <- numeric(n_perm)

for (p in 1:n_perm) {
    y_perm <- sample(y)
    perm_cors[p] <- cor(x, y_perm)
}

# P-value
p_value_cor <- mean(abs(perm_cors) >= abs(obs_cor))

cat("Permutation Test for Correlation:\n")
cat("==================================\n")
cat("  Observed r:", round(obs_cor, 4), "\n")
cat("  Permutation p-value:", format.pval(p_value_cor, digits = 4), "\n")

# Compare to parametric test
cor_test <- cor.test(x, y)
cat("  Parametric p-value:", format.pval(cor_test$p.value, digits = 4), "\n")

# Visualise
cor_perm_data <- data.table(correlation = perm_cors)

ggplot2$ggplot(cor_perm_data, ggplot2$aes(x = correlation)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..),
                            bins = 50, fill = "grey70", alpha = 0.8) +
    ggplot2$geom_density(colour = "#2166AC", linewidth = 1) +
    ggplot2$geom_vline(xintercept = obs_cor, colour = "#D95F02", linewidth = 1.2) +
    ggplot2$geom_vline(xintercept = -obs_cor, colour = "#D95F02",
                        linewidth = 1.2, linetype = "dashed") +
    ggplot2$labs(
        title = "Permutation Null Distribution for Correlation",
        subtitle = paste0("Observed r = ", round(obs_cor, 3),
                         ", p = ", format.pval(p_value_cor, digits = 4)),
        x = "Correlation",
        y = "Density"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(plot.title = ggplot2$element_text(face = "bold"))
```

### 6.8.3 Permutation Test for Regression

**Prose and Intuition**

For regression, we test whether covariates predict the response. Under the null, there's no relationship, so we can permute the response variable.

This tests the **global null** that all coefficients (except intercept) are zero.

```{r permutation_regression, fig.cap="Permutation test for regression F-statistic"}
# Regression: predict mean_radius from texture, perimeter, smoothness
reg_data <- breast_cancer[, .(mean_radius, mean_texture, mean_perimeter, mean_smoothness)]

# Original fit
fit <- lm(mean_radius ~ mean_texture + mean_perimeter + mean_smoothness, data = reg_data)

# Observed F-statistic
obs_F <- summary(fit)$fstatistic[1]

# Permutation test (permute response)
n_perm <- 5000
perm_F <- numeric(n_perm)

for (p in 1:n_perm) {
    reg_data_perm <- copy(reg_data)
    reg_data_perm[, mean_radius := sample(mean_radius)]

    fit_perm <- lm(mean_radius ~ mean_texture + mean_perimeter + mean_smoothness,
                   data = reg_data_perm)
    perm_F[p] <- summary(fit_perm)$fstatistic[1]
}

p_value_F <- mean(perm_F >= obs_F)

cat("Permutation Test for Regression F-statistic:\n")
cat("============================================\n")
cat("  Observed F:", round(obs_F, 2), "\n")
cat("  Permutation p-value:", format.pval(p_value_F, digits = 4), "\n")

# Compare to parametric
param_p <- pf(summary(fit)$fstatistic[1],
              summary(fit)$fstatistic[2],
              summary(fit)$fstatistic[3],
              lower.tail = FALSE)
cat("  Parametric p-value:", format.pval(param_p, digits = 4), "\n")

# Visualise
F_perm_data <- data.table(F_stat = perm_F)

ggplot2$ggplot(F_perm_data, ggplot2$aes(x = F_stat)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..),
                            bins = 50, fill = "grey70", alpha = 0.8) +
    ggplot2$geom_density(colour = "#2166AC", linewidth = 1) +
    ggplot2$geom_vline(xintercept = obs_F, colour = "#D95F02", linewidth = 1.2) +
    ggplot2$xlim(c(0, max(c(perm_F, obs_F)) * 1.1)) +
    ggplot2$labs(
        title = "Permutation Null Distribution for F-statistic",
        subtitle = paste0("Observed F = ", round(obs_F, 1),
                         ", p = ", format.pval(p_value_F, digits = 4)),
        x = "F-statistic",
        y = "Density"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(plot.title = ggplot2$element_text(face = "bold"))
```

### 6.8.4 Exact vs Monte Carlo Permutation Tests

**Prose and Intuition**

For small samples, we can enumerate **all** possible permutations for an **exact** test. For larger samples, we use **Monte Carlo** approximation.

**Exact permutation**:
- For $n_1 + n_2 = 10$ with $n_1 = 5$: only $\binom{10}{5} = 252$ permutations
- Enumerate all and compute exact p-value

**Monte Carlo**:
- Sample from the permutation distribution
- P-value is approximate with Monte Carlo error

```{r exact_permutation}
# Small sample example: exact permutation test
set.seed(123)
group_a <- rnorm(5, mean = 0, sd = 1)
group_b <- rnorm(5, mean = 1, sd = 1)

pooled_small <- c(group_a, group_b)
n_a <- length(group_a)
n_total_small <- length(pooled_small)

# Observed statistic
obs_diff_small <- mean(group_b) - mean(group_a)

# Generate all permutations
library(gtools)  # for combinations
all_combs <- combinations(n_total_small, n_a)

n_exact <- nrow(all_combs)
exact_diffs <- numeric(n_exact)

for (i in 1:n_exact) {
    idx_a <- all_combs[i, ]
    exact_diffs[i] <- mean(pooled_small[-idx_a]) - mean(pooled_small[idx_a])
}

# Exact p-value
p_exact <- mean(abs(exact_diffs) >= abs(obs_diff_small))

cat("Exact Permutation Test (Small Sample):\n")
cat("======================================\n")
cat("  Sample sizes:", n_a, "+", n_total_small - n_a, "\n")
cat("  Total permutations:", n_exact, "\n")
cat("  Observed difference:", round(obs_diff_small, 3), "\n")
cat("  Exact p-value:", round(p_exact, 4), "\n")

# Compare to Monte Carlo
n_mc <- 10000
mc_diffs <- replicate(n_mc, {
    idx <- sample(1:n_total_small, n_a)
    mean(pooled_small[-idx]) - mean(pooled_small[idx])
})
p_mc <- mean(abs(mc_diffs) >= abs(obs_diff_small))
cat("  Monte Carlo p-value (B=10000):", round(p_mc, 4), "\n")
```

---

## 6.9 The Jackknife

### 6.9.1 Leave-One-Out Resampling

**Prose and Intuition**

The **jackknife** (Quenouille 1949, Tukey 1958) predates the bootstrap. It systematically leaves out one observation at a time:

For sample $X_1, \ldots, X_n$, compute:
- $\hat{\theta}_{(-i)}$ = statistic with observation $i$ removed

The **jackknife pseudo-values** are:
$$\tilde{\theta}_i = n\hat{\theta} - (n-1)\hat{\theta}_{(-i)}$$

**Jackknife estimate of variance**:
$$\widehat{Var}_{jack}(\hat{\theta}) = \frac{n-1}{n}\sum_{i=1}^{n}(\hat{\theta}_{(-i)} - \bar{\theta}_{(\cdot)})^2$$

where $\bar{\theta}_{(\cdot)} = \frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(-i)}$.

**Mathematical Framework**

The jackknife estimates the **bias**:
$$\widehat{Bias}_{jack} = (n-1)(\bar{\theta}_{(\cdot)} - \hat{\theta})$$

**Bias-corrected estimate**:
$$\hat{\theta}_{jack} = \hat{\theta} - \widehat{Bias}_{jack} = n\hat{\theta} - (n-1)\bar{\theta}_{(\cdot)}$$

```{r jackknife_basics, fig.cap="Jackknife leave-one-out estimates"}
# Jackknife for variance estimate
x <- breast_cancer$mean_radius[1:100]  # Use subset for demonstration
n <- length(x)

# Original estimate
theta_hat <- var(x)

# Leave-one-out estimates
theta_loo <- numeric(n)
for (i in 1:n) {
    theta_loo[i] <- var(x[-i])
}

# Jackknife variance estimate
theta_bar <- mean(theta_loo)
var_jack <- ((n - 1) / n) * sum((theta_loo - theta_bar)^2)
se_jack <- sqrt(var_jack)

# Jackknife bias estimate
bias_jack <- (n - 1) * (theta_bar - theta_hat)

# Bias-corrected estimate
theta_jack <- theta_hat - bias_jack

cat("Jackknife Analysis of Variance Estimator:\n")
cat("=========================================\n")
cat("  Original estimate:", round(theta_hat, 4), "\n")
cat("  Jackknife mean:", round(theta_bar, 4), "\n")
cat("  Jackknife SE:", round(se_jack, 4), "\n")
cat("  Jackknife bias:", round(bias_jack, 4), "\n")
cat("  Bias-corrected estimate:", round(theta_jack, 4), "\n")

# Visualise leave-one-out estimates
jack_data <- data.table(
    index = 1:n,
    estimate = theta_loo
)

ggplot2$ggplot(jack_data, ggplot2$aes(x = index, y = estimate)) +
    ggplot2$geom_point(colour = "#2166AC", alpha = 0.6) +
    ggplot2$geom_hline(yintercept = theta_hat, linetype = "solid",
                        colour = "red", linewidth = 1) +
    ggplot2$geom_hline(yintercept = theta_bar, linetype = "dashed",
                        colour = "#D95F02", linewidth = 1) +
    ggplot2$annotate("text", x = n * 0.8, y = theta_hat + 0.5,
                      label = "Original estimate", colour = "red") +
    ggplot2$annotate("text", x = n * 0.8, y = theta_bar - 0.5,
                      label = "Jackknife mean", colour = "#D95F02") +
    ggplot2$labs(
        title = "Jackknife Leave-One-Out Estimates",
        subtitle = "Each point is the estimate with one observation removed",
        x = "Observation Removed",
        y = "Variance Estimate"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(plot.title = ggplot2$element_text(face = "bold"))
```

### 6.9.2 Jackknife Pseudo-Values and Influence

**Prose and Intuition**

The **pseudo-values** measure each observation's contribution to the estimate:

$$\tilde{\theta}_i = n\hat{\theta} - (n-1)\hat{\theta}_{(-i)}$$

If $\tilde{\theta}_i$ differs greatly from $\hat{\theta}$, observation $i$ is **influential**.

The pseudo-values are approximately independent with mean $\theta$ and variance $n \cdot Var(\hat{\theta})$. This allows using standard methods (t-tests, regression) on pseudo-values.

```{r jackknife_influence, fig.cap="Jackknife pseudo-values identify influential observations"}
# Compute pseudo-values
pseudo_values <- n * theta_hat - (n - 1) * theta_loo

# Influence: deviation from mean
influence <- pseudo_values - mean(pseudo_values)

# Identify influential observations
influential_idx <- which(abs(influence) > 2 * sd(influence))

cat("Jackknife Influence Analysis:\n")
cat("============================\n")
cat("  Mean pseudo-value:", round(mean(pseudo_values), 4), "\n")
cat("  SD of pseudo-values:", round(sd(pseudo_values), 4), "\n")
cat("  Influential observations (|z| > 2):", length(influential_idx), "\n")

# Visualise influence
influence_data <- data.table(
    index = 1:n,
    pseudo_value = pseudo_values,
    influence = influence,
    influential = (1:n) %in% influential_idx
)

ggplot2$ggplot(influence_data, ggplot2$aes(x = index, y = influence,
                                            colour = influential)) +
    ggplot2$geom_point(size = 2) +
    ggplot2$geom_hline(yintercept = 0, linetype = "solid", colour = "grey40") +
    ggplot2$geom_hline(yintercept = c(-2, 2) * sd(influence),
                        linetype = "dashed", colour = "red") +
    ggplot2$scale_colour_manual(values = c("FALSE" = "#2166AC", "TRUE" = "#D95F02"),
                                 labels = c("Normal", "Influential"),
                                 name = "") +
    ggplot2$labs(
        title = "Jackknife Influence Analysis",
        subtitle = "Observations outside dashed lines are influential",
        x = "Observation Index",
        y = "Influence"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "top"
    )
```

### 6.9.3 Delete-d Jackknife

**Prose and Intuition**

The standard jackknife deletes one observation. The **delete-d jackknife** generalises this by deleting $d$ observations at a time.

For delete-d jackknife:
- Number of subsets: $\binom{n}{d}$
- Each subset has $n - d$ observations

This is useful when:
- Single-observation influence is too small to detect
- Clustered data (delete entire clusters)

```{r delete_d_jackknife}
# Delete-d jackknife for d=5
x_small <- x[1:30]  # Smaller sample for computational feasibility
n_small <- length(x_small)
d <- 5

# Number of subsets
n_subsets <- choose(n_small, d)
cat("Delete-5 Jackknife:\n")
cat("===================\n")
cat("  Sample size:", n_small, "\n")
cat("  d:", d, "\n")
cat("  Number of subsets:", n_subsets, "\n")

# Sample subsets (too many to enumerate all)
n_sample <- min(500, n_subsets)
theta_delete_d <- numeric(n_sample)

for (s in 1:n_sample) {
    delete_idx <- sample(1:n_small, d)
    theta_delete_d[s] <- var(x_small[-delete_idx])
}

# Variance estimate
# Formula for delete-d: Var = (n-d)/(d*n) * (n choose d)^(-1) * sum((theta_i - theta_bar)^2)
# Simplified estimate
var_delete_d <- var(theta_delete_d) * (n_small - d) / d

cat("  Delete-1 SE:", round(se_jack, 4), "\n")
cat("  Delete-5 SE (estimated):", round(sqrt(var_delete_d), 4), "\n")
```

---

## 6.10 Bootstrap vs Jackknife vs Permutation

### 6.10.1 Comparison of Methods

**Prose and Intuition**

| Method | Purpose | Resampling | Assumption |
|--------|---------|------------|------------|
| **Bootstrap** | SE, CI, bias | With replacement | i.i.d. sample |
| **Jackknife** | SE, bias, influence | Leave-one-out | Smooth statistics |
| **Permutation** | Hypothesis testing | Shuffle labels | Exchangeability under $H_0$ |

**When to use each**:
- **Bootstrap**: General-purpose SE and CI estimation
- **Jackknife**: Bias estimation, influence diagnostics, BCa acceleration
- **Permutation**: Exact tests, small samples, non-standard test statistics

```{r method_comparison, fig.cap="SE estimates from bootstrap vs jackknife"}
# Compare bootstrap and jackknife SEs for various statistics
compare_methods <- function(x, statistic, B = 2000) {
    n <- length(x)

    # Bootstrap SE
    boot_vals <- replicate(B, statistic(sample(x, n, replace = TRUE)))
    boot_se <- sd(boot_vals)

    # Jackknife SE
    jack_vals <- sapply(1:n, function(i) statistic(x[-i]))
    jack_se <- sqrt(((n - 1) / n) * sum((jack_vals - mean(jack_vals))^2))

    c(bootstrap = boot_se, jackknife = jack_se)
}

# Apply to various statistics
x_compare <- breast_cancer$mean_radius

stats_to_compare <- list(
    Mean = mean,
    Median = median,
    `Trimmed Mean` = function(x) mean(x, trim = 0.1),
    SD = sd,
    MAD = mad,
    `25th Percentile` = function(x) quantile(x, 0.25)
)

se_comparison <- rbindlist(lapply(names(stats_to_compare), function(name) {
    ses <- compare_methods(x_compare, stats_to_compare[[name]])
    data.table(
        Statistic = name,
        Bootstrap_SE = ses["bootstrap"],
        Jackknife_SE = ses["jackknife"]
    )
}))

cat("Bootstrap vs Jackknife Standard Errors:\n")
cat("=======================================\n")
print(se_comparison[, .(Statistic,
                         Bootstrap_SE = round(Bootstrap_SE, 4),
                         Jackknife_SE = round(Jackknife_SE, 4))])

# Visualise
se_long <- melt(se_comparison, id.vars = "Statistic",
                variable.name = "Method", value.name = "SE")

ggplot2$ggplot(se_long, ggplot2$aes(x = Statistic, y = SE, fill = Method)) +
    ggplot2$geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    ggplot2$scale_fill_manual(values = c("Bootstrap_SE" = "#2166AC",
                                          "Jackknife_SE" = "#D95F02"),
                               labels = c("Bootstrap", "Jackknife")) +
    ggplot2$labs(
        title = "Standard Error Estimates: Bootstrap vs Jackknife",
        x = "",
        y = "Standard Error",
        fill = ""
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "top",
        axis.text.x = ggplot2$element_text(angle = 30, hjust = 1)
    )
```

### 6.10.2 When Jackknife Fails

**Prose and Intuition**

The jackknife fails for **non-smooth** statistics—those that can change discontinuously when a single observation is removed.

**Examples**:
- Median (when sample size is even, removing one observation shifts the median)
- Quantiles
- Range, min, max

For these statistics, the bootstrap is preferred.

```{r jackknife_failure, fig.cap="Jackknife fails for non-smooth statistics like the median"}
# Demonstrate jackknife failure for median
n_demo <- 20
x_demo <- rnorm(n_demo)

# True SE (from simulation)
true_se_median <- sd(replicate(10000, median(sample(x_demo, n_demo, replace = TRUE))))

# Jackknife SE for median
jack_medians <- sapply(1:n_demo, function(i) median(x_demo[-i]))
jack_se_median <- sqrt(((n_demo - 1) / n_demo) * sum((jack_medians - mean(jack_medians))^2))

# Bootstrap SE
boot_se_median <- sd(replicate(2000, median(sample(x_demo, n_demo, replace = TRUE))))

cat("Jackknife for Median (Non-Smooth Statistic):\n")
cat("============================================\n")
cat("  True SE (simulation):", round(true_se_median, 4), "\n")
cat("  Bootstrap SE:", round(boot_se_median, 4), "\n")
cat("  Jackknife SE:", round(jack_se_median, 4), "\n\n")

# Compare for mean (smooth statistic)
true_se_mean <- sd(replicate(10000, mean(sample(x_demo, n_demo, replace = TRUE))))
boot_se_mean <- sd(replicate(2000, mean(sample(x_demo, n_demo, replace = TRUE))))
jack_means <- sapply(1:n_demo, function(i) mean(x_demo[-i]))
jack_se_mean <- sqrt(((n_demo - 1) / n_demo) * sum((jack_means - mean(jack_means))^2))

cat("Comparison with Mean (Smooth Statistic):\n")
cat("========================================\n")
cat("  True SE:", round(true_se_mean, 4), "\n")
cat("  Bootstrap SE:", round(boot_se_mean, 4), "\n")
cat("  Jackknife SE:", round(jack_se_mean, 4), "\n")
```

---

## 6.11 Advanced Applications

### 6.11.1 Permutation Tests for High-Dimensional Data

**Prose and Intuition**

In high-dimensional settings (e.g., genomics), permutation tests are valuable for controlling multiple testing while accounting for correlation structure.

```{r permutation_multiple_testing, fig.cap="Permutation-based FDR control in multiple testing"}
# Simulate high-dimensional data (mini gene expression)
n_genes <- 100
n_samples <- 20
n_group1 <- 10

# Expression matrix (rows = genes, cols = samples)
expression <- matrix(rnorm(n_genes * n_samples), nrow = n_genes)

# Add signal to first 10 genes (true positives)
n_true <- 10
effect_size <- 1.5
expression[1:n_true, 1:n_group1] <- expression[1:n_true, 1:n_group1] + effect_size

groups <- c(rep(1, n_group1), rep(2, n_samples - n_group1))

# Observed t-statistics
calc_t <- function(x, g) {
    x1 <- x[g == 1]
    x2 <- x[g == 2]
    (mean(x1) - mean(x2)) / sqrt(var(x1)/length(x1) + var(x2)/length(x2))
}

obs_t <- apply(expression, 1, calc_t, g = groups)

# Permutation null
n_perm <- 1000
perm_max_t <- numeric(n_perm)

for (p in 1:n_perm) {
    g_perm <- sample(groups)
    perm_t <- apply(expression, 1, calc_t, g = g_perm)
    perm_max_t[p] <- max(abs(perm_t))  # Maximum statistic for FWER control
}

# FWER-adjusted p-values
p_adj <- sapply(abs(obs_t), function(t) mean(perm_max_t >= t))

# Compare to raw p-values
raw_p <- 2 * (1 - pt(abs(obs_t), df = n_samples - 2))

results <- data.table(
    gene = 1:n_genes,
    true_positive = 1:n_genes <= n_true,
    t_stat = obs_t,
    raw_p = raw_p,
    perm_p = p_adj
)

cat("Permutation-Based Multiple Testing:\n")
cat("===================================\n")
cat("  Genes tested:", n_genes, "\n")
cat("  True positives:", n_true, "\n")
cat("  Significant (raw p < 0.05):", sum(raw_p < 0.05), "\n")
cat("  Significant (perm p < 0.05):", sum(p_adj < 0.05), "\n")

# Visualise
ggplot2$ggplot(results, ggplot2$aes(x = -log10(raw_p), y = -log10(perm_p),
                                     colour = true_positive)) +
    ggplot2$geom_point(size = 2, alpha = 0.7) +
    ggplot2$geom_abline(slope = 1, intercept = 0, linetype = "dashed", colour = "grey40") +
    ggplot2$geom_hline(yintercept = -log10(0.05), linetype = "dotted", colour = "red") +
    ggplot2$geom_vline(xintercept = -log10(0.05), linetype = "dotted", colour = "red") +
    ggplot2$scale_colour_manual(values = c("FALSE" = "#2166AC", "TRUE" = "#D95F02"),
                                 labels = c("Null", "True Positive"),
                                 name = "") +
    ggplot2$labs(
        title = "Raw vs Permutation-Adjusted P-values",
        subtitle = "Permutation controls for multiple testing",
        x = "-log10(Raw P-value)",
        y = "-log10(Permutation P-value)"
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "top"
    )
```

### 6.11.2 Block Bootstrap for Dependent Data

**Prose and Intuition**

Standard bootstrap assumes i.i.d. data. For **time series** or **clustered data**, we need to preserve the dependence structure.

**Block bootstrap**: Resample blocks of consecutive observations rather than individual observations.

**Types**:
- **Non-overlapping blocks**: Divide data into fixed blocks
- **Moving blocks**: Overlapping blocks of fixed length
- **Circular blocks**: Wrap-around to handle edges

```{r block_bootstrap, fig.cap="Block bootstrap for time series data"}
# Simulate autocorrelated time series
n_ts <- 200
ar_coef <- 0.7
ts_data <- numeric(n_ts)
ts_data[1] <- rnorm(1)

for (t in 2:n_ts) {
    ts_data[t] <- ar_coef * ts_data[t-1] + rnorm(1)
}

# Estimate mean with standard bootstrap (incorrect for dependent data)
B <- 2000
boot_means_iid <- replicate(B, mean(sample(ts_data, n_ts, replace = TRUE)))

# Block bootstrap
block_size <- 10
n_blocks <- ceiling(n_ts / block_size)

boot_means_block <- numeric(B)
for (b in 1:B) {
    # Sample block starting positions
    block_starts <- sample(1:(n_ts - block_size + 1), n_blocks, replace = TRUE)
    # Extract blocks
    boot_sample <- unlist(lapply(block_starts, function(s) ts_data[s:(s + block_size - 1)]))
    boot_sample <- boot_sample[1:n_ts]  # Trim to original length
    boot_means_block[b] <- mean(boot_sample)
}

# True SE (from AR(1) theory)
true_se <- sqrt(var(ts_data) / n_ts * (1 + ar_coef) / (1 - ar_coef))

cat("Bootstrap for Autocorrelated Data:\n")
cat("==================================\n")
cat("  AR(1) coefficient:", ar_coef, "\n")
cat("  True SE (theory):", round(true_se, 4), "\n")
cat("  i.i.d. Bootstrap SE:", round(sd(boot_means_iid), 4), "\n")
cat("  Block Bootstrap SE:", round(sd(boot_means_block), 4), "\n")

# Visualise
boot_comparison <- rbind(
    data.table(method = "i.i.d. Bootstrap", mean = boot_means_iid),
    data.table(method = "Block Bootstrap", mean = boot_means_block)
)

ggplot2$ggplot(boot_comparison, ggplot2$aes(x = mean, fill = method)) +
    ggplot2$geom_density(alpha = 0.5) +
    ggplot2$geom_vline(xintercept = mean(ts_data), linetype = "dashed", colour = "red") +
    ggplot2$scale_fill_manual(values = c("i.i.d. Bootstrap" = "#E41A1C",
                                          "Block Bootstrap" = "#4DAF4A")) +
    ggplot2$labs(
        title = "i.i.d. vs Block Bootstrap for Time Series",
        subtitle = paste0("Block size = ", block_size, "; AR(1) coefficient = ", ar_coef),
        x = "Bootstrap Mean",
        y = "Density",
        fill = ""
    ) +
    ggplot2$theme_minimal(base_size = 14) +
    ggplot2$theme(
        plot.title = ggplot2$element_text(face = "bold"),
        legend.position = "top"
    )
```

---

## 6.12 Summary and Key Concepts

### Key Takeaways

1. **Permutation Tests**: Test hypotheses by shuffling labels to generate the exact null distribution. Valid without distributional assumptions.

2. **Exchangeability**: Permutation tests assume observations are exchangeable under the null—group labels are arbitrary.

3. **Jackknife**: Leave-one-out resampling for bias estimation, variance estimation, and influence diagnostics.

4. **Pseudo-values**: Jackknife pseudo-values measure each observation's contribution; can be used for regression-based inference.

5. **Non-smooth Statistics**: Jackknife fails for statistics like the median; bootstrap is preferred.

6. **Method Selection**:
   - Hypothesis testing → Permutation
   - Standard errors/CIs → Bootstrap
   - Bias estimation/influence → Jackknife

7. **Dependent Data**: Use block bootstrap to preserve correlation structure.

### Best Practices

1. **Permutation tests**: Use for exact p-values, especially with non-standard test statistics
2. **Report number of permutations**: Monte Carlo error decreases with more permutations
3. **Check jackknife assumptions**: Avoid for quantiles and other non-smooth statistics
4. **Block bootstrap**: Choose block size to capture the dependence structure
5. **Combine methods**: Use jackknife for BCa acceleration in bootstrap CIs

### Communicating to Stakeholders

**For a clinical audience**: "We tested whether mean tumour size differs between treatment groups using a permutation test. By randomly shuffling the group labels 10,000 times, we determined how unusual our observed difference would be if there were truly no effect. The permutation p-value of 0.003 indicates our result is highly unlikely under the null hypothesis."

**For a methods paper**: "The permutation null distribution was constructed from 10,000 random permutations of group labels. For the block bootstrap, we used non-overlapping blocks of size 10, chosen based on the estimated autocorrelation decay. Jackknife pseudo-values were computed to identify influential observations, with five observations exceeding the 2-SD threshold."

**Key vocabulary**:
- Permutation test, exact test, exchangeability
- Null distribution, Monte Carlo p-value
- Jackknife, leave-one-out, pseudo-values
- Influence, delete-d jackknife
- Block bootstrap, moving blocks
