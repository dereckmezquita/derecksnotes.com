---
title: "Statistics with R II: Intermediate"
chapter: "Chapter 3: Generalised Linear Models Framework"
part: "Part 2: Maximum Likelihood Estimation and Inference"
coverImage: 13
author: "Dereck Mezquita"
date: "2026-01-19"
tags: [statistics, mathematics, GLM, MLE, inference, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---



# Part 2: Maximum Likelihood Estimation and Inference

Unlike ordinary least squares, GLMs cannot be solved with a simple closed-form formula. Instead, we use **maximum likelihood estimation (MLE)**, finding the parameter values that make the observed data most probable. This part develops the theory of MLE for GLMs, the iteratively reweighted least squares algorithm, and inference methods including Wald tests, likelihood ratio tests, and deviance.


``` r
box::use(
    data.table[...],
    ggplot2
)
```


``` r
# Load datasets
nhanes <- fread("../../../data/primary/nhanes.csv")
doctor_visits <- fread("../../../data/count/nmes1988_doctor_visits.csv")

# Prepare diabetes data
diabetes_data <- nhanes[!is.na(Diabetes) & !is.na(Age) & !is.na(BMI) & Age >= 18,
                        .(Diabetic = as.integer(Diabetes == "Yes"),
                          Age = Age, BMI = BMI)]
diabetes_data <- diabetes_data[complete.cases(diabetes_data)]

cat("Datasets prepared:\n")
#> Datasets prepared:
cat("  Diabetes data: n =", nrow(diabetes_data), "\n")
#>   Diabetes data: n = 7414
```

---

## 2.1 Maximum Likelihood for GLMs

### 2.1.1 The Likelihood Function

**Mathematical Derivation**

For independent observations $Y_1, \ldots, Y_n$ from an exponential family, the log-likelihood is:
$$\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \log f(Y_i; \theta_i, \phi) = \sum_{i=1}^n \frac{Y_i\theta_i - b(\theta_i)}{a(\phi)} + c(Y_i, \phi)$$

The canonical parameter $\theta_i$ is linked to the linear predictor through:
$$\theta_i = h(\eta_i) = h(\mathbf{x}_i'\boldsymbol{\beta})$$

where $h = (b')^{-1}$ for the canonical link.

**Score Equations**

Taking derivatives and setting to zero:
$$\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{(Y_i - \mu_i) X_{ij}}{a(\phi) V(\mu_i) g'(\mu_i)} = 0$$

These are the **score equations**. Unlike linear regression, they're nonlinear in $\boldsymbol{\beta}$ and require iterative solution.

### 2.1.2 Example: Logistic Regression Log-Likelihood

**Mathematical Derivation**

For logistic regression with $Y_i \sim \text{Bernoulli}(p_i)$:
$$p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}$$

where $\eta_i = \mathbf{x}_i'\boldsymbol{\beta}$.

The log-likelihood is:
$$\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[Y_i \log(p_i) + (1 - Y_i)\log(1 - p_i)\right]$$

Substituting $p_i$:
$$\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[Y_i \eta_i - \log(1 + e^{\eta_i})\right]$$


``` r
# Manual log-likelihood for simple logistic regression
log_likelihood <- function(beta, X, Y) {
    eta <- X %*% beta
    p <- 1 / (1 + exp(-eta))
    # Avoid log(0) issues
    p <- pmax(pmin(p, 1 - 1e-10), 1e-10)
    sum(Y * log(p) + (1 - Y) * log(1 - p))
}

# Prepare design matrix for simple model
X <- cbind(1, diabetes_data$Age)
Y <- diabetes_data$Diabetic

# Grid of parameter values
beta0_seq <- seq(-6, -2, length.out = 50)
beta1_seq <- seq(0.02, 0.08, length.out = 50)

ll_grid <- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)
ll_grid$loglik <- sapply(1:nrow(ll_grid), function(i) {
    log_likelihood(c(ll_grid$beta0[i], ll_grid$beta1[i]), X, Y)
})

# Find MLE from glm
model_logistic <- glm(Diabetic ~ Age, data = diabetes_data, family = binomial)
mle_beta <- coef(model_logistic)

cat("Log-Likelihood Analysis:\n")
#> Log-Likelihood Analysis:
cat("========================\n\n")
#> ========================
cat("MLE from glm():\n")
#> MLE from glm():
cat("  β₀ =", round(mle_beta[1], 4), "\n")
#>   β₀ = -5.0588
cat("  β₁ =", round(mle_beta[2], 5), "\n")
#>   β₁ = 0.05419
cat("  Log-likelihood at MLE:", round(logLik(model_logistic), 2), "\n")
#>   Log-likelihood at MLE: -2123.47

# Visualise
ll_dt <- as.data.table(ll_grid)

ggplot2$ggplot(ll_dt, ggplot2$aes(x = beta0, y = beta1, z = loglik)) +
    ggplot2$geom_contour_filled(bins = 20) +
    ggplot2$geom_point(x = mle_beta[1], y = mle_beta[2],
                       colour = "white", size = 4, shape = 4, stroke = 2) +
    ggplot2$scale_fill_viridis_d(option = "plasma") +
    ggplot2$labs(
        title = "Log-Likelihood Surface for Logistic Regression",
        subtitle = "White X marks the MLE",
        x = expression(beta[0] ~ "(Intercept)"),
        y = expression(beta[1] ~ "(Age)"),
        fill = "Log-Likelihood"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/logistic_likelihood-1.png" alt="Log-likelihood surface for logistic regression">
	Log-likelihood surface for logistic regression
</Figure>

---

## 2.2 Iteratively Reweighted Least Squares (IRLS)

### 2.2.1 The Algorithm

**Mathematical Derivation**

The score equations can be rewritten as a **weighted least squares** problem. At iteration $t$, we solve:
$$(\mathbf{X}'\mathbf{W}^{(t)}\mathbf{X})\boldsymbol{\beta}^{(t+1)} = \mathbf{X}'\mathbf{W}^{(t)}\mathbf{z}^{(t)}$$

where:
- $\mathbf{W}^{(t)} = \text{diag}(w_1^{(t)}, \ldots, w_n^{(t)})$ with $w_i = \frac{1}{V(\mu_i)[g'(\mu_i)]^2}$
- $\mathbf{z}^{(t)}$ is the **working response**: $z_i = \eta_i + (Y_i - \mu_i)g'(\mu_i)$

This is why the algorithm is called **Iteratively Reweighted Least Squares** — at each step, we solve a weighted OLS problem with updated weights.

**IRLS Algorithm:**
1. Initialize $\boldsymbol{\beta}^{(0)}$ (e.g., from OLS)
2. Compute $\eta^{(t)} = \mathbf{X}\boldsymbol{\beta}^{(t)}$
3. Compute $\mu^{(t)} = g^{-1}(\eta^{(t)})$
4. Compute weights $\mathbf{W}^{(t)}$ and working response $\mathbf{z}^{(t)}$
5. Solve weighted OLS: $\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}'\mathbf{W}^{(t)}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}^{(t)}\mathbf{z}^{(t)}$
6. Repeat until convergence


``` r
# Implement IRLS for logistic regression
irls_logistic <- function(X, Y, max_iter = 25, tol = 1e-8) {
    n <- nrow(X)
    p <- ncol(X)

    # Initialize with zeros
    beta <- rep(0, p)
    history <- list()

    for (iter in 1:max_iter) {
        # Linear predictor
        eta <- as.vector(X %*% beta)

        # Mean (inverse logit)
        mu <- 1 / (1 + exp(-eta))
        mu <- pmax(pmin(mu, 1 - 1e-10), 1e-10)  # Avoid 0 and 1

        # Weights: W = mu * (1 - mu) for logistic
        W <- mu * (1 - mu)
        W <- pmax(W, 1e-10)  # Avoid zero weights

        # Working response: z = eta + (y - mu) / (mu * (1 - mu))
        z <- eta + (Y - mu) / W

        # Weighted least squares
        W_diag <- diag(W)
        XtWX <- t(X) %*% W_diag %*% X
        XtWz <- t(X) %*% W_diag %*% z

        beta_new <- solve(XtWX) %*% XtWz

        # Store history
        history[[iter]] <- list(
            iteration = iter,
            beta = as.vector(beta_new),
            loglik = sum(Y * log(mu) + (1 - Y) * log(1 - mu))
        )

        # Check convergence
        if (max(abs(beta_new - beta)) < tol) {
            cat("Converged at iteration", iter, "\n")
            break
        }
        beta <- as.vector(beta_new)
    }

    list(beta = beta, history = history)
}

# Run IRLS
irls_result <- irls_logistic(X, Y)
#> Converged at iteration 7

cat("\nIRLS Results:\n")
#> 
#> IRLS Results:
cat("=============\n")
#> =============
cat("IRLS estimates:\n")
#> IRLS estimates:
cat("  β₀ =", round(irls_result$beta[1], 4), "\n")
#>   β₀ = -5.0588
cat("  β₁ =", round(irls_result$beta[2], 5), "\n")
#>   β₁ = 0.05419
cat("\nglm() estimates:\n")
#> 
#> glm() estimates:
cat("  β₀ =", round(mle_beta[1], 4), "\n")
#>   β₀ = -5.0588
cat("  β₁ =", round(mle_beta[2], 5), "\n")
#>   β₁ = 0.05419

# Extract convergence history
history_dt <- rbindlist(lapply(irls_result$history, function(h) {
    data.table(iteration = h$iteration, beta0 = h$beta[1], beta1 = h$beta[2], loglik = h$loglik)
}))

# Plot convergence
ggplot2$ggplot(history_dt, ggplot2$aes(x = iteration, y = loglik)) +
    ggplot2$geom_line(colour = "#0072B2", size = 1) +
    ggplot2$geom_point(colour = "#0072B2", size = 3) +
    ggplot2$labs(
        title = "IRLS Convergence",
        subtitle = "Log-likelihood increases at each iteration until convergence",
        x = "Iteration",
        y = "Log-Likelihood"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/irls_implementation-1.png" alt="IRLS convergence for logistic regression">
	IRLS convergence for logistic regression
</Figure>

---

## 2.3 Inference in GLMs

### 2.3.1 Asymptotic Distribution of MLEs

**Mathematical Derivation**

Under regularity conditions, the MLE $\hat{\boldsymbol{\beta}}$ is asymptotically normal:
$$\hat{\boldsymbol{\beta}} \stackrel{d}{\to} N(\boldsymbol{\beta}, \mathbf{I}(\boldsymbol{\beta})^{-1})$$

where $\mathbf{I}(\boldsymbol{\beta})$ is the **Fisher information matrix**:
$$\mathbf{I}(\boldsymbol{\beta}) = -E\left[\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'}\right] = \mathbf{X}'\mathbf{W}\mathbf{X} / a(\phi)$$

The standard errors are:
$$SE(\hat{\beta}_j) = \sqrt{[\mathbf{I}(\hat{\boldsymbol{\beta}})^{-1}]_{jj}}$$

### 2.3.2 Wald Tests and Confidence Intervals

**Mathematical Definition**

The **Wald test** for $H_0: \beta_j = 0$ uses:
$$z = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)} \stackrel{H_0}{\sim} N(0, 1)$$

A $(1-\alpha)$% **Wald confidence interval** is:
$$\hat{\beta}_j \pm z_{\alpha/2} \cdot SE(\hat{\beta}_j)$$

For odds ratios in logistic regression:
$$\exp(\hat{\beta}_j \pm z_{\alpha/2} \cdot SE(\hat{\beta}_j))$$


``` r
# Full model with Age and BMI
model_full <- glm(Diabetic ~ Age + BMI, data = diabetes_data, family = binomial)

# Extract Wald-based inference
coef_table <- summary(model_full)$coefficients
ci_wald <- confint.default(model_full)  # Wald-based CI

cat("Wald-Based Inference:\n")
#> Wald-Based Inference:
cat("=====================\n\n")
#> =====================
cat("Coefficients and Wald Tests:\n")
#> Coefficients and Wald Tests:
print(coef_table)
#>                Estimate  Std. Error   z value      Pr(>|z|)
#> (Intercept) -8.27481528 0.270622665 -30.57695 2.479240e-205
#> Age          0.05942748 0.002741811  21.67454 3.568051e-104
#> BMI          0.09609988 0.005727537  16.77857  3.501611e-63

cat("\n95% Wald Confidence Intervals:\n")
#> 
#> 95% Wald Confidence Intervals:
print(ci_wald)
#>                   2.5 %      97.5 %
#> (Intercept) -8.80522596 -7.74440461
#> Age          0.05405363  0.06480134
#> BMI          0.08487411  0.10732564

# Odds ratios with CI
or_table <- data.table(
    Variable = rownames(ci_wald),
    Odds_Ratio = exp(coef(model_full)),
    OR_Lower = exp(ci_wald[, 1]),
    OR_Upper = exp(ci_wald[, 2])
)

cat("\nOdds Ratios (95% CI):\n")
#> 
#> Odds Ratios (95% CI):
print(or_table[-1])  # Exclude intercept
#>    Variable Odds_Ratio OR_Lower OR_Upper
#>      <char>      <num>    <num>    <num>
#> 1:      Age   1.061229 1.055541 1.066947
#> 2:      BMI   1.100869 1.088580 1.113297
```

### 2.3.3 Likelihood Ratio Test

**Mathematical Definition**

The **likelihood ratio test (LRT)** compares nested models:
$$\Lambda = -2[\ell(\hat{\boldsymbol{\beta}}_0) - \ell(\hat{\boldsymbol{\beta}})]$$

Under $H_0$, $\Lambda \stackrel{d}{\to} \chi^2_{df}$, where $df$ is the difference in number of parameters.

The LRT is generally more reliable than the Wald test, especially for small samples or when parameters are near boundary values.


``` r
# Compare models
model_age <- glm(Diabetic ~ Age, data = diabetes_data, family = binomial)
model_full <- glm(Diabetic ~ Age + BMI, data = diabetes_data, family = binomial)

# Likelihood ratio test
lrt <- anova(model_age, model_full, test = "LRT")

cat("Likelihood Ratio Test:\n")
#> Likelihood Ratio Test:
cat("======================\n\n")
#> ======================
cat("Model 1: Diabetic ~ Age\n")
#> Model 1: Diabetic ~ Age
cat("Model 2: Diabetic ~ Age + BMI\n\n")
#> Model 2: Diabetic ~ Age + BMI
print(lrt)
#> Analysis of Deviance Table
#> 
#> Model 1: Diabetic ~ Age
#> Model 2: Diabetic ~ Age + BMI
#>   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
#> 1      7412     4246.9                          
#> 2      7411     3960.7  1   286.19 < 2.2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# Manual calculation
ll_reduced <- logLik(model_age)
ll_full <- logLik(model_full)
lr_stat <- -2 * (as.numeric(ll_reduced) - as.numeric(ll_full))
df <- 1
p_value <- pchisq(lr_stat, df, lower.tail = FALSE)

cat("\nManual Calculation:\n")
#> 
#> Manual Calculation:
cat("  Log-likelihood (reduced):", round(as.numeric(ll_reduced), 2), "\n")
#>   Log-likelihood (reduced): -2123.47
cat("  Log-likelihood (full):", round(as.numeric(ll_full), 2), "\n")
#>   Log-likelihood (full): -1980.37
cat("  LR statistic:", round(lr_stat, 4), "\n")
#>   LR statistic: 286.1879
cat("  df:", df, "\n")
#>   df: 1
cat("  p-value:", format.pval(p_value), "\n")
#>   p-value: < 2.22e-16
```

### 2.3.4 Profile Likelihood Confidence Intervals

**Prose and Intuition**

**Profile likelihood intervals** are more accurate than Wald intervals, especially when the log-likelihood is not quadratic (asymmetric or curved).

The profile likelihood for $\beta_j$ is formed by maximising over all other parameters:
$$\ell_P(\beta_j) = \max_{\beta_{-j}} \ell(\beta_j, \boldsymbol{\beta}_{-j})$$

The interval includes all $\beta_j$ where:
$$\ell_P(\beta_j) \geq \ell(\hat{\boldsymbol{\beta}}) - \frac{\chi^2_{1,\alpha}}{2}$$


``` r
# Profile likelihood confidence intervals
ci_profile <- confint(model_full)  # Profile-based CI
#> Waiting for profiling to be done...

cat("Comparison: Wald vs Profile CI:\n")
#> Comparison: Wald vs Profile CI:
cat("================================\n\n")
#> ================================

comparison <- data.table(
    Variable = rownames(ci_wald),
    Wald_Lower = ci_wald[, 1],
    Wald_Upper = ci_wald[, 2],
    Profile_Lower = ci_profile[, 1],
    Profile_Upper = ci_profile[, 2]
)
print(comparison)
#>       Variable  Wald_Lower  Wald_Upper Profile_Lower Profile_Upper
#>         <char>       <num>       <num>         <num>         <num>
#> 1: (Intercept) -8.80522596 -7.74440461   -8.81351012   -7.75239006
#> 2:         Age  0.05405363  0.06480134    0.05410979    0.06486058
#> 3:         BMI  0.08487411  0.10732564    0.08493032    0.10739146

cat("\nNote: Profile intervals are often slightly asymmetric,\n")
#> 
#> Note: Profile intervals are often slightly asymmetric,
cat("reflecting the true shape of the likelihood.\n")
#> reflecting the true shape of the likelihood.
```

---

## 2.4 Model Fit and Deviance

### 2.4.1 Deviance

**Mathematical Definition**

The **deviance** measures the discrepancy between the fitted model and the saturated model (which has one parameter per observation):
$$D = 2[\ell(\text{saturated}) - \ell(\text{fitted})]$$

For the saturated model, $\hat{\mu}_i = Y_i$ exactly.

**Deviance as a goodness-of-fit measure:**
- Small deviance → good fit
- For Poisson and binomial with large counts: $D \stackrel{\cdot}{\sim} \chi^2_{n-p-1}$ under the correct model

**Deviance residuals:**
$$d_i = \text{sign}(Y_i - \hat{\mu}_i) \sqrt{d_i^2}$$

where $d_i^2$ is the contribution of observation $i$ to the deviance.


``` r
# Deviance components
cat("Deviance Analysis:\n")
#> Deviance Analysis:
cat("==================\n\n")
#> ==================

cat("Model: Diabetic ~ Age + BMI\n")
#> Model: Diabetic ~ Age + BMI
cat("Null deviance:", round(model_full$null.deviance, 2), "on", model_full$df.null, "df\n")
#> Null deviance: 4778.89 on 7413 df
cat("Residual deviance:", round(model_full$deviance, 2), "on", model_full$df.residual, "df\n")
#> Residual deviance: 3960.75 on 7411 df
cat("\nDeviance explained:", round((1 - model_full$deviance / model_full$null.deviance) * 100, 2), "%\n")
#> 
#> Deviance explained: 17.12 %

# Deviance residuals
dev_resid <- residuals(model_full, type = "deviance")

cat("\nDeviance Residuals Summary:\n")
#> 
#> Deviance Residuals Summary:
print(summary(dev_resid))
#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#> -1.9440 -0.4693 -0.2924 -0.1583 -0.1759  3.0272

# Plot deviance residuals
diabetes_data[, dev_residual := dev_resid]
diabetes_data[, fitted_prob := fitted(model_full)]

ggplot2$ggplot(diabetes_data, ggplot2$aes(x = fitted_prob, y = dev_residual)) +
    ggplot2$geom_point(alpha = 0.3) +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed") +
    ggplot2$geom_smooth(method = "loess", colour = "#D55E00", se = FALSE) +
    ggplot2$labs(
        title = "Deviance Residuals vs Fitted Probabilities",
        subtitle = "Check for patterns indicating model misfit",
        x = "Fitted Probability",
        y = "Deviance Residual"
    ) +
    ggplot2$theme_minimal()
#> `geom_smooth()` using formula = 'y ~ x'
```

<Figure src="/courses/statistics-2-intermediate/deviance-1.png" alt="Deviance analysis for model fit">
	Deviance analysis for model fit
</Figure>

### 2.4.2 Pearson Residuals and Overdispersion

**Mathematical Definition**

**Pearson residuals** are:
$$r_i^P = \frac{Y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)}}$$

The **Pearson chi-square statistic** is:
$$X^2 = \sum_{i=1}^n (r_i^P)^2$$

**Overdispersion** occurs when $\text{Var}(Y) > V(\mu)$ expected by the model. We estimate the dispersion parameter:
$$\hat{\phi} = \frac{X^2}{n - p - 1}$$

If $\hat{\phi} > 1$, we have overdispersion.


``` r
# Check overdispersion in Poisson model
visits_data <- doctor_visits[!is.na(visits) & !is.na(age),
                             .(visits = visits, age = age)]
visits_data <- visits_data[complete.cases(visits_data)]

model_poisson <- glm(visits ~ age, data = visits_data, family = poisson)

# Pearson residuals
pearson_resid <- residuals(model_poisson, type = "pearson")
pearson_chisq <- sum(pearson_resid^2)
df_residual <- model_poisson$df.residual

# Dispersion estimate
dispersion_est <- pearson_chisq / df_residual

cat("Overdispersion Check (Poisson Model):\n")
#> Overdispersion Check (Poisson Model):
cat("=====================================\n\n")
#> =====================================
cat("Pearson χ²:", round(pearson_chisq, 2), "\n")
#> Pearson χ²: 34872.24
cat("df:", df_residual, "\n")
#> df: 4404
cat("Estimated dispersion (φ):", round(dispersion_est, 3), "\n")
#> Estimated dispersion (φ): 7.918
cat("\nInterpretation:\n")
#> 
#> Interpretation:
if (dispersion_est > 1.5) {
    cat("  φ >> 1 indicates substantial overdispersion.\n")
    cat("  Consider quasipoisson or negative binomial.\n")
} else if (dispersion_est > 1.1) {
    cat("  φ slightly > 1, mild overdispersion.\n")
} else {
    cat("  φ ≈ 1, no evidence of overdispersion.\n")
}
#>   φ >> 1 indicates substantial overdispersion.
#>   Consider quasipoisson or negative binomial.

# Fit quasipoisson for comparison
model_quasi <- glm(visits ~ age, data = visits_data, family = quasipoisson)

cat("\nStandard Errors Comparison:\n")
#> 
#> Standard Errors Comparison:
se_comparison <- data.table(
    Variable = names(coef(model_poisson)),
    SE_Poisson = summary(model_poisson)$coefficients[, "Std. Error"],
    SE_Quasipoisson = summary(model_quasi)$coefficients[, "Std. Error"]
)
se_comparison[, Ratio := round(SE_Quasipoisson / SE_Poisson, 2)]
print(se_comparison)
#>       Variable  SE_Poisson SE_Quasipoisson Ratio
#>         <char>       <num>           <num> <num>
#> 1: (Intercept) 0.073439315      0.20665654  2.81
#> 2:         age 0.009881429      0.02780611  2.81

cat("\nNote: Quasipoisson SEs are √φ times larger.\n")
#> 
#> Note: Quasipoisson SEs are √φ times larger.
```

---

## 2.5 Model Comparison

### 2.5.1 AIC and BIC for GLMs

**Mathematical Definition**

**AIC** (Akaike Information Criterion):
$$AIC = -2\ell(\hat{\boldsymbol{\beta}}) + 2p$$

**BIC** (Bayesian Information Criterion):
$$BIC = -2\ell(\hat{\boldsymbol{\beta}}) + p \log(n)$$

Lower values indicate better models (balancing fit and complexity).


``` r
# Compare several models
models <- list(
    "Age only" = glm(Diabetic ~ Age, data = diabetes_data, family = binomial),
    "BMI only" = glm(Diabetic ~ BMI, data = diabetes_data, family = binomial),
    "Age + BMI" = glm(Diabetic ~ Age + BMI, data = diabetes_data, family = binomial),
    "Age * BMI" = glm(Diabetic ~ Age * BMI, data = diabetes_data, family = binomial)
)

# Extract criteria
model_comparison <- data.table(
    Model = names(models),
    n_params = sapply(models, function(m) length(coef(m))),
    LogLik = sapply(models, function(m) round(as.numeric(logLik(m)), 2)),
    AIC = sapply(models, AIC),
    BIC = sapply(models, BIC)
)
model_comparison <- model_comparison[order(AIC)]

cat("Model Comparison:\n")
#> Model Comparison:
cat("=================\n\n")
#> =================
print(model_comparison)
#>        Model n_params   LogLik      AIC      BIC
#>       <char>    <int>    <num>    <num>    <num>
#> 1: Age + BMI        3 -1980.37 3966.750 3987.483
#> 2: Age * BMI        4 -1979.65 3967.301 3994.945
#> 3:  Age only        2 -2123.47 4250.938 4264.760
#> 4:  BMI only        2 -2262.93 4529.853 4543.675

cat("\nBest model by AIC:", model_comparison[1, Model], "\n")
#> 
#> Best model by AIC: Age + BMI
cat("Best model by BIC:", model_comparison[which.min(BIC), Model], "\n")
#> Best model by BIC: Age + BMI
```

---

## Communicating to Stakeholders

### Explaining GLM Results

**For Clinical Collaborators:**

"We used logistic regression to identify factors associated with diabetes. This method is specifically designed for yes/no outcomes and gives us results in terms of 'odds ratios' — how much more likely someone is to have diabetes given a change in a risk factor.

**Key findings:**

1. **Age effect**: For each additional year of age, the odds of having diabetes increase by about 6.1% (95% CI: 5.6% to 6.7%).

2. **BMI effect**: For each additional unit of BMI, the odds increase by about 10.1%.

**Model validation:**
We used likelihood ratio tests to confirm that both age and BMI significantly improve our predictions beyond what we'd expect by chance alone (p < 0.001). The model correctly distinguishes between diabetic and non-diabetic individuals about 70% of the time."

---

## Quick Reference

### MLEs for Common GLMs

| Distribution | Log-Likelihood |
|--------------|----------------|
| Binomial | $\sum[Y_i\log p_i + (1-Y_i)\log(1-p_i)]$ |
| Poisson | $\sum[Y_i\log\mu_i - \mu_i]$ |
| Normal | $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{\sum(Y_i-\mu_i)^2}{2\sigma^2}$ |

### Test Statistics

| Test | Statistic | Distribution |
|------|-----------|--------------|
| Wald | $z = \hat{\beta}/SE(\hat{\beta})$ | $N(0,1)$ |
| LRT | $-2[\ell_0 - \ell_1]$ | $\chi^2_{df}$ |
| Score | Based on score function | $\chi^2_{df}$ |

### R Code Patterns

```r
# Fit GLM
model <- glm(Y ~ X1 + X2, family = binomial, data = mydata)

# Summary with Wald tests
summary(model)

# Confidence intervals
confint.default(model)  # Wald-based
confint(model)          # Profile likelihood

# Likelihood ratio test
anova(model_reduced, model_full, test = "LRT")

# AIC/BIC
AIC(model1, model2)
BIC(model1, model2)

# Deviance residuals
residuals(model, type = "deviance")

# Pearson residuals
residuals(model, type = "pearson")

# Overdispersion check
sum(residuals(model, "pearson")^2) / model$df.residual
```
