---
title: "Statistics with R II: Intermediate"
chapter: "Chapter 2: Regression Diagnostics and Remedies"
part: "Part 2: Transformations and Robust Methods"
coverImage: 13
author: "Dereck Mezquita"
date: "2026-01-19"
tags: [statistics, mathematics, regression, transformations, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---



# Part 2: Transformations and Robust Methods

When diagnostic plots reveal assumption violations, we have several remedial options. This part covers **transformations** to stabilise variance and linearise relationships, **weighted least squares** to handle heteroscedasticity, and **robust regression** methods that resist the influence of outliers.


``` r
box::use(
    data.table[...],
    ggplot2
)
```


``` r
# Load datasets
nhanes <- fread("../../../data/primary/nhanes.csv")
hospital_los <- fread("../../../data/count/arizona_hospital_los.csv")

# Create cleaned hospital LOS data for use throughout chapter
# Note: age75 is a binary indicator for age > 75
# Assign to global environment so MASS::boxcox() can find it
los_data <<- hospital_los[!is.na(los) & los > 0,
                          .(los = los, age75 = age75, type1 = type1)]
los_data[, log_los := log(los)]

cat("Datasets loaded:\n")
#> Datasets loaded:
cat("  NHANES:", nrow(nhanes), "observations\n")
#>   NHANES: 10000 observations
cat("  Hospital LOS:", nrow(hospital_los), "observations\n")
#>   Hospital LOS: 1798 observations
cat("  Cleaned LOS data:", nrow(los_data), "observations\n")
#>   Cleaned LOS data: 1798 observations
```

---

## Table of Contents

## 2.1 Variable Transformations

### 2.1.1 Why Transform?

**Prose and Intuition**

Transformations serve three purposes:

1. **Stabilise variance**: Make error variance constant (fix heteroscedasticity)
2. **Linearise relationships**: Convert curved relationships to straight lines
3. **Normalise distributions**: Make skewed residuals more symmetric

The key insight: what matters is the *transformed* scale. If log(Y) has a linear relationship with X and constant error variance, then working on the log scale is appropriate—even if Y itself is skewed.

### 2.1.2 The Log Transformation

**Prose and Intuition**

The **logarithmic transformation** is the most common. It's appropriate when:
- The response is strictly positive
- The relationship is multiplicative rather than additive
- Variance increases with the mean (common for counts, concentrations, costs)

**Mathematical Interpretation**

If we model $\log(Y) = \beta_0 + \beta_1 X + \varepsilon$, then:
$$Y = e^{\beta_0} \cdot e^{\beta_1 X} \cdot e^{\varepsilon}$$

A one-unit increase in $X$ *multiplies* $Y$ by $e^{\beta_1}$.

For small $\beta_1$: $e^{\beta_1} \approx 1 + \beta_1$, so $\beta_1$ approximates the *percentage* change in $Y$.


``` r
# Hospital LOS data (cleaned in load_data chunk)
cat("Hospital Length of Stay Distribution:\n")
#> Hospital Length of Stay Distribution:
cat("=====================================\n\n")
#> =====================================
cat("Mean:", round(mean(los_data$los), 2), "days\n")
#> Mean: 4.85 days
cat("Median:", median(los_data$los), "days\n")
#> Median: 4 days
cat("SD:", round(sd(los_data$los), 2), "days\n")
#> SD: 3.74 days
cat("Skewness: Positive (right-skewed)\n")
#> Skewness: Positive (right-skewed)

# Compare distributions
p1 <- ggplot2$ggplot(los_data, ggplot2$aes(x = los)) +
    ggplot2$geom_histogram(bins = 30, fill = "#0072B2", alpha = 0.7) +
    ggplot2$labs(title = "Original Scale", x = "Length of Stay (days)", y = "Count") +
    ggplot2$theme_minimal()

p2 <- ggplot2$ggplot(los_data, ggplot2$aes(x = log_los)) +
    ggplot2$geom_histogram(bins = 30, fill = "#009E73", alpha = 0.7) +
    ggplot2$labs(title = "Log Scale", x = "log(Length of Stay)", y = "Count") +
    ggplot2$theme_minimal()

print(p1)
```

<Figure src="/courses/statistics-2-intermediate/log_transform-1.png" alt="Log transformation for hospital length of stay">
	Log transformation for hospital length of stay
</Figure>


``` r
print(p2)
```

<Figure src="/courses/statistics-2-intermediate/log_transform_hist-1.png" alt="Log transformation makes the distribution more symmetric">
	Log transformation makes the distribution more symmetric
</Figure>


``` r
# Fit both models using available predictors
model_original <- lm(los ~ age75 + type1, data = los_data)
model_log <- lm(log_los ~ age75 + type1, data = los_data)

cat("Model Comparison:\n")
#> Model Comparison:
cat("=================\n\n")
#> =================
cat("ORIGINAL SCALE: los ~ age75 + type1\n")
#> ORIGINAL SCALE: los ~ age75 + type1
print(summary(model_original)$coefficients)
#>              Estimate Std. Error   t value     Pr(>|t|)
#> (Intercept) 2.8896379  0.1452206 19.898267 8.954962e-80
#> age75       0.6823371  0.1839311  3.709743 2.137833e-04
#> type1       2.7397330  0.1721057 15.918896 1.790190e-53
cat("\nR² =", round(summary(model_original)$r.squared, 4), "\n\n")
#> 
#> R<U+00B2> = 0.1327

cat("LOG SCALE: log(los) ~ age75 + type1\n")
#> LOG SCALE: log(los) ~ age75 + type1
print(summary(model_log)$coefficients)
#>              Estimate Std. Error   t value      Pr(>|t|)
#> (Intercept) 0.8985806 0.02628690 34.183592 1.127514e-197
#> age75       0.1253416 0.03329403  3.764687  1.721241e-04
#> type1       0.6233549 0.03115347 20.009162  1.457372e-80
cat("\nR² =", round(summary(model_log)$r.squared, 4), "\n\n")
#> 
#> R<U+00B2> = 0.1912

cat("Interpretation (log model):\n")
#> Interpretation (log model):
cat("- Age > 75 increases LOS by approximately",
    round((exp(coef(model_log)["age75"]) - 1) * 100, 1), "%\n")
#> - Age > 75 increases LOS by approximately 13.4 %
cat("- Type1 admission increases LOS by approximately",
    round((exp(coef(model_log)["type1"]) - 1) * 100, 1), "%\n")
#> - Type1 admission increases LOS by approximately 86.5 %
```


``` r
# Compare residual plots
los_data[, `:=`(
    resid_orig = residuals(model_original),
    fitted_orig = fitted(model_original),
    resid_log = residuals(model_log),
    fitted_log = fitted(model_log)
)]

p1 <- ggplot2$ggplot(los_data, ggplot2$aes(x = fitted_orig, y = resid_orig)) +
    ggplot2$geom_point(alpha = 0.3) +
    ggplot2$geom_smooth(method = "loess", colour = "#D55E00", se = FALSE) +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed") +
    ggplot2$labs(title = "Original Scale: Residuals vs Fitted",
                 subtitle = "Clear heteroscedasticity",
                 x = "Fitted", y = "Residual") +
    ggplot2$theme_minimal()

p2 <- ggplot2$ggplot(los_data, ggplot2$aes(x = fitted_log, y = resid_log)) +
    ggplot2$geom_point(alpha = 0.3) +
    ggplot2$geom_smooth(method = "loess", colour = "#D55E00", se = FALSE) +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed") +
    ggplot2$labs(title = "Log Scale: Residuals vs Fitted",
                 subtitle = "Much improved",
                 x = "Fitted", y = "Residual") +
    ggplot2$theme_minimal()

print(p1)
#> `geom_smooth()` using formula = 'y ~ x'
```

<Figure src="/courses/statistics-2-intermediate/log_diagnostics-1.png" alt="Diagnostic plots show improvement with log transformation">
	Diagnostic plots show improvement with log transformation
</Figure>


``` r
print(p2)
#> `geom_smooth()` using formula = 'y ~ x'
```

<Figure src="/courses/statistics-2-intermediate/log_diagnostics_improved-1.png" alt="Residual plot after log transformation">
	Residual plot after log transformation
</Figure>

### 2.1.3 The Box-Cox Transformation

**Prose and Intuition**

The **Box-Cox transformation** is a family of power transformations that includes log as a special case. It helps find the optimal transformation automatically.

**Mathematical Definition**

For $Y > 0$:
$$Y^{(\lambda)} = \begin{cases}
\frac{Y^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\
\log(Y) & \text{if } \lambda = 0
\end{cases}$$

The parameter $\lambda$ is chosen to maximise the likelihood (or make residuals most normal).

**Common values:**
- $\lambda = 1$: No transformation
- $\lambda = 0.5$: Square root
- $\lambda = 0$: Log
- $\lambda = -1$: Reciprocal


``` r
# Box-Cox transformation
library(MASS)

# Find optimal lambda
bc <- boxcox(model_original, lambda = seq(-2, 2, by = 0.1), plotit = FALSE)
optimal_lambda <- bc$x[which.max(bc$y)]

cat("Box-Cox Analysis:\n")
#> Box-Cox Analysis:
cat("=================\n\n")
#> =================
cat("Optimal λ:", round(optimal_lambda, 3), "\n")
#> Optimal <U+03BB>: 0
cat("95% CI: Check if λ = 0 (log) or λ = 1 (no transform) is included\n")
#> 95% CI: Check if <U+03BB> = 0 (log) or <U+03BB> = 1 (no transform) is included

# Plot Box-Cox profile
bc_dt <- data.table(lambda = bc$x, loglik = bc$y)

ggplot2$ggplot(bc_dt, ggplot2$aes(x = lambda, y = loglik)) +
    ggplot2$geom_line(colour = "#0072B2", size = 1) +
    ggplot2$geom_vline(xintercept = optimal_lambda, colour = "#D55E00",
                       linetype = "dashed", size = 1) +
    ggplot2$geom_vline(xintercept = 0, colour = "#009E73",
                       linetype = "dotted", size = 1) +
    ggplot2$annotate("text", x = optimal_lambda + 0.1, y = max(bc$y),
                     label = paste("Optimal λ =", round(optimal_lambda, 2)),
                     hjust = 0, colour = "#D55E00") +
    ggplot2$annotate("text", x = 0.1, y = min(bc$y) + diff(range(bc$y)) * 0.1,
                     label = "λ = 0 (log)", hjust = 0, colour = "#009E73") +
    ggplot2$labs(
        title = "Box-Cox Profile Likelihood",
        subtitle = "Peak indicates optimal transformation",
        x = "λ (Power Parameter)",
        y = "Log-Likelihood"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/boxcox-1.png" alt="Box-Cox transformation finds optimal power">
	Box-Cox transformation finds optimal power
</Figure>

### 2.1.4 Transforming Predictors

**Prose and Intuition**

Sometimes the nonlinearity is in the predictors, not the response. Common predictor transformations:

- **Log(X)**: For predictors spanning orders of magnitude (e.g., income, concentration)
- **X²**: For U-shaped or inverted-U relationships
- **√X**: For diminishing returns
- **Polynomial terms**: For complex curved relationships


``` r
# Example: Age effect on blood pressure may be nonlinear
bp_data <- nhanes[!is.na(BPSysAve) & !is.na(Age) & Age >= 18,
                  .(SBP = BPSysAve, Age = Age)]
bp_data <- bp_data[complete.cases(bp_data)]

# Compare linear vs quadratic
model_linear <- lm(SBP ~ Age, data = bp_data)
model_quad <- lm(SBP ~ Age + I(Age^2), data = bp_data)

cat("Comparing Linear vs Quadratic Age Effect:\n")
#> Comparing Linear vs Quadratic Age Effect:
cat("==========================================\n\n")
#> ==========================================
cat("Linear model: SBP ~ Age\n")
#> Linear model: SBP ~ Age
cat("  R² =", round(summary(model_linear)$r.squared, 4), "\n\n")
#>   R<U+00B2> = 0.1733
cat("Quadratic model: SBP ~ Age + Age²\n")
#> Quadratic model: SBP ~ Age + Age<U+00B2>
cat("  R² =", round(summary(model_quad)$r.squared, 4), "\n")
#>   R<U+00B2> = 0.1805
cat("  Age coefficient:", round(coef(model_quad)["Age"], 3), "\n")
#>   Age coefficient: -0.053
cat("  Age² coefficient:", round(coef(model_quad)["I(Age^2)"], 5), "\n")
#>   Age<U+00B2> coefficient: 0.00477

# Test if quadratic term is significant
anova_test <- anova(model_linear, model_quad)
cat("\nF-test for quadratic term:\n")
#> 
#> F-test for quadratic term:
cat("  F =", round(anova_test$F[2], 2), ", p =", format.pval(anova_test$`Pr(>F)`[2]), "\n")
#>   F = 63.52 , p = 1.836e-15

# Visualise
age_seq <- data.table(Age = seq(18, 80, by = 1))
age_seq[, linear_pred := predict(model_linear, newdata = .SD)]
age_seq[, quad_pred := predict(model_quad, newdata = .SD)]

ggplot2$ggplot(bp_data[sample(.N, min(.N, 2000))], ggplot2$aes(x = Age, y = SBP)) +
    ggplot2$geom_point(alpha = 0.2) +
    ggplot2$geom_line(data = age_seq, ggplot2$aes(y = linear_pred),
                      colour = "#0072B2", size = 1) +
    ggplot2$geom_line(data = age_seq, ggplot2$aes(y = quad_pred),
                      colour = "#D55E00", size = 1) +
    ggplot2$labs(
        title = "Linear vs Quadratic Age Effect on Blood Pressure",
        subtitle = "Blue: linear | Orange: quadratic",
        x = "Age (years)",
        y = "Systolic Blood Pressure (mmHg)"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/predictor_transform-1.png" alt="Polynomial terms capture nonlinear relationships">
	Polynomial terms capture nonlinear relationships
</Figure>

---

## 2.2 Weighted Least Squares

### 2.2.1 The Idea

**Prose and Intuition**

When error variance is not constant (heteroscedasticity), ordinary least squares gives equal weight to all observations. But observations with larger variance are less reliable and should receive less weight.

**Weighted least squares (WLS)** minimises:
$$\sum_{i=1}^n w_i (Y_i - \hat{Y}_i)^2$$

where $w_i = 1/\sigma_i^2$ gives less weight to observations with higher variance.

### 2.2.2 Implementation

**Mathematical Derivation**

If we know the variance pattern $\text{Var}(\varepsilon_i) = \sigma^2 / w_i$, the WLS estimator is:
$$\hat{\boldsymbol{\beta}}_{WLS} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{Y}$$

where $\mathbf{W} = \text{diag}(w_1, w_2, \ldots, w_n)$.

In practice, we estimate the weights from the data, often using the fitted values from an initial OLS fit.


``` r
# Example where variance increases with fitted values
# Use hospital LOS data (original scale)
model_ols <- lm(los ~ age75 + type1, data = los_data)

# Estimate variance function: assume Var(e) proportional to E(Y)²
fitted_vals <- fitted(model_ols)
weights <- 1 / fitted_vals^2

# Weighted least squares
model_wls <- lm(los ~ age75 + type1, data = los_data, weights = weights)

cat("OLS vs WLS Comparison:\n")
#> OLS vs WLS Comparison:
cat("======================\n\n")
#> ======================
cat("OLS Coefficients:\n")
#> OLS Coefficients:
print(coef(model_ols))
#> (Intercept)       age75       type1 
#>   2.8896379   0.6823371   2.7397330
cat("\nWLS Coefficients:\n")
#> 
#> WLS Coefficients:
print(coef(model_wls))
#> (Intercept)       age75       type1 
#>   3.0050622   0.3420568   2.6914703

# Compare standard errors
cat("\nStandard Errors:\n")
#> 
#> Standard Errors:
se_comparison <- data.table(
    Variable = names(coef(model_ols)),
    SE_OLS = summary(model_ols)$coefficients[, "Std. Error"],
    SE_WLS = summary(model_wls)$coefficients[, "Std. Error"]
)
print(se_comparison)
#>       Variable    SE_OLS     SE_WLS
#>         <char>     <num>      <num>
#> 1: (Intercept) 0.1452206 0.09107428
#> 2:       age75 0.1839311 0.17660081
#> 3:       type1 0.1721057 0.14969309
```

### 2.2.3 Feasible GLS

**Prose and Intuition**

In practice, we don't know the true variance structure. **Feasible GLS (FGLS)** estimates it from the data:

1. Fit OLS and compute residuals
2. Model the variance as a function of predictors or fitted values
3. Use estimated variances as weights
4. Refit with WLS


``` r
# Step 1: OLS residuals
los_data[, resid_ols := residuals(model_ols)]
los_data[, fitted_ols := fitted(model_ols)]

# Step 2: Model variance (regress log(e²) on log(fitted))
los_data[, log_resid_sq := log(resid_ols^2 + 0.01)]  # Add small constant to avoid log(0)
los_data[, log_fitted := log(fitted_ols)]

var_model <- lm(log_resid_sq ~ log_fitted, data = los_data)

cat("Variance Function Estimation:\n")
#> Variance Function Estimation:
cat("=============================\n\n")
#> =============================
cat("Model: log(e²) ~ log(fitted)\n")
#> Model: log(e<U+00B2>) ~ log(fitted)
cat("Coefficient:", round(coef(var_model)["log_fitted"], 3), "\n")
#> Coefficient: 2.262
cat("If coefficient ≈ 2, variance is proportional to fitted²\n")
#> If coefficient <U+2248> 2, variance is proportional to fitted<U+00B2>

# Step 3: Compute weights
los_data[, est_var := exp(predict(var_model))]
los_data[, fgls_weight := 1 / est_var]

# Step 4: Refit with weights
model_fgls <- lm(los ~ age75 + type1, data = los_data, weights = fgls_weight)

cat("\nFGLS Coefficients:\n")
#> 
#> FGLS Coefficients:
print(summary(model_fgls)$coefficients)
#>              Estimate Std. Error   t value      Pr(>|t|)
#> (Intercept) 3.0158505 0.08673268 34.771790 5.613617e-203
#> age75       0.2992612 0.17621482  1.698275  8.962920e-02
#> type1       2.6870405 0.15128974 17.760890  3.824479e-65
```

---

## 2.3 Robust Regression

### 2.3.1 Why Robust Methods?

**Prose and Intuition**

Standard OLS is sensitive to outliers because the squared loss function gives disproportionate weight to large residuals. A single extreme point can dramatically change the fitted line.

**Robust regression** methods reduce the influence of outliers by using loss functions that grow more slowly than quadratic.

### 2.3.2 M-Estimation

**Mathematical Definition**

**M-estimation** minimises:
$$\sum_{i=1}^n \rho\left(\frac{e_i}{s}\right)$$

where $\rho$ is a robust loss function (less sensitive to large values than $\rho(x) = x^2$).

Common choices:
- **Huber's function**: Quadratic for small residuals, linear for large
- **Tukey's bisquare**: Completely downweights extreme outliers
- **Least absolute deviation (LAD)**: $\rho(x) = |x|$


``` r
# Create data with outliers
set.seed(42)
n <- 100
x <- runif(n, 0, 10)
y <- 2 + 3 * x + rnorm(n, 0, 2)

# Add outliers
outlier_idx <- c(95, 96, 97, 98, 99, 100)
y[outlier_idx] <- y[outlier_idx] + c(20, 25, 30, -25, -30, -35)

robust_data <- data.table(x = x, y = y, is_outlier = 1:n %in% outlier_idx)

# Fit OLS
model_ols <- lm(y ~ x, data = robust_data)

# Fit robust regression (M-estimation with Huber weights)
library(MASS)
model_rlm <- rlm(y ~ x, data = robust_data)

cat("OLS vs Robust Regression with Outliers:\n")
#> OLS vs Robust Regression with Outliers:
cat("========================================\n\n")
#> ========================================
cat("True model: Y = 2 + 3X + ε\n\n")
#> True model: Y = 2 + 3X + <U+03B5>
cat("OLS estimates:\n")
#> OLS estimates:
cat("  Intercept:", round(coef(model_ols)[1], 3), "(true: 2)\n")
#>   Intercept: 1.467 (true: 2)
cat("  Slope:", round(coef(model_ols)[2], 3), "(true: 3)\n\n")
#>   Slope: 3.063 (true: 3)
cat("Robust (Huber) estimates:\n")
#> Robust (Huber) estimates:
cat("  Intercept:", round(coef(model_rlm)[1], 3), "\n")
#>   Intercept: 1.494
cat("  Slope:", round(coef(model_rlm)[2], 3), "\n")
#>   Slope: 3.091

# Visualise
ggplot2$ggplot(robust_data, ggplot2$aes(x = x, y = y)) +
    ggplot2$geom_point(ggplot2$aes(colour = is_outlier, shape = is_outlier), size = 2) +
    ggplot2$geom_abline(intercept = 2, slope = 3, colour = "#009E73",
                        size = 1.2, linetype = "dashed") +
    ggplot2$geom_abline(intercept = coef(model_ols)[1], slope = coef(model_ols)[2],
                        colour = "#D55E00", size = 1) +
    ggplot2$geom_abline(intercept = coef(model_rlm)[1], slope = coef(model_rlm)[2],
                        colour = "#0072B2", size = 1) +
    ggplot2$scale_colour_manual(values = c("FALSE" = "gray50", "TRUE" = "#D55E00")) +
    ggplot2$scale_shape_manual(values = c("FALSE" = 16, "TRUE" = 17)) +
    ggplot2$labs(
        title = "OLS vs Robust Regression",
        subtitle = "Green dashed: true line | Orange: OLS | Blue: Robust (Huber)",
        x = "X", y = "Y",
        colour = "Outlier", shape = "Outlier"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/robust_regression-1.png" alt="Robust regression resists outliers">
	Robust regression resists outliers
</Figure>

### 2.3.3 Examining Robust Weights

**Prose and Intuition**

Robust methods assign weights to observations based on their residuals. Outliers receive lower weights, reducing their influence on the fit.


``` r
# Get weights from robust fit
robust_weights <- model_rlm$w

robust_data[, robust_weight := robust_weights]

cat("Robust Weights:\n")
#> Robust Weights:
cat("===============\n\n")
#> ===============
cat("Outlier observations:\n")
#> Outlier observations:
print(robust_data[is_outlier == TRUE, .(x = round(x, 2), y = round(y, 2),
                                         weight = round(robust_weight, 3))])
#>        x      y weight
#>    <num>  <num>  <num>
#> 1:  9.26  48.94  0.127
#> 2:  7.34  51.25  0.088
#> 3:  3.33  41.03  0.082
#> 4:  5.15  -8.41  0.093
#> 5:  7.44  -4.29  0.083
#> 6:  6.19 -16.54  0.064

cat("\nNon-outlier weight statistics:\n")
#> 
#> Non-outlier weight statistics:
cat("  Mean:", round(mean(robust_data[is_outlier == FALSE, robust_weight]), 4), "\n")
#>   Mean: 0.9574
cat("  Min:", round(min(robust_data[is_outlier == FALSE, robust_weight]), 4), "\n")
#>   Min: 0.3939

# Visualise weights
ggplot2$ggplot(robust_data, ggplot2$aes(x = x, y = y)) +
    ggplot2$geom_point(ggplot2$aes(size = robust_weight, colour = is_outlier), alpha = 0.7) +
    ggplot2$scale_size_continuous(range = c(0.5, 4), name = "Weight") +
    ggplot2$scale_colour_manual(values = c("FALSE" = "#0072B2", "TRUE" = "#D55E00")) +
    ggplot2$labs(
        title = "Robust Regression Weights",
        subtitle = "Outliers (orange) receive near-zero weights",
        x = "X", y = "Y",
        colour = "Outlier"
    ) +
    ggplot2$theme_minimal()
```

<Figure src="/courses/statistics-2-intermediate/robust_weights-1.png" alt="Robust regression downweights outliers">
	Robust regression downweights outliers
</Figure>

### 2.3.4 Least Trimmed Squares (LTS)

**Prose and Intuition**

**Least Trimmed Squares** takes a more aggressive approach: it minimises the sum of the smallest $(n - k)$ squared residuals, completely ignoring the $k$ largest.

This has a high **breakdown point** — it remains reliable even when up to 50% of the data are outliers.


``` r
# LTS regression using lqs() with method = "lts"
library(MASS)
n_use <- round(0.75 * nrow(robust_data))  # Use 75% of data
model_lts <- lqs(y ~ x, data = robust_data, method = "lts", quantile = n_use)

cat("Least Trimmed Squares:\n")
#> Least Trimmed Squares:
cat("======================\n\n")
#> ======================
cat("Uses", n_use, "of", nrow(robust_data), "observations\n")
#> Uses 75 of 100 observations
cat("LTS estimates:\n")
#> LTS estimates:
cat("  Intercept:", round(coef(model_lts)[1], 3), "(true: 2)\n")
#>   Intercept: 0.9 (true: 2)
cat("  Slope:", round(coef(model_lts)[2], 3), "(true: 3)\n")
#>   Slope: 3.23 (true: 3)
```

---

## 2.4 Dealing with Influential Points

### 2.4.1 Should We Remove Outliers?

**Prose and Intuition**

The decision to remove influential observations is context-dependent:

**Remove if:**
- Data entry error (typo, wrong units)
- Measurement malfunction
- Subject doesn't belong to the population of interest

**Keep if:**
- Real but unusual observation
- No clear reason for exclusion
- Removing would bias results

**Best practice:**
- Report results with and without influential points
- Investigate the cause of unusual values
- Use robust methods as a sensitivity check


``` r
# Identify influential points in hospital data
model_full <- lm(los ~ age75 + type1, data = los_data)
los_data[, cooks_d := cooks.distance(model_full)]

# Threshold for removal
cooks_threshold <- 4 / nrow(los_data)
influential <- los_data[cooks_d > cooks_threshold]

cat("Sensitivity Analysis:\n")
#> Sensitivity Analysis:
cat("=====================\n\n")
#> =====================
cat("Influential observations (Cook's D > 4/n):", nrow(influential), "\n")
#> Influential observations (Cook's D > 4/n): 63

# Refit without influential points
los_clean <- los_data[cooks_d <= cooks_threshold]
model_clean <- lm(los ~ age75 + type1, data = los_clean)

# Compare
cat("\nCoefficient Comparison:\n")
#> 
#> Coefficient Comparison:
comparison <- data.table(
    Variable = names(coef(model_full)),
    Full_Data = coef(model_full),
    Without_Influential = coef(model_clean)
)
comparison[, Percent_Change := round((Without_Influential - Full_Data) / Full_Data * 100, 2)]
print(comparison)
#>       Variable Full_Data Without_Influential Percent_Change
#>         <char>     <num>               <num>          <num>
#> 1: (Intercept) 2.8896379           2.7062115          -6.35
#> 2:       age75 0.6823371           0.3937835         -42.29
#> 3:       type1 2.7397330           2.4892819          -9.14

cat("\nRecommendation: Report both results and note the sensitivity.\n")
#> 
#> Recommendation: Report both results and note the sensitivity.
```

---

## 2.5 Summary: Choosing the Right Remedy


``` r
cat("Decision Guide for Regression Remedies:\n")
#> Decision Guide for Regression Remedies:
cat("=========================================\n\n")
#> =========================================

cat("PROBLEM: Heteroscedasticity (non-constant variance)\n")
#> PROBLEM: Heteroscedasticity (non-constant variance)
cat("  → Transform Y (log, sqrt) if variance increases with mean\n")
#>   <U+2192> Transform Y (log, sqrt) if variance increases with mean
cat("  → Use weighted least squares\n")
#>   <U+2192> Use weighted least squares
cat("  → Use robust/sandwich standard errors\n\n")
#>   <U+2192> Use robust/sandwich standard errors

cat("PROBLEM: Nonlinearity\n")
#> PROBLEM: Nonlinearity
cat("  → Transform X (log, polynomial)\n")
#>   <U+2192> Transform X (log, polynomial)
cat("  → Transform Y\n")
#>   <U+2192> Transform Y
cat("  → Use GAMs (generalized additive models)\n\n")
#>   <U+2192> Use GAMs (generalized additive models)

cat("PROBLEM: Outliers\n")
#> PROBLEM: Outliers
cat("  → Investigate cause\n")
#>   <U+2192> Investigate cause
cat("  → Use robust regression (M-estimation, LTS)\n")
#>   <U+2192> Use robust regression (M-estimation, LTS)
cat("  → Sensitivity analysis (with/without)\n\n")
#>   <U+2192> Sensitivity analysis (with/without)

cat("PROBLEM: Non-normal residuals\n")
#> PROBLEM: Non-normal residuals
cat("  → With large n: Usually not a concern (CLT)\n")
#>   <U+2192> With large n: Usually not a concern (CLT)
cat("  → Transform Y\n")
#>   <U+2192> Transform Y
cat("  → Use bootstrap for inference\n\n")
#>   <U+2192> Use bootstrap for inference

cat("PROBLEM: Independence violation\n")
#> PROBLEM: Independence violation
cat("  → Clustered data: Use mixed-effects models (Chapter 6)\n")
#>   <U+2192> Clustered data: Use mixed-effects models (Chapter 6)
cat("  → Time series: Use time series methods (Chapter 7)\n")
#>   <U+2192> Time series: Use time series methods (Chapter 7)
cat("  → Spatial data: Use spatial models (Part III)\n")
#>   <U+2192> Spatial data: Use spatial models (Part III)
```

---

## Communicating to Stakeholders

### Explaining Transformations

**For Clinical Collaborators:**

"We found that the relationship between age and hospital length of stay isn't well-captured by our initial model. The statistical diagnostics showed that:

1. Patients with longer predicted stays had more variable actual stays
2. The errors weren't evenly distributed

To address this, we analysed length of stay on the *logarithmic scale*. This is common in healthcare because costs and durations tend to follow multiplicative rather than additive patterns.

On this scale, we find that each additional year of age is associated with approximately a 1% increase in length of stay. This means the effect of age compounds: a 70-year-old patient stays about 50% longer than a 30-year-old, all else being equal.

We also ran a 'robust' analysis that downweights unusual observations. The results were similar, giving us confidence that our findings aren't driven by a few extreme cases."

---

## Quick Reference

### Common Transformations

| Transformation | When to Use | Interpretation |
|----------------|-------------|----------------|
| $\log(Y)$ | Right-skewed, variance ∝ mean | Multiplicative effects |
| $\sqrt{Y}$ | Count data, mild skew | Square root scale |
| $1/Y$ | Strong right skew | Reciprocal scale |
| $\log(X)$ | X spans orders of magnitude | Elasticity |
| $X^2$ | U-shaped relationship | Quadratic |

### Robust Methods

| Method | Loss Function | Breakdown Point |
|--------|---------------|-----------------|
| OLS | $e^2$ | 0% |
| Huber | Quadratic small, linear large | ~25% |
| Tukey bisquare | Bounded | ~50% |
| LAD | $|e|$ | ~30% |
| LTS | Trimmed sum | ~50% |

### R Code Patterns

```r
# Log transformation
model_log <- lm(log(Y) ~ X1 + X2, data = mydata)

# Box-Cox transformation
library(MASS)
bc <- boxcox(model)
optimal_lambda <- bc$x[which.max(bc$y)]

# Weighted least squares
model_wls <- lm(Y ~ X, data = mydata, weights = 1/variance)

# Robust regression (Huber)
library(MASS)
model_robust <- rlm(Y ~ X, data = mydata)

# Robust regression (Tukey bisquare)
model_robust <- rlm(Y ~ X, data = mydata, method = "MM")

# Least trimmed squares
model_lts <- ltsreg(Y ~ X, data = mydata)

# Robust standard errors
library(sandwich)
library(lmtest)
coeftest(model, vcov = vcovHC(model, type = "HC1"))
```
