---
title: "Statistics with R"
chapter: "Foundations of Statistical Thinking"
coverImage: 13
author: "Dereck Mezquita"
date: 2023-10-20
tags: [statistics, mathematics, probability, data]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        # x <- paste0("/courses/", x)
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng) # optipng = '-o7'
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7)
```

# Foundations of Statistical Thinking

Statistics transforms raw data into actionable knowledge. This chapter lays the groundwork for understanding statistical thinking and the computational approaches used throughout this book. We'll explore how statistics fits into the modern scientific process, the core concepts driving statistical analysis, and how R empowers researchers to implement these methodologies efficiently.

## 1.1 The Role of Statistics in the Modern Scientific Process

Statistics serves as the bridge between data collection and scientific insight. In contemporary research across disciplines, statistical methodology provides the framework for:

* Extracting meaningful patterns from complex datasets
* Quantifying uncertainty in our observations
* Testing hypotheses rigorously
* Making predictions with measurable confidence
* Communicating findings with precision

### 1.1.1 From Data Collection to Inference

The journey from raw data to scientific conclusions follows a structured path:

* **Data Collection**: Gathering observations through experiments, surveys, sensors, or other means
* **Data Processing**: Cleaning, transforming, and preparing data for analysis
* **Exploratory Analysis**: Identifying patterns, relationships, and potential issues
* **Statistical Modelling**: Fitting mathematical models to extract underlying structure
* **Inference**: Drawing conclusions about populations from sample data
* **Communication**: Presenting findings using appropriate visualisations and interpretations

Let's demonstrate this process with a simple example:

```{r data_to_inference, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD, setDT],
    ggplot2
)

# 1. Data Collection (simulated experiment)
set.seed(42)
n <- 100
treatment <- factor(rep(c("Control", "Treatment"), each = n/2))
response <- c(rnorm(n/2, mean = 5, sd = 1.2),
              rnorm(n/2, mean = 6.5, sd = 1.5))
experiment_dt <- data.table(treatment, response)

# 2. Data Processing
# Check for missing values or outliers
summary(experiment_dt)

# 3. Exploratory Analysis
# Calculate summary statistics by group
summary_stats <- experiment_dt[, .(
    n = .N,
    mean = mean(response),
    sd = sd(response),
    median = median(response),
    min = min(response),
    max = max(response)
), by = treatment]

print(summary_stats)

# Visualise the distributions
ggplot2$ggplot(experiment_dt, ggplot2$aes(x = treatment, y = response, fill = treatment)) +
    ggplot2$geom_boxplot(alpha = 0.7) +
    ggplot2$geom_jitter(width = 0.2, alpha = 0.5) +
    ggplot2$labs(
        title = "Response by Treatment Group",
        x = "Treatment Group",
        y = "Response"
    ) +
    ggplot2$theme_minimal()

# 4. Statistical Modelling
# Conduct t-test for difference between groups
t_test_result <- t.test(response ~ treatment, data = experiment_dt)

# 5. Inference
print(t_test_result)

# 6. Communication - Visualise confidence intervals
ci_dt <- data.table(
    group = c("Control", "Treatment"),
    mean = c(t_test_result$estimate[1], t_test_result$estimate[2]),
    lower = c(
        t_test_result$estimate[1] - qt(0.975, t_test_result$parameter) * t_test_result$stderr,
        t_test_result$estimate[2] - qt(0.975, t_test_result$parameter) * t_test_result$stderr
    ),
    upper = c(
        t_test_result$estimate[1] + qt(0.975, t_test_result$parameter) * t_test_result$stderr,
        t_test_result$estimate[2] + qt(0.975, t_test_result$parameter) * t_test_result$stderr
    )
)

ggplot2$ggplot(ci_dt, ggplot2$aes(x = group, y = mean, colour = group)) +
    ggplot2$geom_point(size = 3) +
    ggplot2$geom_errorbar(ggplot2$aes(ymin = lower, ymax = upper), width = 0.2) +
    ggplot2$labs(
        title = "95% Confidence Intervals for Group Means",
        x = "Group",
        y = "Mean Response"
    ) +
    ggplot2$theme_minimal()
```

This example demonstrates how we move from raw data to statistical inference through a series of structured steps. The confidence intervals and hypothesis test provide a framework for making claims about the treatment effect with quantified uncertainty.

### 1.1.2 Confirmatory vs. Exploratory Analysis

Statistical analyses typically fall into two broad categories:

* **Confirmatory Analysis**: Testing pre-defined hypotheses
  * Requires specified hypotheses before data collection
  * Controls error rates through formal inference procedures
  * Produces results that can be interpreted in terms of statistical significance
  * Examples: Clinical trials, experimental research, quality control

* **Exploratory Analysis**: Discovering patterns without pre-specified hypotheses
  * Generates hypotheses for future testing
  * Examines relationships between many variables
  * Uses visualisation and descriptive statistics heavily
  * Does not control for multiple testing issues
  * Examples: Data mining, preliminary research, anomaly detection

Both approaches are valuable but serve different purposes and require different interpretations. Let's demonstrate the difference:

```{r confirmatory_vs_exploratory, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD, setDT, like],
    ggplot2,
    stats[cor]
)

# Generate multivariate dataset
set.seed(123)
n <- 200
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- rnorm(n)
y <- 2*x1 + 0.5*x2 + rnorm(n, sd = 1)  # y genuinely depends on x1 and x2

multivar_dt <- data.table(y, x1, x2, x3, x4)

# Confirmatory analysis - testing specific hypothesis
# Here we test if x1 has a significant effect on y
confirmatory_model <- lm(y ~ x1, data = multivar_dt)
summary(confirmatory_model)

# Exploratory analysis - looking at correlations among all variables
cor_matrix <- cor(multivar_dt)
print(cor_matrix)

# Visualise the correlation matrix
cor_dt <- as.data.table(cor_matrix, keep.rownames = TRUE)
setnames(cor_dt, "rn", "variable1")
correlation_long <- melt(cor_dt, id.vars = "variable1", 
                      variable.name = "variable2", value.name = "correlation")

ggplot2$ggplot(correlation_long, 
       ggplot2$aes(x = variable1, y = variable2, fill = correlation)) +
  ggplot2$geom_tile() +
  ggplot2$scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1)) +
  ggplot2$labs(title = "Correlation Matrix Heatmap (Exploratory Analysis)") +
  ggplot2$theme_minimal() +
  ggplot2$theme(axis.text.x = ggplot2$element_text(angle = 45, hjust = 1))

# Visualise the relationship that would be tested in confirmatory analysis
ggplot2$ggplot(multivar_dt, ggplot2$aes(x = x1, y = y)) +
  ggplot2$geom_point(alpha = 0.6) +
  ggplot2$geom_smooth(method = "lm", formula = y ~ x) +
  ggplot2$labs(
    title = "Confirmatory Analysis: Testing Specific Relationship",
    x = "Predictor (x1)",
    y = "Response (y)"
  ) +
  ggplot2$theme_minimal()
```

The confirmatory analysis focuses on testing a specific hypothesis about the relationship between y and x1, while the exploratory analysis examines all possible relationships in the data without preconceived hypotheses.

### 1.1.3 Statistical Paradigms: Frequentist, Bayesian, and Likelihoodist Approaches

Statistics has evolved several philosophical frameworks for inference:

* **Frequentist Statistics**
  * Defines probability as long-run frequency
  * Uses p-values and confidence intervals
  * Focuses on controlling error rates
  * Does not incorporate prior information formally
  * Most commonly used in scientific publications

* **Bayesian Statistics**
  * Treats probability as a degree of belief
  * Updates prior beliefs with observed data
  * Produces posterior distributions rather than point estimates
  * Provides a natural framework for sequential updating
  * Growing in popularity due to computational advances

* **Likelihoodist Approach**
  * Focuses on the likelihood function
  * Uses likelihood ratios to compare hypotheses
  * Avoids priors but doesn't have the frequency-based guarantees
  * Provides a middle ground between frequentist and Bayesian approaches

Let's demonstrate these approaches with a simple coin flipping example:

```{r statistical_paradigms, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD],
    ggplot2
)

# Example: Observed 7 heads in 10 coin flips
# Question: Is the coin fair?

# Frequentist approach - Binomial test
freq_test <- binom.test(7, 10, p = 0.5)
print(freq_test)

# Bayesian approach - Beta-Binomial model
# Prior: Beta(1,1) (uniform prior)
# Posterior: Beta(1+7, 1+3) = Beta(8, 4)

# Function to compute Beta PDF
beta_pdf <- function(x, alpha, beta) {
  return(dbeta(x, alpha, beta))
}

# Create grid of values for theta (probability of heads)
theta_grid <- seq(0, 1, length.out = 100)

# Compute prior, likelihood, and posterior
prior <- dbeta(theta_grid, 1, 1)
likelihood <- dbinom(7, 10, theta_grid)
posterior <- dbeta(theta_grid, 8, 4)

# Normalize likelihood to compare on same scale
likelihood_scaled <- likelihood / max(likelihood)

# Create data table for plotting
paradigms_dt <- data.table(
  theta = rep(theta_grid, 3),
  density = c(prior, likelihood_scaled, posterior),
  type = factor(rep(c("Prior", "Likelihood", "Posterior"), each = length(theta_grid)),
                levels = c("Prior", "Likelihood", "Posterior"))
)

# Visualise the distributions
ggplot2$ggplot(paradigms_dt, ggplot2$aes(x = theta, y = density, color = type)) +
  ggplot2$geom_line(linewidth = 1.2) +
  ggplot2$labs(
    title = "Statistical Paradigms Comparison",
    subtitle = "Analyzing 7 heads in 10 coin flips",
    x = "θ (Probability of Heads)",
    y = "Density/Scaled Likelihood",
    color = "Distribution Type"
  ) +
  ggplot2$geom_vline(xintercept = 0.5, linetype = "dashed", color = "darkgrey") +
  ggplot2$annotate("text", x = 0.51, y = 0.2, label = "Fair Coin (θ = 0.5)") +
  ggplot2$theme_minimal()

# Compute Bayesian credible interval
bayes_ci <- qbeta(c(0.025, 0.975), 8, 4)
cat("Bayesian 95% credible interval:", bayes_ci[1], "to", bayes_ci[2], "\n")

# Compute likelihoodist support interval
# Based on likelihood ratio relative to maximum likelihood
relative_likelihood <- likelihood / max(likelihood)
support_interval <- range(theta_grid[relative_likelihood > 1/8])
cat("Likelihoodist support interval:", support_interval[1], "to", support_interval[2], "\n")
```

Each approach provides a different perspective on the same question. The frequentist approach gives a p-value and confidence interval, the Bayesian approach provides a posterior distribution and credible interval, and the likelihoodist approach focuses on the likelihood function and support interval.

Throughout this book, we'll use all three paradigms where appropriate, with a particular focus on implementing these approaches efficiently in R.

## 1.2 Working with Data in R

Think of data as the raw material that statistics helps you shape into insights. Just as a carpenter needs the right tools for different types of wood, a statistician needs the right data structures for different analytical tasks. Let's explore how R helps us organise and work with data efficiently.

### 1.2.1 Efficient Data Structures for Statistical Computing

Have you ever tried to organise a large collection of items? You might use different systems: a filing cabinet for documents, a toolbox with compartments, or a wardrobe with shelves and hangers. Similarly, R offers several ways to organise data, each with its own strengths:

* **Vectors**: The simplest data structure - like a row of identical boxes holding similar items
  * Perfect for storing a series of numbers (like ages or temperatures)
  * Makes math operations lightning-fast because R can work on all values at once
  * The foundation that other data structures build upon

* **Matrices**: Think of these as spreadsheets with rows and columns of the same type of value
  * Great for mathematical operations like those used in linear algebra
  * Essential for calculations involving multiple dimensions
  * Stores data in a memory-efficient way that speeds up calculations

* **Data Frames**: Like a spreadsheet where each column can hold a different type of data
  * The workhorse of statistical analysis
  * Columns can be numbers, text, dates, or other types
  * Allows you to keep related information together (like a patient's age, blood pressure, and diagnosis)

* **Lists**: Imagine a container that can hold anything - other containers, single items, mixed types
  * The most flexible data structure
  * Perfect for storing complex results from statistical analyses
  * Can hold elements of different types and sizes

* **Factors**: Special vectors designed for categorical data
  * Efficiently stores repeated values (like "male"/"female" or "control"/"treatment")
  * Keeps track of all possible categories, even absent ones
  * Crucial for statistical models that need to know all possible groups

Let's see these in action with a simple example:

```{r data_structures_basic, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD],
    ggplot2
)

# A simple vector of body temperatures
temperature <- c(36.8, 37.1, 36.9, 37.5, 36.4, 37.2)
cat("Vector example (temperatures in °C):\n")
print(temperature)
cat("Mean temperature:", mean(temperature), "°C\n")

# A matrix of blood pressure readings [systolic, diastolic]
# Each row is a different person
bp_matrix <- matrix(
  c(120, 80, 115, 75, 140, 90, 125, 85, 130, 82), 
  ncol = 2, 
  byrow = TRUE,
  dimnames = list(paste0("Person", 1:5), c("Systolic", "Diastolic"))
)
cat("\nMatrix example (blood pressure readings):\n")
print(bp_matrix)

# A data frame with mixed types of information
patients <- data.frame(
  name = c("Alice", "Bob", "Charlie", "Diana", "Evan"),
  age = c(45, 52, 38, 61, 29),
  sex = factor(c("F", "M", "M", "F", "M")),
  temperature = c(36.8, 37.1, 36.9, 37.5, 36.4),
  diagnosis = c("Healthy", "Hypertension", "Healthy", "Diabetes", "Healthy")
)
cat("\nData frame example (patient records):\n")
print(patients)

# A list containing different types of data
patient_info <- list(
  demographics = data.frame(
    total_patients = 5,
    mean_age = mean(patients$age),
    sex_ratio = table(patients$sex)
  ),
  temperature_stats = list(
    mean = mean(patients$temperature),
    range = range(patients$temperature)
  ),
  diagnoses = table(patients$diagnosis)
)
cat("\nList example (summary information):\n")
str(patient_info)

# Creating a factor to represent categories
diagnosis_factor <- factor(
  c("Healthy", "Hypertension", "Healthy", "Diabetes", "Healthy"),
  levels = c("Healthy", "Hypertension", "Diabetes", "Flu")  # Notice we include "Flu" even though no one has it
)
cat("\nFactor example (patient diagnoses):\n")
print(diagnosis_factor)
cat("Levels (all possible categories):", levels(diagnosis_factor), "\n")
```

Why does the choice of data structure matter? Because it affects:
1. How quickly your analysis runs
2. How much memory your data uses
3. How easily you can perform different operations
4. How clearly your code expresses your intent

For small datasets, these differences might be subtle. But as your data grows to thousands or millions of records, choosing the right structure becomes crucial for efficient analysis.

### 1.2.2 Data.table for High-Performance Data Manipulation

Imagine you're sorting a deck of cards. If you have just a few cards, any method works fine. But with thousands of cards, you'd need a systematic approach. That's where data.table comes in - it's like having a card-sorting machine when others are sorting by hand.

Data.table is a supercharged version of R's standard data.frame that's built for speed and efficiency. Here's why it's transformative for statistical work:

* **Blazing fast**: Operations that might take minutes with standard data frames often take seconds with data.table
* **Memory efficient**: It updates data in place rather than creating copies (great for big datasets)
* **Concise syntax**: You can accomplish in one line what might take several lines with other approaches
* **Seamless integration**: Works with base R functions while offering powerful extensions

Let's see how data.table makes common statistical tasks more efficient using a real-world example:

```{r data_table_intro, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD, setDT, :=],
    ggplot2
)

# Create a dataset of health measurements (1000 people)
set.seed(123)
n <- 1000

# Generate some realistic health data
health_dt <- data.table(
  person_id = 1:n,
  age = sample(18:80, n, replace = TRUE),
  sex = sample(c("M", "F"), n, replace = TRUE),
  bmi = round(rnorm(n, mean = 26, sd = 4.5), 1),
  systolic_bp = round(rnorm(n, mean = 120, sd = 15)),
  cholesterol = round(rnorm(n, mean = 5.2, sd = 1.1), 2),
  exercise_weekly = sample(0:7, n, replace = TRUE)  # days per week
)

# Let's add a health risk score based on these values
health_dt[, health_risk := 
  (systolic_bp > 140) * 2 +  # High blood pressure
  (cholesterol > 6.2) * 2 +  # High cholesterol
  (bmi > 30) * 1.5 +         # Obesity
  (bmi < 18.5) * 1 +         # Underweight
  (exercise_weekly < 3) * 1  # Insufficient exercise
]

# 1. Calculating summary statistics by group
# How do average measurements differ between men and women?
sex_comparison <- health_dt[, .(
  count = .N,
  avg_age = round(mean(age), 1),
  avg_bmi = round(mean(bmi), 1),
  avg_systolic = round(mean(systolic_bp), 1),
  avg_cholesterol = round(mean(cholesterol), 2),
  avg_risk = round(mean(health_risk), 2)
), by = sex]

cat("Health measurements by sex:\n")
print(sex_comparison)

# 2. Finding high-risk individuals and summarizing
high_risk <- health_dt[health_risk >= 3]
cat("\nNumber of high-risk individuals:", nrow(high_risk), 
    "(", round(100 * nrow(high_risk) / n, 1), "% of population)\n")

# 3. Looking at risk by age group
health_dt[, age_group := cut(age, 
                            breaks = c(0, 30, 45, 60, 100),
                            labels = c("18-30", "31-45", "46-60", "61+"))]

risk_by_age_sex <- health_dt[, .(
  avg_risk = round(mean(health_risk), 2),
  high_risk_pct = round(100 * mean(health_risk >= 3), 1),
  count = .N
), by = .(age_group, sex)]

cat("\nHealth risk by age group and sex:\n")
print(risk_by_age_sex)

# Visualize the results
ggplot2$ggplot(risk_by_age_sex, 
              ggplot2$aes(x = age_group, y = high_risk_pct, 
                         fill = sex, label = paste0(high_risk_pct, "%"))) +
  ggplot2$geom_bar(stat = "identity", position = "dodge") +
  ggplot2$geom_text(position = ggplot2$position_dodge(width = 0.9), vjust = -0.5) +
  ggplot2$labs(
    title = "Percentage of High-Risk Individuals by Age Group and Sex",
    x = "Age Group",
    y = "Percentage with High Risk Score (≥3)",
    fill = "Sex"
  ) +
  ggplot2$theme_minimal()
```

What makes data.table special? Let's break down its approach:

1. **Fast grouping with `by`**: The `by` parameter lets you split your data into groups and apply calculations to each group independently. This is much faster than the traditional approach of splitting data, applying functions, and recombining results.

2. **Concise syntax with `[...]`**: Data.table uses a special syntax inside square brackets that follows a pattern:
   * `DT[where, select|compute, by]` (filter rows, select/compute columns, group by)
   * This lets you perform complex operations in a single, readable line

3. **In-place modification with `:=`**: Rather than creating copies of your data (which can be memory-intensive), data.table can modify data in place using the `:=` operator.

4. **Special symbols like `.N` and `.SD`**: Data.table provides helpful shortcuts:
   * `.N` gives you the count of rows
   * `.SD` represents the subset of data for each group
   * These make common operations more concise and readable

Let's see some more advanced data.table features in action:

```{r data_table_advanced, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD, setDT, :=, setkey],
    ggplot2
)

# Continuing with our health dataset
# Let's add some more realistic health data and show advanced features

# 1. Adding new columns efficiently
health_dt[, `:=`(
  diastolic_bp = round(rnorm(n, mean = 80 - 0.1*(18-age), sd = 10)),
  hdl = round(rnorm(n, mean = ifelse(sex == "F", 1.6, 1.4), sd = 0.4), 2),
  smoking = sample(c("Never", "Former", "Current"), n, 
                  prob = c(0.6, 0.2, 0.2), replace = TRUE)
)]

# 2. Calculate multiple values at once
bp_summary <- health_dt[, .(
  systolic_mean = mean(systolic_bp),
  systolic_sd = sd(systolic_bp),
  diastolic_mean = mean(diastolic_bp),
  diastolic_sd = sd(diastolic_bp),
  count = .N,
  high_bp_pct = 100 * mean(systolic_bp >= 140 | diastolic_bp >= 90)
), by = .(age_group, sex)]

cat("Blood pressure summary by age group and sex:\n")
print(bp_summary)

# 3. Using .SD (Subset of Data) for flexible operations
# Calculate multiple statistics for multiple columns
health_vars <- c("systolic_bp", "diastolic_bp", "cholesterol", "hdl")

health_summary <- health_dt[, lapply(.SD, function(x) {
  c(mean = mean(x), median = median(x), sd = sd(x))
}), .SDcols = health_vars, by = .(age_group, sex)]

# A cleaner way to display this complex result
cat("\nComprehensive health statistics by age group and sex:\n")
first_group <- health_summary[1]
print(first_group)
cat("... [similar statistics for other groups] ...\n")

# 4. Creating a key for ultra-fast lookups
setkey(health_dt, person_id)

# Now lookups by person_id are extremely fast
person_42 <- health_dt[42]
cat("\nQuick lookup for person #42:\n")
print(person_42)

# 5. Chaining operations (performing multiple steps in sequence)
# Find people with high cholesterol but good HDL levels
high_chol_good_hdl <- health_dt[cholesterol > 6.0][hdl > 1.5]
cat("\nNumber of people with high cholesterol but good HDL:", nrow(high_chol_good_hdl), "\n")

# 6. Complex filtering with multiple conditions
at_risk <- health_dt[systolic_bp > 140 | 
                   diastolic_bp > 90 | 
                   cholesterol > 6.2 | 
                   bmi > 30 |
                   (smoking == "Current" & age > 50)]

cat("\nTotal people with at least one risk factor:", nrow(at_risk), 
    "(", round(100 * nrow(at_risk) / n, 1), "% of population)\n")

# Visualize risk factors by age and sex
ggplot2$ggplot(health_dt, ggplot2$aes(x = age, y = systolic_bp, color = sex)) +
  ggplot2$geom_point(alpha = 0.5) +
  ggplot2$geom_smooth(method = "loess") +
  ggplot2$geom_hline(yintercept = 140, linetype = "dashed", color = "red") +
  ggplot2$labs(
    title = "Systolic Blood Pressure by Age and Sex",
    subtitle = "Red line indicates hypertension threshold (140 mmHg)",
    x = "Age",
    y = "Systolic BP (mmHg)",
    color = "Sex"
  ) +
  ggplot2$theme_minimal()
```

The power of data.table becomes clear when working with larger datasets and more complex operations. Throughout this book, we'll leverage this efficiency to implement statistical methods that scale well to real-world data sizes.

### 1.2.3 Tidy Data Principles and Their Implementation

Have you ever tried to analyse data that's formatted inconsistently or awkwardly structured? It's like trying to build a house with materials scattered across different construction sites. This is where "tidy data" principles come in - they provide a consistent way to organise data that makes analysis much more straightforward.

The core principles of tidy data are simple:

1. **Each variable forms a column** - One column shouldn't contain multiple types of information
2. **Each observation forms a row** - A single row should represent one complete observation
3. **Each type of observational unit forms a table** - Different types of entities should be in separate tables

Let's see what this means in practice:

```{r tidy_data_intro, echo=TRUE}
box::use(
    data.table[data.table, .N, .SD, setDT, :=],
    ggplot2
)

# Example 1: Untidy data - Multiple variables in one column
messy_data <- data.table(
  patient = c("John", "Mary", "Robert", "Alicia"),
  info = c("42,M,185", "29,F,165", "58,M,177", "35,F,172")
)

cat("Messy data (multiple variables in one column):\n")
print(messy_data)

# Converting to tidy format
tidy_data <- copy(messy_data)
tidy_data[, c("age", "sex", "height") := tstrsplit(info, ",")]
tidy_data[, `:=`(
  age = as.integer(age),
  height = as.integer(height),
  info = NULL  # Remove the original messy column
)]

cat("\nTidy version (each variable in its own column):\n")
print(tidy_data)

# Example 2: Untidy data - Multiple observations in one row
messy_bp <- data.table(
  patient = c("John", "Mary", "Robert", "Alicia"),
  day1_systolic = c(120, 118, 142, 116),
  day1_diastolic = c(82, 76, 88, 78),
  day2_systolic = c(122, 120, 138, 118),
  day2_diastolic = c(80, 75, 85, 76)
)

cat("\nMessy blood pressure data (multiple observations per row):\n")
print(messy_bp)

# Converting to tidy format - each observation in its own row
bp_long <- melt(messy_bp, 
               id.vars = "patient",
               measure.vars = patterns("^day\\d+_systolic", "^day\\d+_diastolic"),
               variable.name = "day",
               value.name = c("systolic", "diastolic"))

# Clean up the day variable
bp_long[, day := paste("Day", gsub("day(\\d+)_systolic", "\\1", day))]

cat("\nTidy version (each observation in its own row):\n")
print(bp_long)

# Example 3: Value vs. variable confusion
messy_lab <- data.table(
  patient = c("John", "Mary", "Robert", "Alicia"),
  test = c("cholesterol", "glucose", "cholesterol", "glucose"),
  result = c(5.8, 4.9, 6.2, 5.1)
)

cat("\nMessy lab data (test name is a value, not a variable):\n")
print(messy_lab)

# Converting to tidy format - spread the test types into columns
lab_wide <- dcast(messy_lab, patient ~ test, value.var = "result")

cat("\nTidy version (each test type as a separate variable):\n")
print(lab_wide)

# Visualizing becomes much easier with tidy data
ggplot2$ggplot(bp_long, ggplot2$aes(x = day, y = systolic, group = patient, color = patient)) +
  ggplot2$geom_point() + 
  ggplot2$geom_line() +
  ggplot2$labs(
    title = "Systolic Blood Pressure by Patient Over Time",
    x = "Measurement Day",
    y = "Systolic BP (mmHg)",
    color = "Patient"
  ) +
  ggplot2$theme_minimal()
```

Why does tidy data matter for statistics? Because it makes your analysis workflow more:

* **Consistent**: You apply the same patterns across different datasets
* **Intuitive**: Variables naturally map to dimensions in your visualisations
* **Compatible**: Many R packages (especially ggplot2) work best with tidy data
* **Flexible**: It's easier to reshape tidy data for different types of analysis

While data.table works perfectly with tidy principles, it also gives you the performance to handle the transformations needed to tidy up messy data efficiently.

When you combine efficient data structures, high-performance tools like data.table, and tidy data principles, you build a solid foundation for statistical analysis that can handle real-world complexity while remaining clear and maintainable.
