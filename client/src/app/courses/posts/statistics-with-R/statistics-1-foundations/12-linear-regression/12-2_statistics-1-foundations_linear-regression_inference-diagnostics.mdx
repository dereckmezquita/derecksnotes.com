---
title: "Regression Inference and Diagnostics"
subtitle: "Part 2 of Chapter 12: Linear Regression"
author: "Dereck Mezquita"
date: "2026-01-18"
output: html_document
---



## 12.11 Model Diagnostics: Checking Assumptions

The validity of regression inference depends on the model assumptions. This section covers how to diagnose violations and their consequences.


``` r
box::use(
    data.table[...],
    ggplot2
)
```


``` r
# Load NHANES data
nhanes <- fread("../../../data/primary/nhanes.csv")

# Create analysis dataset
set.seed(123)
reg_data <- nhanes[!is.na(Height) & !is.na(Weight) & Age >= 18]
sample_data <- reg_data[sample(.N, 500)]

# Fit the model
model <- lm(Weight ~ Height, data = sample_data)
```

---

## 12.12 Residual Analysis

### 12.12.1 Types of Residuals

Three types of residuals are commonly used:

1. **Raw residuals**: $e_i = Y_i - \hat{Y}_i$
2. **Standardised residuals**: $r_i = e_i / \hat{\sigma}$
3. **Studentised residuals**: $t_i = e_i / (s_{-i}\sqrt{1 - h_{ii}})$

Where $h_{ii}$ is the leverage (diagonal of the hat matrix) and $s_{-i}$ is the standard error computed without observation $i$.


``` r
# Extract different residual types
sample_data[, `:=`(
    raw_resid = residuals(model),
    std_resid = rstandard(model),      # Standardised
    stud_resid = rstudent(model),      # Studentised (external)
    fitted = fitted(model),
    leverage = hatvalues(model)
)]

cat("Comparison of Residual Types\n")
```

```
## Comparison of Residual Types
```

``` r
cat("============================\n\n")
```

```
## ============================
```

``` r
print(sample_data[1:10, .(Height, Weight, fitted, raw_resid, std_resid, stud_resid)])
```

```
##     Height Weight   fitted  raw_resid  std_resid stud_resid
##      <num>  <num>    <num>      <num>      <num>      <num>
##  1:  169.3   94.2 81.61708  12.582917  0.7028091  0.7024516
##  2:  171.1   78.6 83.27751  -4.677515 -0.2612716 -0.2610271
##  3:  174.0   89.6 85.95266   3.647345  0.2037727  0.2035765
##  4:  164.3   89.2 77.00477  12.195228  0.6812914  0.6809244
##  5:  155.7   75.1 69.07160   6.028403  0.3372837  0.3369834
##  6:  164.5   64.5 77.18926 -12.689264 -0.7088785 -0.7085240
##  7:  171.9  101.9 84.01548  17.884515  0.9990160  0.9990141
##  8:  157.6  115.8 70.82428  44.975725  2.5152023  2.5287891
##  9:  168.1   70.4 80.51013 -10.110128 -0.5646952 -0.5643087
## 10:  179.1   74.5 90.65721 -16.157213 -0.9033833 -0.9032163
```

### 12.12.2 Residual Plots

The four standard diagnostic plots assess different assumptions:


``` r
# Create diagnostic data
diag_data <- data.table(
    fitted = fitted(model),
    residuals = residuals(model),
    std_resid = rstandard(model),
    leverage = hatvalues(model),
    cooks_d = cooks.distance(model)
)

# 1. Residuals vs Fitted (linearity, homoscedasticity)
p1 <- ggplot2$ggplot(diag_data, ggplot2$aes(x = fitted, y = residuals)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed", colour = "#D55E00") +
    ggplot2$geom_smooth(method = "loess", se = FALSE, colour = "#009E73") +
    ggplot2$labs(
        title = "1. Residuals vs Fitted",
        subtitle = "Check: Linearity and homoscedasticity",
        x = "Fitted Values",
        y = "Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 11)

# 2. Q-Q Plot (normality)
p2 <- ggplot2$ggplot(diag_data, ggplot2$aes(sample = std_resid)) +
    ggplot2$stat_qq(colour = "#0072B2", alpha = 0.5) +
    ggplot2$stat_qq_line(colour = "#D55E00", linewidth = 1) +
    ggplot2$labs(
        title = "2. Normal Q-Q Plot",
        subtitle = "Check: Normality of residuals",
        x = "Theoretical Quantiles",
        y = "Standardised Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 11)

# 3. Scale-Location (homoscedasticity)
p3 <- ggplot2$ggplot(diag_data, ggplot2$aes(x = fitted, y = sqrt(abs(std_resid)))) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_smooth(method = "loess", se = FALSE, colour = "#009E73") +
    ggplot2$labs(
        title = "3. Scale-Location",
        subtitle = "Check: Homoscedasticity (equal variance)",
        x = "Fitted Values",
        y = expression(sqrt("|Standardised Residuals|"))
    ) +
    ggplot2$theme_minimal(base_size = 11)

# 4. Residuals vs Leverage (influential points)
p4 <- ggplot2$ggplot(diag_data, ggplot2$aes(x = leverage, y = std_resid)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed", colour = "#D55E00") +
    ggplot2$geom_vline(xintercept = 2 * 2 / nrow(sample_data), linetype = "dashed", colour = "#009E73") +
    ggplot2$labs(
        title = "4. Residuals vs Leverage",
        subtitle = "Check: Influential observations",
        x = "Leverage",
        y = "Standardised Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 11)

print(p1)
```

![plot of chunk diagnostic_plots](/courses/statistics-1-foundations/diagnostic_plots-1.png)

``` r
print(p2)
```

![plot of chunk diagnostic_plots](/courses/statistics-1-foundations/diagnostic_plots-2.png)

``` r
print(p3)
```

![plot of chunk diagnostic_plots](/courses/statistics-1-foundations/diagnostic_plots-3.png)

``` r
print(p4)
```

![plot of chunk diagnostic_plots](/courses/statistics-1-foundations/diagnostic_plots-4.png)

---

## 12.13 Checking Linearity

### 12.13.1 What to Look For

In the residuals vs fitted plot, look for:
- **Random scatter**: Good—linearity assumption met
- **Curved pattern**: Bad—relationship may be non-linear


``` r
# Create examples of linear and non-linear relationships
set.seed(456)
n <- 200
x <- runif(n, 0, 10)

# Linear relationship
y_linear <- 2 + 3 * x + rnorm(n, 0, 3)
# Quadratic relationship
y_quadratic <- 2 + 3 * x - 0.3 * x^2 + rnorm(n, 0, 2)

examples <- data.table(
    x = rep(x, 2),
    y = c(y_linear, y_quadratic),
    type = rep(c("Linear (Assumption Met)", "Non-linear (Violation)"), each = n)
)

# Fit linear models to both
model_lin <- lm(y_linear ~ x)
model_quad <- lm(y_quadratic ~ x)

resid_examples <- data.table(
    fitted = c(fitted(model_lin), fitted(model_quad)),
    residuals = c(residuals(model_lin), residuals(model_quad)),
    type = rep(c("Linear (Assumption Met)", "Non-linear (Violation)"), each = n)
)

ggplot2$ggplot(resid_examples, ggplot2$aes(x = fitted, y = residuals)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed", colour = "#D55E00") +
    ggplot2$geom_smooth(method = "loess", se = FALSE, colour = "#009E73") +
    ggplot2$facet_wrap(~type) +
    ggplot2$labs(
        title = "Detecting Non-Linearity in Residual Plots",
        subtitle = "Left: random scatter (good); Right: curved pattern (violation)",
        x = "Fitted Values",
        y = "Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk linearity_examples](/courses/statistics-1-foundations/linearity_examples-1.png)

### 12.13.2 Solutions for Non-Linearity

1. **Transform predictor**: Use $\log(X)$, $\sqrt{X}$, or $X^2$
2. **Transform response**: Use $\log(Y)$ or $\sqrt{Y}$
3. **Add polynomial terms**: Include $X^2$, $X^3$, etc.
4. **Use non-linear models**: Generalised additive models (GAMs)

---

## 12.14 Checking Homoscedasticity

### 12.14.1 Constant Variance Assumption

Homoscedasticity means the variance of residuals is constant across all levels of the predictor.

**Heteroscedasticity** (non-constant variance) appears as:
- Fan or funnel shapes in residual plots
- Increasing or decreasing spread as fitted values increase


``` r
# Create examples
set.seed(789)
x <- runif(n, 1, 10)

# Homoscedastic
y_homo <- 2 + 3 * x + rnorm(n, 0, 3)
# Heteroscedastic (variance increases with x)
y_hetero <- 2 + 3 * x + rnorm(n, 0, 0.5 * x)

# Fit models
model_homo <- lm(y_homo ~ x)
model_hetero <- lm(y_hetero ~ x)

var_examples <- data.table(
    fitted = c(fitted(model_homo), fitted(model_hetero)),
    residuals = c(residuals(model_homo), residuals(model_hetero)),
    type = rep(c("Homoscedastic (Good)", "Heteroscedastic (Violation)"), each = n)
)

ggplot2$ggplot(var_examples, ggplot2$aes(x = fitted, y = residuals)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = 0, linetype = "dashed", colour = "#D55E00") +
    ggplot2$facet_wrap(~type) +
    ggplot2$labs(
        title = "Detecting Heteroscedasticity",
        subtitle = "Left: constant spread (good); Right: increasing spread (violation)",
        x = "Fitted Values",
        y = "Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk heteroscedasticity_examples](/courses/statistics-1-foundations/heteroscedasticity_examples-1.png)

### 12.14.2 Formal Tests for Heteroscedasticity

**Breusch-Pagan Test** tests $H_0$: Homoscedasticity vs $H_1$: Heteroscedasticity


``` r
# Manual Breusch-Pagan test
resid_sq <- residuals(model_hetero)^2
bp_model <- lm(resid_sq ~ x)
bp_r2 <- summary(bp_model)$r.squared
bp_stat <- n * bp_r2
bp_pvalue <- pchisq(bp_stat, df = 1, lower.tail = FALSE)

cat("Breusch-Pagan Test for Heteroscedasticity\n")
```

```
## Breusch-Pagan Test for Heteroscedasticity
```

``` r
cat("=========================================\n\n")
```

```
## =========================================
```

``` r
cat("H₀: Constant variance (homoscedasticity)\n")
```

```
## H<U+2080>: Constant variance (homoscedasticity)
```

``` r
cat("H₁: Variance depends on fitted values\n\n")
```

```
## H<U+2081>: Variance depends on fitted values
```

``` r
cat(sprintf("Test statistic: BP = n × R² = %d × %.4f = %.3f\n", n, bp_r2, bp_stat))
```

```
## Test statistic: BP = n <U+00D7> R<U+00B2> = 200 <U+00D7> 0.1502 = 30.048
```

``` r
cat(sprintf("p-value: %.4f\n\n", bp_pvalue))
```

```
## p-value: 0.0000
```

``` r
if (bp_pvalue < 0.05) {
    cat("Conclusion: Reject H₀. Evidence of heteroscedasticity.\n")
} else {
    cat("Conclusion: Fail to reject H₀. No evidence of heteroscedasticity.\n")
}
```

```
## Conclusion: Reject H<U+2080>. Evidence of heteroscedasticity.
```

### 12.14.3 Consequences and Solutions

**Consequences of heteroscedasticity:**
- OLS estimates remain unbiased
- Standard errors are biased (usually underestimated)
- t-tests and confidence intervals are invalid

**Solutions:**
1. **Transform response**: Often $\log(Y)$ stabilises variance
2. **Weighted least squares**: Down-weight high-variance observations
3. **Robust standard errors**: Use heteroscedasticity-consistent (HC) standard errors

---

## 12.15 Checking Normality

### 12.15.1 Q-Q Plots

The Q-Q plot compares the distribution of residuals to a theoretical normal distribution.


``` r
# Create examples
set.seed(101)

# Normal residuals
y_normal <- 2 + 3 * x + rnorm(n, 0, 3)
# Heavy-tailed residuals (t-distribution)
y_heavy <- 2 + 3 * x + rt(n, df = 3) * 2
# Skewed residuals
y_skew <- 2 + 3 * x + rexp(n, 0.5) - 2

models <- list(
    Normal = lm(y_normal ~ x),
    HeavyTails = lm(y_heavy ~ x),
    Skewed = lm(y_skew ~ x)
)

qq_data <- rbindlist(lapply(names(models), function(nm) {
    data.table(
        std_resid = rstandard(models[[nm]]),
        type = nm
    )
}))

qq_data[, type := factor(type, levels = c("Normal", "HeavyTails", "Skewed"))]

ggplot2$ggplot(qq_data, ggplot2$aes(sample = std_resid)) +
    ggplot2$stat_qq(colour = "#0072B2", alpha = 0.5) +
    ggplot2$stat_qq_line(colour = "#D55E00", linewidth = 1) +
    ggplot2$facet_wrap(~type) +
    ggplot2$labs(
        title = "Q-Q Plot Patterns",
        subtitle = "Normal: straight line; Heavy tails: S-curve; Skewed: curved",
        x = "Theoretical Quantiles",
        y = "Sample Quantiles"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk normality_examples](/courses/statistics-1-foundations/normality_examples-1.png)

### 12.15.2 Formal Normality Tests

**Shapiro-Wilk Test** is recommended for moderate sample sizes (n < 5000).


``` r
# Test residuals from our height-weight model
sw_test <- shapiro.test(residuals(model))

cat("Shapiro-Wilk Test for Normality of Residuals\n")
```

```
## Shapiro-Wilk Test for Normality of Residuals
```

``` r
cat("=============================================\n\n")
```

```
## =============================================
```

``` r
cat("H₀: Residuals are normally distributed\n")
```

```
## H<U+2080>: Residuals are normally distributed
```

``` r
cat("H₁: Residuals are not normally distributed\n\n")
```

```
## H<U+2081>: Residuals are not normally distributed
```

``` r
cat(sprintf("W = %.4f\n", sw_test$statistic))
```

```
## W = 0.9442
```

``` r
cat(sprintf("p-value = %.4f\n\n", sw_test$p.value))
```

```
## p-value = 0.0000
```

``` r
if (sw_test$p.value < 0.05) {
    cat("Conclusion: Evidence against normality.\n")
    cat("          However, regression is robust to moderate departures\n")
    cat("          from normality with large samples (CLT applies).\n")
} else {
    cat("Conclusion: No evidence against normality.\n")
}
```

```
## Conclusion: Evidence against normality.
##           However, regression is robust to moderate departures
##           from normality with large samples (CLT applies).
```

### 12.15.3 When Does Normality Matter?

**Normality matters for:**
- Small sample inference (t-tests, F-tests, confidence intervals)
- Prediction intervals

**Normality matters less when:**
- Sample size is large (n > 30-50): CLT ensures approximately normal sampling distributions
- Only using the model for estimation (not prediction intervals)

---

## 12.16 Influential Observations

### 12.16.1 Leverage

**Leverage** measures how unusual an observation's predictor value is. High-leverage points have the potential to influence the regression line.

$$h_{ii} = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{j=1}^n (X_j - \bar{X})^2}$$

Rule of thumb: Points with $h_{ii} > 2p/n$ deserve attention (where $p$ is the number of parameters).


``` r
# Calculate leverage
sample_data[, leverage := hatvalues(model)]

# Threshold
p <- 2  # intercept + slope
leverage_threshold <- 2 * p / nrow(sample_data)

# Count high leverage points
high_leverage <- sample_data[leverage > leverage_threshold]

cat("Leverage Analysis\n")
```

```
## Leverage Analysis
```

``` r
cat("=================\n\n")
```

```
## =================
```

``` r
cat(sprintf("Mean leverage: %.4f\n", mean(sample_data$leverage)))
```

```
## Mean leverage: 0.0040
```

``` r
cat(sprintf("Threshold (2p/n): %.4f\n", leverage_threshold))
```

```
## Threshold (2p/n): 0.0080
```

``` r
cat(sprintf("Points with high leverage: %d (%.1f%%)\n\n",
            nrow(high_leverage), 100 * nrow(high_leverage) / nrow(sample_data)))
```

```
## Points with high leverage: 46 (9.2%)
```

``` r
# Visualise
ggplot2$ggplot(sample_data, ggplot2$aes(x = Height, y = leverage)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = leverage_threshold, linetype = "dashed", colour = "#D55E00") +
    ggplot2$labs(
        title = "Leverage Values",
        subtitle = "Points far from the mean of X have higher leverage",
        x = "Height (cm)",
        y = "Leverage (hᵢᵢ)"
    ) +
    ggplot2$annotate("text", x = max(sample_data$Height) - 5, y = leverage_threshold + 0.002,
                     label = "Threshold = 2p/n", colour = "#D55E00") +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk leverage](/courses/statistics-1-foundations/leverage-1.png)

### 12.16.2 Cook's Distance

**Cook's Distance** combines leverage and residual size to measure overall influence on the fitted values:

$$D_i = \frac{e_i^2}{p \cdot \text{MSE}} \times \frac{h_{ii}}{(1 - h_{ii})^2}$$

Common thresholds: $D_i > 1$ or $D_i > 4/n$


``` r
# Calculate Cook's distance
sample_data[, cooks_d := cooks.distance(model)]

# Thresholds
threshold_1 <- 1
threshold_4n <- 4 / nrow(sample_data)

# Identify influential points
influential <- sample_data[cooks_d > threshold_4n]

cat("Cook's Distance Analysis\n")
```

```
## Cook's Distance Analysis
```

``` r
cat("========================\n\n")
```

```
## ========================
```

``` r
cat(sprintf("Threshold 4/n: %.4f\n", threshold_4n))
```

```
## Threshold 4/n: 0.0080
```

``` r
cat(sprintf("Observations exceeding threshold: %d\n\n", nrow(influential)))
```

```
## Observations exceeding threshold: 21
```

``` r
# Show most influential points
cat("Top 10 Most Influential Points:\n")
```

```
## Top 10 Most Influential Points:
```

``` r
print(sample_data[order(-cooks_d)][1:10, .(Height, Weight, leverage, std_resid, cooks_d)])
```

```
##     Height Weight    leverage std_resid    cooks_d
##      <num>  <num>       <num>     <num>      <num>
##  1:  186.8  203.0 0.008334794  5.896822 0.14612880
##  2:  188.6  159.1 0.009667754  3.346219 0.05465419
##  3:  152.0  125.6 0.007561915  3.357348 0.04294288
##  4:  152.0  125.6 0.007561915  3.357348 0.04294288
##  5:  146.3   99.3 0.011966064  2.183630 0.02887404
##  6:  184.9  146.5 0.007065754  2.827399 0.02844345
##  7:  139.9   79.8 0.018430938  1.425073 0.01906650
##  8:  157.6  115.8 0.004476987  2.515202 0.01422494
##  9:  160.1  120.4 0.003497220  2.642182 0.01225011
## 10:  186.8   67.9 0.008334794 -1.673132 0.01176414
```


``` r
# Create index for plotting
sample_data[, obs := .I]

ggplot2$ggplot(sample_data, ggplot2$aes(x = obs, y = cooks_d)) +
    ggplot2$geom_segment(ggplot2$aes(xend = obs, yend = 0), colour = "#0072B2", alpha = 0.5) +
    ggplot2$geom_point(colour = "#0072B2") +
    ggplot2$geom_hline(yintercept = threshold_4n, linetype = "dashed", colour = "#D55E00") +
    ggplot2$labs(
        title = "Cook's Distance",
        subtitle = "Identifying influential observations",
        x = "Observation Index",
        y = "Cook's Distance"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk cooks_plot](/courses/statistics-1-foundations/cooks_plot-1.png)

### 12.16.3 Influence Plot

An influence plot combines residuals, leverage, and Cook's distance:


``` r
# Create influence plot
sample_data[, influential := cooks_d > threshold_4n]

ggplot2$ggplot(sample_data, ggplot2$aes(x = leverage, y = std_resid)) +
    ggplot2$geom_point(ggplot2$aes(size = cooks_d, colour = influential), alpha = 0.6) +
    ggplot2$scale_size_continuous(range = c(1, 8), name = "Cook's D") +
    ggplot2$scale_colour_manual(values = c("#0072B2", "#D55E00"), name = "Influential") +
    ggplot2$geom_hline(yintercept = c(-2, 0, 2), linetype = c("dashed", "solid", "dashed"),
                       colour = c("#E69F00", "black", "#E69F00")) +
    ggplot2$geom_vline(xintercept = leverage_threshold, linetype = "dashed", colour = "#009E73") +
    ggplot2$labs(
        title = "Influence Plot",
        subtitle = "Size shows Cook's D; Colour shows influential (> 4/n)",
        x = "Leverage",
        y = "Standardised Residuals"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk influence_plot](/courses/statistics-1-foundations/influence_plot-1.png)

### 12.16.4 What to Do with Influential Points

1. **Investigate**: Are they data entry errors? Special cases?
2. **Run sensitivity analysis**: Fit with and without the point
3. **Report both analyses**: Show robustness (or lack thereof)
4. **Don't automatically remove**: May contain valuable information


``` r
# Compare model with and without most influential point
most_influential <- which.max(sample_data$cooks_d)

model_all <- lm(Weight ~ Height, data = sample_data)
model_without <- lm(Weight ~ Height, data = sample_data[-most_influential])

cat("Sensitivity Analysis: Most Influential Observation\n")
```

```
## Sensitivity Analysis: Most Influential Observation
```

``` r
cat("===================================================\n\n")
```

```
## ===================================================
```

``` r
cat("Observation details:\n")
```

```
## Observation details:
```

``` r
print(sample_data[most_influential, .(Height, Weight, std_resid, leverage, cooks_d)])
```

```
##    Height Weight std_resid    leverage   cooks_d
##     <num>  <num>     <num>       <num>     <num>
## 1:  186.8    203  5.896822 0.008334794 0.1461288
```

``` r
cat("\n\nCoefficient Comparison:\n")
```

```
## 
## 
## Coefficient Comparison:
```

``` r
coef_compare <- data.table(
    Parameter = c("Intercept", "Slope"),
    With_All = coef(model_all),
    Without = coef(model_without),
    Change_Pct = 100 * (coef(model_without) - coef(model_all)) / coef(model_all)
)
print(coef_compare)
```

```
##    Parameter    With_All     Without Change_Pct
##       <char>       <num>       <num>      <num>
## 1: Intercept -74.5557691 -68.4502823  -8.189154
## 2:     Slope   0.9224622   0.8850424  -4.056509
```

``` r
cat("\nR² Comparison:\n")
```

```
## 
## R<U+00B2> Comparison:
```

``` r
cat(sprintf("With all data: %.4f\n", summary(model_all)$r.squared))
```

```
## With all data: 0.2133
```

``` r
cat(sprintf("Without influential point: %.4f\n", summary(model_without)$r.squared))
```

```
## Without influential point: 0.2105
```

---

## 12.17 DFBETAS and DFFITS

### 12.17.1 DFBETAS

DFBETAS measures how much each coefficient changes when observation $i$ is removed:

$$\text{DFBETAS}_{j,i} = \frac{\hat{\beta}_j - \hat{\beta}_{j(-i)}}{\text{SE}(\hat{\beta}_j)_{(-i)}}$$


``` r
# Calculate DFBETAS
dfb <- dfbetas(model)
sample_data[, dfbeta_intercept := dfb[, 1]]
sample_data[, dfbeta_slope := dfb[, 2]]

# Threshold
dfbetas_threshold <- 2 / sqrt(nrow(sample_data))

cat("DFBETAS Analysis\n")
```

```
## DFBETAS Analysis
```

``` r
cat("================\n\n")
```

```
## ================
```

``` r
cat(sprintf("Threshold: 2/√n = %.4f\n\n", dfbetas_threshold))
```

```
## Threshold: 2/<U+221A>n = 0.0894
```

``` r
cat("Observations with large DFBETAS for slope:\n")
```

```
## Observations with large DFBETAS for slope:
```

``` r
large_dfbeta <- sample_data[abs(dfbeta_slope) > dfbetas_threshold]
print(large_dfbeta[order(-abs(dfbeta_slope))][1:5, .(Height, Weight, dfbeta_slope)])
```

```
##    Height Weight dfbeta_slope
##     <num>  <num>        <num>
## 1:  186.8  203.0    0.4881831
## 2:  188.6  159.1    0.2975087
## 3:  152.0  125.6   -0.2539755
## 4:  152.0  125.6   -0.2539755
## 5:  146.3   99.3   -0.2201444
```

### 12.17.2 DFFITS

DFFITS measures the scaled difference in fitted values when observation $i$ is removed:

$$\text{DFFITS}_i = \frac{\hat{Y}_i - \hat{Y}_{i(-i)}}{\hat{\sigma}_{(-i)}\sqrt{h_{ii}}}$$


``` r
# Calculate DFFITS
sample_data[, dffits := dffits(model)]

# Threshold
dffits_threshold <- 2 * sqrt(p / nrow(sample_data))

cat("DFFITS Analysis\n")
```

```
## DFFITS Analysis
```

``` r
cat("===============\n\n")
```

```
## ===============
```

``` r
cat(sprintf("Threshold: 2√(p/n) = %.4f\n\n", dffits_threshold))
```

```
## Threshold: 2<U+221A>(p/n) = 0.1265
```

``` r
cat("Observations with large DFFITS:\n")
```

```
## Observations with large DFFITS:
```

``` r
large_dffits <- sample_data[abs(dffits) > dffits_threshold]
print(large_dffits[order(-abs(dffits))][1:5, .(Height, Weight, dffits, cooks_d)])
```

```
##    Height Weight    dffits    cooks_d
##     <num>  <num>     <num>      <num>
## 1:  186.8  203.0 0.5599689 0.14612880
## 2:  188.6  159.1 0.3340630 0.05465419
## 3:  152.0  125.6 0.2961389 0.04294288
## 4:  152.0  125.6 0.2961389 0.04294288
## 5:  146.3   99.3 0.2412245 0.02887404
```

---

## 12.18 Transformations

### 12.18.1 Log Transformation

The log transformation is commonly used when:
- Response is strictly positive
- Variance increases with the mean
- Relationship is multiplicative


``` r
# Example with income data (simulated)
set.seed(202)
years_exp <- runif(100, 0, 30)
income <- exp(10 + 0.05 * years_exp + rnorm(100, 0, 0.5))  # Log-linear relationship

income_data <- data.table(years = years_exp, income = income)

# Compare models
model_raw <- lm(income ~ years, data = income_data)
model_log <- lm(log(income) ~ years, data = income_data)

# Plot comparison
p1 <- ggplot2$ggplot(income_data, ggplot2$aes(x = years, y = income)) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_smooth(method = "lm", colour = "#D55E00") +
    ggplot2$labs(title = "Raw Scale", x = "Years Experience", y = "Income") +
    ggplot2$theme_minimal()

p2 <- ggplot2$ggplot(income_data, ggplot2$aes(x = years, y = log(income))) +
    ggplot2$geom_point(alpha = 0.5, colour = "#0072B2") +
    ggplot2$geom_smooth(method = "lm", colour = "#D55E00") +
    ggplot2$labs(title = "Log Scale", x = "Years Experience", y = "log(Income)") +
    ggplot2$theme_minimal()

print(p1)
```

![plot of chunk log_transform](/courses/statistics-1-foundations/log_transform-1.png)

``` r
print(p2)
```

![plot of chunk log_transform](/courses/statistics-1-foundations/log_transform-2.png)

``` r
cat("\nModel Comparison\n")
```

```
## 
## Model Comparison
```

``` r
cat("================\n\n")
```

```
## ================
```

``` r
cat(sprintf("Raw model R²: %.4f\n", summary(model_raw)$r.squared))
```

```
## Raw model R<U+00B2>: 0.3650
```

``` r
cat(sprintf("Log model R²: %.4f\n", summary(model_log)$r.squared))
```

```
## Log model R<U+00B2>: 0.3913
```

### 12.18.2 Interpreting Log-Transformed Models

For $\log(Y) = \beta_0 + \beta_1 X$:

- A one-unit increase in $X$ is associated with approximately a $100 \times \beta_1$% change in $Y$ (for small $\beta_1$)
- More precisely: $Y$ is multiplied by $e^{\beta_1}$


``` r
cat("Interpreting the Log Model\n")
```

```
## Interpreting the Log Model
```

``` r
cat("==========================\n\n")
```

```
## ==========================
```

``` r
cat(sprintf("Slope: %.4f\n", coef(model_log)[2]))
```

```
## Slope: 0.0450
```

``` r
cat(sprintf("\nInterpretation:\n"))
```

```
## 
## Interpretation:
```

``` r
cat(sprintf("Each additional year of experience is associated with\n"))
```

```
## Each additional year of experience is associated with
```

``` r
cat(sprintf("approximately a %.1f%% increase in income.\n", 100 * coef(model_log)[2]))
```

```
## approximately a 4.5% increase in income.
```

``` r
cat(sprintf("\nExact multiplier: e^%.4f = %.4f\n",
            coef(model_log)[2], exp(coef(model_log)[2])))
```

```
## 
## Exact multiplier: e^0.0450 = 1.0460
```

### 12.18.3 Box-Cox Transformation

The Box-Cox transformation finds the optimal power transformation:

$$Y^{(\lambda)} = \begin{cases} \frac{Y^\lambda - 1}{\lambda} & \lambda \neq 0 \\ \log(Y) & \lambda = 0 \end{cases}$$


``` r
# Box-Cox for our height-weight model
# Note: Response must be positive
positive_data <- sample_data[Weight > 0]

# Calculate log-likelihood for different lambda values
lambda_seq <- seq(-2, 2, by = 0.1)

boxcox_ll <- sapply(lambda_seq, function(lam) {
    if (abs(lam) < 0.001) {
        y_trans <- log(positive_data$Weight)
    } else {
        y_trans <- (positive_data$Weight^lam - 1) / lam
    }
    mod <- lm(y_trans ~ Height, data = positive_data)
    # Jacobian adjustment
    n <- nrow(positive_data)
    ll <- -n/2 * log(sum(residuals(mod)^2)/n) + (lam - 1) * sum(log(positive_data$Weight))
    return(ll)
})

bc_data <- data.table(lambda = lambda_seq, loglik = boxcox_ll)
best_lambda <- lambda_seq[which.max(boxcox_ll)]

ggplot2$ggplot(bc_data, ggplot2$aes(x = lambda, y = loglik)) +
    ggplot2$geom_line(colour = "#0072B2", linewidth = 1) +
    ggplot2$geom_vline(xintercept = best_lambda, linetype = "dashed", colour = "#D55E00") +
    ggplot2$geom_vline(xintercept = 1, linetype = "dotted", colour = "#009E73") +
    ggplot2$labs(
        title = "Box-Cox Transformation",
        subtitle = sprintf("Optimal λ = %.1f; λ = 1 means no transformation needed", best_lambda),
        x = "λ",
        y = "Log-Likelihood"
    ) +
    ggplot2$theme_minimal(base_size = 12)
```

![plot of chunk boxcox](/courses/statistics-1-foundations/boxcox-1.png)

---

## 12.19 Communicating Diagnostic Findings

### 12.19.1 Reporting Diagnostics

When reporting regression diagnostics:

1. **State assumptions checked**: Linearity, homoscedasticity, normality, influential points
2. **Describe diagnostic approach**: Which plots and tests were used
3. **Report findings**: Any violations detected
4. **Describe remedies applied**: Transformations, robust methods, sensitivity analyses
5. **Acknowledge limitations**: What couldn't be fixed


``` r
cat("Example Diagnostic Report\n")
```

```
## Example Diagnostic Report
```

``` r
cat("=========================\n\n")
```

```
## =========================
```

``` r
cat("Model Diagnostics:\n\n")
```

```
## Model Diagnostics:
```

``` r
cat("1. Linearity: The residuals vs fitted plot showed no systematic pattern,\n")
```

```
## 1. Linearity: The residuals vs fitted plot showed no systematic pattern,
```

``` r
cat("   suggesting the linear assumption is reasonable.\n\n")
```

```
##    suggesting the linear assumption is reasonable.
```

``` r
cat("2. Homoscedasticity: The scale-location plot showed approximately constant\n")
```

```
## 2. Homoscedasticity: The scale-location plot showed approximately constant
```

``` r
cat("   spread across fitted values. No formal test was significant.\n\n")
```

```
##    spread across fitted values. No formal test was significant.
```

``` r
# Actually run Shapiro-Wilk
sw <- shapiro.test(residuals(model))
cat(sprintf("3. Normality: The Q-Q plot showed minor deviations in the tails.\n"))
```

```
## 3. Normality: The Q-Q plot showed minor deviations in the tails.
```

``` r
cat(sprintf("   Shapiro-Wilk test: W = %.3f, p = %.3f. With n = %d,\n",
            sw$statistic, sw$p.value, nrow(sample_data)))
```

```
##    Shapiro-Wilk test: W = 0.944, p = 0.000. With n = 500,
```

``` r
cat("   inference remains valid by CLT.\n\n")
```

```
##    inference remains valid by CLT.
```

``` r
cat(sprintf("4. Influential observations: %d observations had Cook's D > 4/n.\n",
            sum(sample_data$cooks_d > 4/nrow(sample_data))))
```

```
## 4. Influential observations: 21 observations had Cook's D > 4/n.
```

``` r
cat("   Sensitivity analysis showed coefficients stable when these\n")
```

```
##    Sensitivity analysis showed coefficients stable when these
```

``` r
cat("   observations were excluded (slope changed by < 5%).\n")
```

```
##    observations were excluded (slope changed by < 5%).
```

---

## Quick Reference

### Diagnostic Checks

| Assumption | Plot | Test | Remedy |
|------------|------|------|--------|
| Linearity | Residuals vs Fitted | - | Transform X or Y, add polynomial |
| Homoscedasticity | Scale-Location | Breusch-Pagan | Transform Y, WLS, robust SE |
| Normality | Q-Q Plot | Shapiro-Wilk | Transform Y (large n: less critical) |
| Independence | Residuals vs Order | Durbin-Watson | Model time structure |
| Influential points | Influence plot | Cook's D, DFBETAS | Investigate, sensitivity analysis |

### Thresholds

| Measure | Threshold | Meaning |
|---------|-----------|---------|
| Leverage | $> 2p/n$ | Unusual predictor value |
| Cook's D | $> 4/n$ or $> 1$ | Influential on all fitted values |
| DFBETAS | $> 2/\sqrt{n}$ | Influential on specific coefficient |
| DFFITS | $> 2\sqrt{p/n}$ | Influential on specific fitted value |
| Standardised residual | $> 2$ or $< -2$ | Potential outlier |

### R Functions

| Function | Purpose |
|----------|---------|
| `rstandard(model)` | Standardised residuals |
| `rstudent(model)` | Studentised residuals |
| `hatvalues(model)` | Leverage values |
| `cooks.distance(model)` | Cook's distance |
| `dfbetas(model)` | DFBETAS |
| `dffits(model)` | DFFITS |
| `plot(model)` | Four diagnostic plots |

