---
title: "Statistics with R I: Foundations"
chapter: "Chapter 2: Descriptive Statistics — Summarising Data Numerically"
part: "Part 2: Measures of Dispersion and Position"
coverImage: 13
author: "Dereck Mezquita"
date: "`r Sys.Date()`"
tags: [statistics, mathematics, descriptive, data, R, biomedical]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# https://bookdown.org/yihui/rmarkdown-cookbook/hook-html5.html
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(dpi = 300, fig.width = 10, fig.height = 7, comment = "#>", warning = FALSE, collapse = TRUE)
```

```{r packages, message=FALSE, warning=FALSE}
box::use(
    data.table,
    ggplot2
)
```

```{r load_data, message=FALSE}
# Load NHANES data for examples
nhanes <- data.table$fread("../../../data/primary/nhanes.csv")
bmi_clean <- nhanes[!is.na(BMI), BMI]
bp_data <- nhanes[!is.na(BPSysAve), BPSysAve]
```

## 2.2 Measures of Dispersion (Spread)

Central tendency tells us where the data are located; dispersion tells us how spread out they are. Two datasets can have identical means yet vastly different spreads.

### 2.2.1 Range and Interquartile Range

**Range**

The simplest measure of spread is the range: the difference between maximum and minimum values.

$$\text{Range} = x_{\text{max}} - x_{\text{min}}$$

```{r range_from_scratch}
# Implement range from scratch
my_range <- function(x) {
    x <- x[!is.na(x)]
    return(max(x) - min(x))
}

# Example with NHANES blood pressure
cat("Range of systolic BP:", my_range(bp_data), "mmHg\n")
cat("Min:", min(bp_data), " Max:", max(bp_data), "\n")
```

**Limitations:** The range uses only two values and is extremely sensitive to outliers. A single extreme observation can dramatically inflate the range.

**Interquartile Range (IQR)**

The IQR is the range of the middle 50% of the data:

$$\text{IQR} = Q_3 - Q_1$$

where $Q_1$ is the first quartile (25th percentile) and $Q_3$ is the third quartile (75th percentile).

```{r iqr_from_scratch, fig.cap="The IQR captures the spread of the middle 50% of data"}
# Implement quartiles and IQR from scratch
my_quantile <- function(x, p) {
    # Simple linear interpolation method
    x <- sort(x[!is.na(x)])
    n <- length(x)

    # Position of the quantile
    h <- (n - 1) * p + 1

    # Lower and upper indices
    lo <- floor(h)
    hi <- ceiling(h)

    # Linear interpolation
    return(x[lo] + (h - lo) * (x[hi] - x[lo]))
}

my_iqr <- function(x) {
    q1 <- my_quantile(x, 0.25)
    q3 <- my_quantile(x, 0.75)
    return(q3 - q1)
}

# Test
cat("Q1:", my_quantile(bp_data, 0.25), "\n")
cat("Q3:", my_quantile(bp_data, 0.75), "\n")
cat("Our IQR:", my_iqr(bp_data), "\n")
cat("Built-in IQR():", IQR(bp_data), "\n")

# Visualise
bp_dt <- data.table$data.table(bp = bp_data)
q1_val <- quantile(bp_data, 0.25)
q3_val <- quantile(bp_data, 0.75)

ggplot2$ggplot(bp_dt, ggplot2$aes(x = bp)) +
    ggplot2$geom_histogram(bins = 40, fill = "#56B4E9", colour = "white", alpha = 0.8) +
    ggplot2$geom_vline(xintercept = c(q1_val, q3_val),
               colour = "red", linetype = "dashed", size = 1) +
    ggplot2$annotate("rect", xmin = q1_val, xmax = q3_val,
             ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
    ggplot2$annotate("text", x = (q1_val + q3_val) / 2, y = 800,
             label = paste("IQR =", round(q3_val - q1_val, 1)),
             colour = "red", size = 5) +
    ggplot2$labs(
        title = "Interquartile Range: Middle 50% of Data",
        subtitle = "Systolic blood pressure from NHANES",
        x = "Systolic Blood Pressure (mmHg)",
        y = "Count"
    ) +
    ggplot2$theme_minimal()
```

### 2.2.2 Variance

Variance measures the average squared deviation from the mean. Why squared? Squaring ensures all deviations are positive and gives more weight to larger deviations.

**Population Variance**

$$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$$

**Sample Variance**

$$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

**Why divide by n-1?**

This is one of the most common questions in statistics. The intuition: when we estimate the population variance from a sample, we use $\bar{x}$ instead of $\mu$. Since $\bar{x}$ is calculated from the same data, the deviations from $\bar{x}$ are systematically smaller than deviations from $\mu$. Dividing by $n-1$ instead of $n$ corrects this underestimation.

The formal proof involves showing that $E[s^2] = \sigma^2$ when we divide by $n-1$ (making $s^2$ an unbiased estimator), whilst dividing by $n$ gives $E[\hat{\sigma}^2] = \frac{n-1}{n}\sigma^2$.

```{r variance_derivation}
# Implement variance from scratch
# Population variance (divide by N)
my_pop_variance <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)

    # Sum of squared deviations
    ss <- sum((x - mean_x)^2)

    return(ss / n)
}

# Sample variance (divide by n-1)
my_sample_variance <- function(x) {
    x <- x[!is.na(x)]
    n <- length(x)
    mean_x <- mean(x)

    # Sum of squared deviations
    ss <- sum((x - mean_x)^2)

    return(ss / (n - 1))
}

# Test
cat("Population variance:", my_pop_variance(bmi_clean), "\n")
cat("Sample variance:", my_sample_variance(bmi_clean), "\n")
cat("Built-in var():", var(bmi_clean), "\n")
```

**Demonstrating n-1 correction through simulation:**

```{r bessel_correction, fig.cap="Dividing by n-1 corrects the bias in variance estimation"}
set.seed(789)

# True population
pop_size <- 100000
pop_mean <- 50
pop_sd <- 10
population <- rnorm(pop_size, mean = pop_mean, sd = pop_sd)
true_variance <- var(population) * (pop_size - 1) / pop_size  # Population variance

# Repeatedly sample and estimate variance both ways
n_simulations <- 5000
sample_size <- 10

results <- data.table$data.table(
    sim = 1:n_simulations,
    var_n = numeric(n_simulations),
    var_n_minus_1 = numeric(n_simulations)
)

for (i in 1:n_simulations) {
    samp <- sample(population, sample_size)
    mean_samp <- mean(samp)
    ss <- sum((samp - mean_samp)^2)

    results[i, var_n := ss / sample_size]
    results[i, var_n_minus_1 := ss / (sample_size - 1)]
}

cat("True population variance:", round(true_variance, 2), "\n")
cat("Mean of s² (divide by n):", round(mean(results$var_n), 2),
    "- biased LOW\n")
cat("Mean of s² (divide by n-1):", round(mean(results$var_n_minus_1), 2),
    "- unbiased\n")

# Visualise
results_long <- data.table$melt(
    results,
    id.vars = "sim",
    variable.name = "method",
    value.name = "variance"
)
results_long[, method := factor(method,
    levels = c("var_n", "var_n_minus_1"),
    labels = c("Divide by n (biased)", "Divide by n-1 (unbiased)")
)]

ggplot2$ggplot(results_long, ggplot2$aes(x = variance, fill = method)) +
    ggplot2$geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
    ggplot2$geom_vline(xintercept = true_variance, colour = "black",
               linetype = "dashed", size = 1.2) +
    ggplot2$facet_wrap(~method, ncol = 1) +
    ggplot2$labs(
        title = "Why We Divide by n-1: Bessel's Correction",
        subtitle = paste("5000 samples of size 10; true variance =",
                        round(true_variance, 1)),
        x = "Estimated Variance",
        y = "Count"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

### 2.2.3 Standard Deviation

The standard deviation is the square root of variance:

$$s = \sqrt{s^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$

The standard deviation returns us to the original units of measurement. If we measure weight in kilograms, variance is in "kilograms squared" (which is difficult to interpret), but standard deviation is in kilograms.

```{r sd_from_scratch}
# Implement standard deviation from scratch
my_sd <- function(x) {
    return(sqrt(my_sample_variance(x)))
}

# Test
cat("Our implementation:", my_sd(bmi_clean), "\n")
cat("Built-in sd():", sd(bmi_clean), "\n")

# Interpretation: for approximately normal data
# About 68% of data fall within 1 SD of the mean
# About 95% fall within 2 SD
# About 99.7% fall within 3 SD
mean_bmi <- mean(bmi_clean)
sd_bmi <- sd(bmi_clean)

within_1sd <- mean(bmi_clean >= mean_bmi - sd_bmi &
                   bmi_clean <= mean_bmi + sd_bmi)
within_2sd <- mean(bmi_clean >= mean_bmi - 2*sd_bmi &
                   bmi_clean <= mean_bmi + 2*sd_bmi)
within_3sd <- mean(bmi_clean >= mean_bmi - 3*sd_bmi &
                   bmi_clean <= mean_bmi + 3*sd_bmi)

cat("\nEmpirical rule check for BMI data:\n")
cat("Within 1 SD:", round(within_1sd * 100, 1), "% (expected ~68%)\n")
cat("Within 2 SD:", round(within_2sd * 100, 1), "% (expected ~95%)\n")
cat("Within 3 SD:", round(within_3sd * 100, 1), "% (expected ~99.7%)\n")
```

### 2.2.4 Coefficient of Variation

The coefficient of variation (CV) expresses standard deviation as a percentage of the mean:

$$\text{CV} = \frac{s}{\bar{x}} \times 100\%$$

The CV enables comparison of variability across different scales.

```{r cv_from_scratch}
# Implement coefficient of variation from scratch
my_cv <- function(x) {
    x <- x[!is.na(x)]
    return(sd(x) / mean(x) * 100)
}

# Compare variability of height and weight
height_data <- nhanes[!is.na(Height), Height]
weight_data <- nhanes[!is.na(Weight), Weight]

cat("Height: mean =", round(mean(height_data), 1), "cm,",
    "SD =", round(sd(height_data), 1), "cm,",
    "CV =", round(my_cv(height_data), 1), "%\n")

cat("Weight: mean =", round(mean(weight_data), 1), "kg,",
    "SD =", round(sd(weight_data), 1), "kg,",
    "CV =", round(my_cv(weight_data), 1), "%\n")

cat("\nWeight is relatively more variable than height\n")
cat("(even though height has a larger SD in absolute terms)\n")
```

### 2.2.5 Mean Absolute Deviation

The mean absolute deviation (MAD) is easier to interpret than variance: it is the average distance from the mean.

$$\text{MAD} = \frac{1}{n} \sum_{i=1}^{n} |x_i - \bar{x}|$$

```{r mad_from_scratch}
# Implement mean absolute deviation from scratch
my_mad_mean <- function(x) {
    x <- x[!is.na(x)]
    return(mean(abs(x - mean(x))))
}

# Test
cat("Mean absolute deviation (from mean):", my_mad_mean(bmi_clean), "\n")
cat("Standard deviation:", sd(bmi_clean), "\n")
cat("\nMAD is always smaller than SD because squaring amplifies large deviations\n")
```

### 2.2.6 Robust Measures of Spread

When outliers are present, robust measures outperform classical ones.

**Median Absolute Deviation (MAD)**

The MAD uses the median instead of the mean:

$$\text{MAD} = \text{median}(|x_i - \text{median}(x)|)$$

For comparison with SD, we often multiply by a scale factor (1.4826 for normal data):

```{r robust_mad, fig.cap="MAD is robust to outliers whilst SD is sensitive"}
# Implement median absolute deviation from scratch
my_mad_median <- function(x, constant = 1.4826) {
    x <- x[!is.na(x)]
    med <- median(x)
    return(constant * median(abs(x - med)))
}

# Compare sensitivity to outliers
clean_data <- c(10, 11, 12, 12, 13, 13, 14, 15, 16)
contaminated <- c(10, 11, 12, 12, 13, 13, 14, 15, 100)  # One outlier

cat("Clean data:\n")
cat("  SD:", round(sd(clean_data), 2), "\n")
cat("  MAD:", round(my_mad_median(clean_data), 2), "\n\n")

cat("With outlier (100):\n")
cat("  SD:", round(sd(contaminated), 2), "(increased dramatically)\n")
cat("  MAD:", round(my_mad_median(contaminated), 2), "(barely changed)\n")

# Visualise comparison
set.seed(111)
normal_sample <- rnorm(100, mean = 50, sd = 10)
contaminated_sample <- c(rnorm(95, mean = 50, sd = 10),
                         rnorm(5, mean = 100, sd = 5))  # 5% outliers

comparison_data <- data.table$data.table(
    value = c(normal_sample, contaminated_sample),
    dataset = rep(c("Clean Data", "5% Contaminated"), each = 100)
)

stats_summary <- comparison_data[, .(
    SD = sd(value),
    MAD = mad(value)
), by = dataset]

print(stats_summary)

ggplot2$ggplot(comparison_data, ggplot2$aes(x = value, fill = dataset)) +
    ggplot2$geom_histogram(bins = 25, alpha = 0.7, colour = "white") +
    ggplot2$facet_wrap(~dataset, ncol = 1) +
    ggplot2$labs(
        title = "Robust vs Classical Measures of Spread",
        subtitle = "SD inflates with outliers; MAD remains stable",
        x = "Value",
        y = "Count"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(legend.position = "none")
```

## 2.3 Measures of Position

Position measures tell us where individual observations stand relative to the distribution.

### 2.3.1 Percentiles and Quantiles

The **p-th percentile** is the value below which p% of the data fall.

**Formal definition:** For data sorted in ascending order, the p-th percentile $P_p$ satisfies:
- At least p% of observations are ≤ $P_p$
- At least (100-p)% of observations are ≥ $P_p$

**Quantiles** are the same concept on a 0-1 scale: the 0.25 quantile equals the 25th percentile.

```{r percentiles_from_scratch}
# Implement percentile function from scratch
# Using linear interpolation (Type 7 in R's quantile function)
my_percentile <- function(x, p) {
    x <- sort(x[!is.na(x)])
    n <- length(x)

    # Convert percentage to proportion if necessary
    if (p > 1) p <- p / 100

    # Calculate the index
    index <- 1 + (n - 1) * p

    # Get lower and upper indices
    lo <- floor(index)
    hi <- ceiling(index)

    # Interpolate
    if (lo == hi) {
        return(x[lo])
    } else {
        return(x[lo] + (index - lo) * (x[hi] - x[lo]))
    }
}

# Test with NHANES BMI
cat("Selected BMI percentiles:\n")
for (p in c(5, 10, 25, 50, 75, 90, 95)) {
    cat(p, "th percentile:", round(my_percentile(bmi_clean, p), 1), "\n")
}

cat("\nBuilt-in quantile() for comparison:\n")
print(quantile(bmi_clean, probs = c(0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95)))
```

### 2.3.2 Quartiles and the Five-Number Summary

Quartiles divide the data into four equal parts:

- Q1 (25th percentile): 25% of data below
- Q2 (50th percentile): median, 50% below
- Q3 (75th percentile): 75% below

The **five-number summary** provides a concise distribution overview:

$$\{\text{Min}, Q_1, \text{Median}, Q_3, \text{Max}\}$$

```{r five_number_summary, fig.cap="The five-number summary provides a concise distribution snapshot"}
# Implement five-number summary from scratch
my_fivenum <- function(x) {
    x <- x[!is.na(x)]
    x_sorted <- sort(x)

    c(
        Min = min(x),
        Q1 = my_percentile(x, 25),
        Median = median(x),
        Q3 = my_percentile(x, 75),
        Max = max(x)
    )
}

# Example with blood pressure
bp_fivenum <- my_fivenum(bp_data)
print(round(bp_fivenum, 1))

# Compare to built-in
cat("\nBuilt-in fivenum():\n")
print(round(fivenum(bp_data), 1))

# Visualise with boxplot
bp_dt <- data.table$data.table(bp = bp_data)

ggplot2$ggplot(bp_dt, ggplot2$aes(y = bp)) +
    ggplot2$geom_boxplot(fill = "#56B4E9", alpha = 0.7, width = 0.3) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Min"],
             label = paste("Min =", round(bp_fivenum["Min"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Q1"],
             label = paste("Q1 =", round(bp_fivenum["Q1"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Median"],
             label = paste("Median =", round(bp_fivenum["Median"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Q3"],
             label = paste("Q3 =", round(bp_fivenum["Q3"], 0)), hjust = 0) +
    ggplot2$annotate("text", x = 0.25, y = bp_fivenum["Max"],
             label = paste("Max =", round(bp_fivenum["Max"], 0)), hjust = 0) +
    ggplot2$labs(
        title = "Five-Number Summary Visualised",
        subtitle = "Systolic blood pressure from NHANES",
        y = "Systolic Blood Pressure (mmHg)"
    ) +
    ggplot2$theme_minimal() +
    ggplot2$theme(axis.text.x = ggplot2$element_blank(),
          axis.title.x = ggplot2$element_blank())
```

### 2.3.3 Z-Scores (Standard Scores)

A **z-score** expresses how many standard deviations an observation is from the mean:

$$z = \frac{x - \bar{x}}{s}$$

Z-scores enable comparison across different variables and populations.

```{r zscore_from_scratch, fig.cap="Z-scores standardise data to a common scale"}
# Implement z-score from scratch
my_zscore <- function(x) {
    x_mean <- mean(x, na.rm = TRUE)
    x_sd <- sd(x, na.rm = TRUE)
    return((x - x_mean) / x_sd)
}

# Calculate z-scores for BMI
bmi_z <- my_zscore(bmi_clean)

cat("Original BMI: mean =", round(mean(bmi_clean), 1),
    ", SD =", round(sd(bmi_clean), 1), "\n")
cat("Z-scores: mean =", round(mean(bmi_z), 6),
    ", SD =", round(sd(bmi_z), 6), "\n")

# Interpretation examples
example_bmis <- c(18.5, 25, 30, 40)
example_z <- (example_bmis - mean(bmi_clean)) / sd(bmi_clean)

cat("\nZ-score interpretation:\n")
for (i in seq_along(example_bmis)) {
    cat("BMI =", example_bmis[i], "-> z =", round(example_z[i], 2))
    if (abs(example_z[i]) < 1) {
        cat(" (within 1 SD, typical)\n")
    } else if (abs(example_z[i]) < 2) {
        cat(" (1-2 SD from mean)\n")
    } else {
        cat(" (>2 SD from mean, unusual)\n")
    }
}

# Visualise
z_dt <- data.table$data.table(original = bmi_clean, zscore = bmi_z)

ggplot2$ggplot(z_dt, ggplot2$aes(x = zscore)) +
    ggplot2$geom_histogram(ggplot2$aes(y = ..density..), bins = 40,
                   fill = "#E69F00", colour = "white", alpha = 0.8) +
    ggplot2$geom_density(colour = "#D55E00", size = 1) +
    ggplot2$geom_vline(xintercept = c(-2, -1, 0, 1, 2),
               linetype = c("dotted", "dashed", "solid", "dashed", "dotted"),
               colour = "blue") +
    ggplot2$labs(
        title = "Z-Score Distribution",
        subtitle = "BMI standardised; vertical lines at z = -2, -1, 0, 1, 2",
        x = "Z-Score",
        y = "Density"
    ) +
    ggplot2$theme_minimal()
```

### 2.3.4 Percentile Ranks

The **percentile rank** of a value tells us what percentage of the data fall at or below that value.

```{r percentile_rank}
# Implement percentile rank from scratch
my_percentile_rank <- function(x, value) {
    x <- x[!is.na(x)]
    # Proportion of values less than or equal to the given value
    return(mean(x <= value) * 100)
}

# Example: what percentile is a BMI of 25?
cat("BMI = 25 is at the", round(my_percentile_rank(bmi_clean, 25), 1),
    "percentile\n")
cat("BMI = 30 is at the", round(my_percentile_rank(bmi_clean, 30), 1),
    "percentile\n")
cat("BMI = 40 is at the", round(my_percentile_rank(bmi_clean, 40), 1),
    "percentile\n")

# Application: clinical reference ranges
# Often defined as 5th to 95th percentile
cat("\nReference range (5th-95th percentile) for BMI:\n")
cat(round(quantile(bmi_clean, 0.05), 1), "to",
    round(quantile(bmi_clean, 0.95), 1), "kg/m²\n")
```

---

## Communicating to Stakeholders

When explaining dispersion and position to collaborators:

**On spread:**
> "The standard deviation tells us how much values typically differ from the average. For blood pressure, an SD of 15 means most readings fall within about 15 mmHg above or below the average."

**On IQR:**
> "The interquartile range captures the middle 50% of patients. It's less affected by extreme values than the range."

**On z-scores:**
> "A z-score tells us how unusual a value is. A z-score of 2 means the value is two standard deviations above average—quite unusual, occurring in less than 3% of cases."

**On percentiles:**
> "If a patient is at the 90th percentile for BMI, that means 90% of people in our reference population have a lower BMI."

---

## Quick Reference

### Dispersion Formulae

| Measure | Formula | R Function |
|---------|---------|------------|
| Range | $x_{\max} - x_{\min}$ | `range(x); diff(range(x))` |
| IQR | $Q_3 - Q_1$ | `IQR(x)` |
| Variance | $s^2 = \frac{1}{n-1}\sum(x_i - \bar{x})^2$ | `var(x)` |
| SD | $s = \sqrt{s^2}$ | `sd(x)` |
| CV | $\frac{s}{\bar{x}} \times 100\%$ | `sd(x)/mean(x)*100` |
| MAD | $\text{median}(\|x_i - \text{median}(x)\|)$ | `mad(x)` |

### Position Formulae

| Measure | Definition | R Function |
|---------|------------|------------|
| Percentile | Value below which p% fall | `quantile(x, p)` |
| Z-score | $z = \frac{x - \bar{x}}{s}$ | `scale(x)` |
| Percentile rank | % of data ≤ value | `ecdf(x)(value) * 100` |

### When to Use Each Measure

| Data Characteristic | Spread Measure | Position Measure |
|--------------------|----------------|------------------|
| Symmetric, no outliers | SD, Variance | Z-scores |
| Skewed or outliers | IQR, MAD | Percentiles |
| Comparing across scales | CV | Z-scores |
| Clinical reference | IQR | Percentile ranks |

### Five-Number Summary

$$\{\text{Min}, Q_1, \text{Median}, Q_3, \text{Max}\}$$

R: `fivenum(x)` or `summary(x)`
