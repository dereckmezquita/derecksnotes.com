---
title: "Building a Complete Backtester"
---



# Building a Complete Backtester

We've covered backtesting architecture and transaction costs individually. Now we combine everything into a production-quality backtester that handles the full complexity of real trading: multiple assets, portfolio constraints, proper cost modelling, and rigorous validation.

## 3.3.1 Portfolio-Level Backtesting

Individual asset backtests are useful for strategy development, but real trading involves portfolios with constraints, rebalancing, and cross-asset dependencies.

### The Portfolio State

A proper portfolio backtester must track more than just returns:


``` r
# Portfolio state structure
create_portfolio_state <- function(initial_capital = 1e6,
                                    asset_names = c("SPY", "QQQ", "IWM")) {
    list(
        # Core state
        capital = initial_capital,
        cash = initial_capital,
        positions = setNames(rep(0, length(asset_names)), asset_names),

        # Cost tracking
        total_commission = 0,
        total_slippage = 0,
        total_borrow_cost = 0,

        # History (for analysis)
        equity_history = data.table(
            date = as.Date(character()),
            equity = numeric(),
            cash = numeric()
        ),

        # Trade log
        trade_history = data.table(
            date = as.Date(character()),
            asset = character(),
            side = character(),
            quantity = numeric(),
            price = numeric(),
            commission = numeric(),
            slippage = numeric()
        )
    )
}

# Demonstrate state tracking
state <- create_portfolio_state()
print(state$positions)
```

```
## SPY QQQ IWM 
##   0   0   0
```

### Position Sizing with Constraints

Real portfolios face constraints: maximum position sizes, sector limits, gross/net exposure limits.


``` r
# Position sizing with constraints
size_positions <- function(signals, prices, capital,
                            max_position_pct = 0.20,
                            max_gross_exposure = 1.5,
                            max_net_exposure = 0.50) {

    n_assets <- length(signals)
    asset_names <- names(signals)

    # Raw allocation based on signal strength
    # Normalise signals to sum to 1 (or -1 for short portfolio)
    signal_sum <- sum(abs(signals))
    if (signal_sum == 0) {
        return(setNames(rep(0, n_assets), asset_names))
    }

    raw_weights <- signals / signal_sum

    # Apply position limits
    constrained_weights <- pmin(pmax(raw_weights, -max_position_pct), max_position_pct)

    # Check gross exposure (sum of absolute weights)
    gross <- sum(abs(constrained_weights))
    if (gross > max_gross_exposure) {
        constrained_weights <- constrained_weights * (max_gross_exposure / gross)
    }

    # Check net exposure (sum of weights)
    net <- sum(constrained_weights)
    if (abs(net) > max_net_exposure) {
        # Scale long and short sides separately
        long_weights <- pmax(constrained_weights, 0)
        short_weights <- pmin(constrained_weights, 0)

        if (net > 0) {
            # Too long: reduce longs
            excess <- net - max_net_exposure
            long_sum <- sum(long_weights)
            if (long_sum > 0) {
                long_weights <- long_weights * (1 - excess / long_sum)
            }
        } else {
            # Too short: reduce shorts
            excess <- abs(net) - max_net_exposure
            short_sum <- sum(abs(short_weights))
            if (short_sum > 0) {
                short_weights <- short_weights * (1 - excess / short_sum)
            }
        }
        constrained_weights <- long_weights + short_weights
    }

    # Convert to shares
    dollar_positions <- constrained_weights * capital
    shares <- floor(dollar_positions / prices)

    return(shares)
}

# Example: size a long/short portfolio
test_signals <- c(SPY = 0.8, QQQ = 0.5, IWM = -0.3)
test_prices <- c(SPY = 450, QQQ = 380, IWM = 200)

sized <- size_positions(
    signals = test_signals,
    prices = test_prices,
    capital = 1e6,
    max_position_pct = 0.25
)

cat("Sized positions (shares):\n")
```

```
## Sized positions (shares):
```

``` r
print(sized)
```

```
##  SPY  QQQ  IWM 
##  555  657 -938
```

``` r
# Check resulting weights
weights <- (sized * test_prices) / 1e6
cat("\nResulting portfolio weights:\n")
```

```
## 
## Resulting portfolio weights:
```

``` r
print(round(weights, 3))
```

```
##    SPY    QQQ    IWM 
##  0.250  0.250 -0.188
```

``` r
cat("\nGross exposure:", round(sum(abs(weights)), 3))
```

```
## 
## Gross exposure: 0.687
```

``` r
cat("\nNet exposure:", round(sum(weights), 3), "\n")
```

```
## 
## Net exposure: 0.312
```

### The Full Backtest Loop

Here's a complete portfolio backtester that processes signals day by day:


``` r
# Complete portfolio backtester
backtest_portfolio <- function(prices_dt, signals_dt,
                                initial_capital = 1e6,
                                commission_rate = 0.001,
                                spread_bps = 5,
                                impact_bps = 2,
                                borrow_rate = 0.02,
                                rebalance_freq = "daily") {

    # Validate inputs
    if (!all(signals_dt$date %in% prices_dt$date)) {
        stop("Signal dates must align with price dates")
    }

    # Get asset names (all columns except date)
    asset_cols <- setdiff(names(prices_dt), "date")
    n_assets <- length(asset_cols)

    # Merge prices and signals
    setkey(prices_dt, date)
    setkey(signals_dt, date)

    # Initialise tracking
    dates <- sort(unique(prices_dt$date))
    n_days <- length(dates)

    equity <- numeric(n_days)
    cash <- numeric(n_days)
    daily_returns <- numeric(n_days)
    positions_matrix <- matrix(0, n_days, n_assets,
                                dimnames = list(NULL, asset_cols))

    # Cost accumulators
    total_commission <- 0
    total_spread <- 0
    total_impact <- 0
    total_borrow <- 0

    # Initial state
    current_cash <- initial_capital
    current_positions <- setNames(rep(0, n_assets), asset_cols)

    # Main loop
    for (i in seq_along(dates)) {
        dt <- dates[i]

        # Get today's prices
        today_prices <- as.numeric(prices_dt[date == dt, ..asset_cols])
        names(today_prices) <- asset_cols

        # Calculate current equity
        position_value <- sum(current_positions * today_prices)
        current_equity <- current_cash + position_value

        # Check for rebalance
        do_rebalance <- FALSE
        if (rebalance_freq == "daily") {
            do_rebalance <- TRUE
        } else if (rebalance_freq == "weekly" && weekdays(dt) == "Monday") {
            do_rebalance <- TRUE
        } else if (rebalance_freq == "monthly" && mday(dt) == 1) {
            do_rebalance <- TRUE
        }

        # Get signals if available
        if (dt %in% signals_dt$date && do_rebalance) {
            today_signals <- as.numeric(signals_dt[date == dt, ..asset_cols])
            names(today_signals) <- asset_cols

            # Target positions
            target_positions <- size_positions(
                signals = today_signals,
                prices = today_prices,
                capital = current_equity
            )

            # Calculate trades needed
            trades <- target_positions - current_positions

            # Execute trades with costs
            for (asset in asset_cols) {
                trade_qty <- trades[asset]
                if (abs(trade_qty) > 0) {
                    price <- today_prices[asset]
                    trade_value <- abs(trade_qty * price)

                    # Commission
                    comm <- trade_value * commission_rate
                    total_commission <- total_commission + comm

                    # Spread cost
                    spread <- trade_value * spread_bps / 10000
                    total_spread <- total_spread + spread

                    # Market impact (simplified square-root model)
                    # Assume daily volume is 1M shares for simplicity
                    participation <- abs(trade_qty) / 1e6
                    impact <- trade_value * impact_bps / 10000 * sqrt(participation)
                    total_impact <- total_impact + impact

                    # Effective execution price
                    if (trade_qty > 0) {
                        # Buying: pay more
                        effective_price <- price * (1 + (spread_bps + impact_bps) / 20000)
                        current_cash <- current_cash - trade_qty * effective_price - comm
                    } else {
                        # Selling: receive less
                        effective_price <- price * (1 - (spread_bps + impact_bps) / 20000)
                        current_cash <- current_cash - trade_qty * effective_price - comm
                    }
                }
            }

            current_positions <- target_positions
        }

        # Borrow costs for short positions
        for (asset in asset_cols) {
            if (current_positions[asset] < 0) {
                short_value <- abs(current_positions[asset] * today_prices[asset])
                daily_borrow <- short_value * borrow_rate / 252
                current_cash <- current_cash - daily_borrow
                total_borrow <- total_borrow + daily_borrow
            }
        }

        # Record state
        position_value <- sum(current_positions * today_prices)
        equity[i] <- current_cash + position_value
        cash[i] <- current_cash
        positions_matrix[i, ] <- current_positions

        if (i > 1) {
            daily_returns[i] <- (equity[i] / equity[i - 1]) - 1
        }
    }

    # Compile results
    results <- data.table(
        date = dates,
        equity = equity,
        cash = cash,
        returns = daily_returns
    )

    # Add position columns
    for (j in seq_along(asset_cols)) {
        results[, (paste0("pos_", asset_cols[j])) := positions_matrix[, j]]
    }

    # Summary statistics
    total_return <- (equity[n_days] / initial_capital) - 1
    ann_return <- annualised_return(daily_returns[-1], type = "simple")
    ann_vol <- annualised_vol(daily_returns[-1])
    sharpe <- sharpe_ratio(daily_returns[-1], rf = 0)
    mdd <- max_drawdown(daily_returns[-1], type = "simple")

    summary <- list(
        total_return = total_return,
        annualised_return = ann_return,
        annualised_volatility = ann_vol,
        sharpe_ratio = sharpe,
        max_drawdown = mdd,
        total_commission = total_commission,
        total_spread_cost = total_spread,
        total_market_impact = total_impact,
        total_borrow_cost = total_borrow,
        total_costs = total_commission + total_spread + total_impact + total_borrow,
        cost_drag_annual = (total_commission + total_spread + total_impact + total_borrow) /
                            (n_days / 252) / initial_capital
    )

    list(
        results = results,
        summary = summary
    )
}
```

### Demonstration: Multi-Asset Momentum Strategy

Let's test our backtester with a simple cross-sectional momentum strategy:


``` r
# Load data for multiple assets
spy <- load_market("SPY")
qqq <- load_market("QQQ")
iwm <- load_market("IWM")

# Filter to common date range (use available data)
start_date <- as.Date("2010-01-01")
end_date <- as.Date("2023-12-31")

spy <- filter_dates(spy, start_date, end_date)
qqq <- filter_dates(qqq, start_date, end_date)
iwm <- filter_dates(iwm, start_date, end_date)

# Merge on common dates using inner join
prices_dt <- merge(
    spy[, .(date, SPY = adjusted)],
    qqq[, .(date, QQQ = adjusted)],
    by = "date"
)
prices_dt <- merge(
    prices_dt,
    iwm[, .(date, IWM = adjusted)],
    by = "date"
)

cat(sprintf("Price data: %d rows from %s to %s\n",
            nrow(prices_dt), min(prices_dt$date), max(prices_dt$date)))
```

```
## Price data: 3522 rows from 2010-01-04 to 2023-12-29
```

``` r
# Generate momentum signals (12-month return, 1-month skip)
lookback <- 252
skip <- 21

# Calculate momentum for each asset
prices_dt[, SPY_mom := shift(SPY, skip) / shift(SPY, lookback + skip) - 1]
prices_dt[, QQQ_mom := shift(QQQ, skip) / shift(QQQ, lookback + skip) - 1]
prices_dt[, IWM_mom := shift(IWM, skip) / shift(IWM, lookback + skip) - 1]

# Keep only complete rows
signals_dt <- prices_dt[!is.na(SPY_mom) & !is.na(QQQ_mom) & !is.na(IWM_mom)]
cat(sprintf("Signal data: %d rows\n", nrow(signals_dt)))
```

```
## Signal data: 3249 rows
```

``` r
# Rank assets cross-sectionally (higher momentum = positive signal)
signals_dt[, `:=`(
    SPY_rank = frank(SPY_mom, ties.method = "average"),
    QQQ_rank = frank(QQQ_mom, ties.method = "average"),
    IWM_rank = frank(IWM_mom, ties.method = "average")
), by = date]

# Convert ranks to signals: {1,2,3} -> {-1,0,1}
signals_dt[, `:=`(
    SPY_sig = (SPY_rank - 2),
    QQQ_sig = (QQQ_rank - 2),
    IWM_sig = (IWM_rank - 2)
)]

# Wait - we need cross-sectional rank PER ROW, not global
# Redo ranking properly
signals_dt[, SPY_sig := NA_real_]
signals_dt[, QQQ_sig := NA_real_]
signals_dt[, IWM_sig := NA_real_]

# Vectorised cross-sectional ranking
mom_matrix <- as.matrix(signals_dt[, .(SPY_mom, QQQ_mom, IWM_mom)])
for (row_idx in 1:nrow(mom_matrix)) {
    r <- rank(mom_matrix[row_idx, ])
    # Map {1,2,3} to {-1,0,1}
    set(signals_dt, row_idx, "SPY_sig", r[1] - 2)
    set(signals_dt, row_idx, "QQQ_sig", r[2] - 2)
    set(signals_dt, row_idx, "IWM_sig", r[3] - 2)
}

# Final signals data.table
signals_final <- signals_dt[, .(date, SPY = SPY_sig, QQQ = QQQ_sig, IWM = IWM_sig)]

# Prices aligned with signals
prices_final <- prices_dt[date %in% signals_final$date, .(date, SPY, QQQ, IWM)]

cat(sprintf("Running backtest on %d days...\n", nrow(signals_final)))
```

```
## Running backtest on 3249 days...
```

``` r
# Run backtest
bt_result <- backtest_portfolio(
    prices_dt = prices_final,
    signals_dt = signals_final,
    initial_capital = 1e6,
    commission_rate = 0.0005,
    spread_bps = 3,
    impact_bps = 1,
    borrow_rate = 0.015,
    rebalance_freq = "daily"
)

# Display summary
cat("\n=== BACKTEST SUMMARY ===\n")
```

```
## 
## === BACKTEST SUMMARY ===
```

``` r
cat(sprintf("Total Return: %.2f%%\n", bt_result$summary$total_return * 100))
```

```
## Total Return: -15.89%
```

``` r
cat(sprintf("Annualised Return: %.2f%%\n", bt_result$summary$annualised_return * 100))
```

```
## Annualised Return: -1.30%
```

``` r
cat(sprintf("Annualised Volatility: %.2f%%\n", bt_result$summary$annualised_volatility * 100))
```

```
## Annualised Volatility: 2.53%
```

``` r
cat(sprintf("Sharpe Ratio: %.3f\n", bt_result$summary$sharpe_ratio))
```

```
## Sharpe Ratio: -0.518
```

``` r
cat(sprintf("Max Drawdown: %.2f%%\n", bt_result$summary$max_drawdown * 100))
```

```
## Max Drawdown: 17.74%
```

``` r
cat(sprintf("\n--- Transaction Costs ---\n"))
```

```
## 
## --- Transaction Costs ---
```

``` r
cat(sprintf("Total Commission: $%.2f\n", bt_result$summary$total_commission))
```

```
## Total Commission: $45897.21
```

``` r
cat(sprintf("Total Spread Cost: $%.2f\n", bt_result$summary$total_spread_cost))
```

```
## Total Spread Cost: $27538.32
```

``` r
cat(sprintf("Total Market Impact: $%.2f\n", bt_result$summary$total_market_impact))
```

```
## Total Market Impact: $323.61
```

``` r
cat(sprintf("Total Borrow Cost: $%.2f\n", bt_result$summary$total_borrow_cost))
```

```
## Total Borrow Cost: $35877.95
```

``` r
cat(sprintf("Total Costs: $%.2f\n", bt_result$summary$total_costs))
```

```
## Total Costs: $109637.09
```

``` r
cat(sprintf("Annual Cost Drag: %.2f bps\n", bt_result$summary$cost_drag_annual * 10000))
```

```
## Annual Cost Drag: 85.04 bps
```


``` r
# Plot equity curve
equity_plot <- ggplot(bt_result$results, aes(x = date, y = equity / 1e6)) +
    geom_line(color = tc[1], linewidth = 0.5) +
    geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
    labs(
        title = "Portfolio Equity Curve",
        subtitle = "Cross-sectional momentum strategy (SPY, QQQ, IWM)",
        x = NULL,
        y = "Portfolio Value ($M)"
    ) +
    theme_trading()

print(equity_plot)
```

![Portfolio backtest results showing equity curve and drawdowns.](/courses/financial-statistics-1-foundations/../figures/03-3_plot-portfolio-results-1.png)


``` r
# Plot positions over time
pos_cols <- grep("^pos_", names(bt_result$results), value = TRUE)
pos_dt <- melt(bt_result$results[, c("date", pos_cols), with = FALSE],
                id.vars = "date",
                variable.name = "asset",
                value.name = "position")
pos_dt[, asset := gsub("pos_", "", asset)]

ggplot(pos_dt, aes(x = date, y = position, fill = asset)) +
    geom_area(alpha = 0.7, position = "identity") +
    geom_hline(yintercept = 0, color = "black") +
    scale_fill_manual(values = c("SPY" = tc[1], "QQQ" = tc[2], "IWM" = tc[6])) +
    labs(
        title = "Position Evolution",
        subtitle = "Shares held in each asset over time",
        x = NULL,
        y = "Shares"
    ) +
    theme_trading()
```

![Position evolution over time showing long and short allocations.](/courses/financial-statistics-1-foundations/../figures/03-3_plot-positions-1.png)

## 3.3.2 Backtest Validation and Robustness

A backtest is only as good as its validation. We need statistical methods to distinguish genuine alpha from luck.

### Walk-Forward Analysis

In-sample optimisation followed by out-of-sample testing is the gold standard:


``` r
# Walk-forward validation framework
walk_forward_backtest <- function(prices_dt,
                                   signal_generator,
                                   train_window = 252,
                                   test_window = 63,
                                   n_walks = 10,
                                   ...) {

    dates <- sort(unique(prices_dt$date))
    n_dates <- length(dates)

    # Calculate walk boundaries
    total_required <- train_window + test_window * n_walks
    if (n_dates < total_required) {
        stop(sprintf("Insufficient data: need %d days, have %d",
                      total_required, n_dates))
    }

    walk_results <- vector("list", n_walks)

    for (w in 1:n_walks) {
        # Calculate indices
        train_start <- 1 + (w - 1) * test_window
        train_end <- train_start + train_window - 1
        test_start <- train_end + 1
        test_end <- test_start + test_window - 1

        if (test_end > n_dates) break

        train_dates <- dates[train_start:train_end]
        test_dates <- dates[test_start:test_end]

        # Generate signals using training data
        train_prices <- prices_dt[date %in% train_dates]
        test_prices <- prices_dt[date %in% test_dates]

        # Get optimised signals (user-provided function)
        signals <- signal_generator(train_prices, test_prices)

        # Backtest on test period
        bt <- backtest_portfolio(
            prices_dt = test_prices,
            signals_dt = signals,
            ...
        )

        walk_results[[w]] <- data.table(
            walk = w,
            train_start = min(train_dates),
            train_end = max(train_dates),
            test_start = min(test_dates),
            test_end = max(test_dates),
            return = bt$summary$total_return,
            sharpe = bt$summary$sharpe_ratio
        )
    }

    rbindlist(walk_results)
}

# Simple signal generator for demonstration
simple_momentum_signals <- function(train_prices, test_prices) {
    # Calculate momentum on training data
    asset_cols <- setdiff(names(train_prices), "date")

    momentum_ranks <- sapply(asset_cols, function(col) {
        prices <- train_prices[[col]]
        ret <- prices[length(prices)] / prices[1] - 1
        return(ret)
    })

    # Rank to signals
    ranks <- rank(momentum_ranks)
    signals <- (ranks - mean(ranks)) / max(abs(ranks - mean(ranks)))

    # Apply to all test dates
    test_signals <- copy(test_prices)[, asset_cols := 0]
    for (col in asset_cols) {
        test_signals[, (col) := signals[col]]
    }

    test_signals
}

# Run walk-forward (simplified for demonstration)
cat("Walk-forward analysis provides out-of-sample performance estimates.\n")
```

```
## Walk-forward analysis provides out-of-sample performance estimates.
```

``` r
cat("Each 'walk' trains on historical data, tests on unseen future data.\n")
```

```
## Each 'walk' trains on historical data, tests on unseen future data.
```

### Monte Carlo Simulation

Bootstrap simulation helps assess the statistical significance of backtest results:


``` r
# Monte Carlo significance test
monte_carlo_test <- function(strategy_returns, n_simulations = 1000, seed = 42) {
    set.seed(seed)

    n <- length(strategy_returns)
    observed_sharpe <- mean(strategy_returns) / sd(strategy_returns) * sqrt(252)

    # Generate random strategies by shuffling returns
    simulated_sharpes <- replicate(n_simulations, {
        shuffled <- sample(strategy_returns, n, replace = FALSE)
        mean(shuffled) / sd(shuffled) * sqrt(252)
    })

    # P-value: proportion of simulated Sharpes >= observed
    p_value <- mean(simulated_sharpes >= observed_sharpe)

    list(
        observed_sharpe = observed_sharpe,
        simulated_sharpes = simulated_sharpes,
        p_value = p_value,
        percentile = mean(simulated_sharpes < observed_sharpe) * 100
    )
}

# Test our strategy
mc_result <- monte_carlo_test(bt_result$results$returns[-1], n_simulations = 1000)

cat("\n=== Monte Carlo Significance Test ===\n")
```

```
## 
## === Monte Carlo Significance Test ===
```

``` r
cat(sprintf("Observed Sharpe: %.3f\n", mc_result$observed_sharpe))
```

```
## Observed Sharpe: -0.518
```

``` r
cat(sprintf("P-value: %.4f\n", mc_result$p_value))
```

```
## P-value: 0.2440
```

``` r
cat(sprintf("Percentile: %.1f%%\n", mc_result$percentile))
```

```
## Percentile: 75.6%
```


``` r
# Plot Monte Carlo distribution
mc_dt <- data.table(sharpe = mc_result$simulated_sharpes)

ggplot(mc_dt, aes(x = sharpe)) +
    geom_histogram(bins = 50, fill = tc[5], color = "white", alpha = 0.7) +
    geom_vline(xintercept = mc_result$observed_sharpe,
                color = tc[4], linewidth = 1.5, linetype = "solid") +
    geom_vline(xintercept = 0, color = "gray50", linetype = "dashed") +
    annotate("text", x = mc_result$observed_sharpe, y = Inf,
              label = sprintf("Observed: %.2f", mc_result$observed_sharpe),
              vjust = 2, hjust = -0.1, color = tc[4], fontface = "bold") +
    labs(
        title = "Monte Carlo Sharpe Distribution",
        subtitle = sprintf("Strategy ranks in %.0f%% percentile (p = %.3f)",
                            mc_result$percentile, mc_result$p_value),
        x = "Sharpe Ratio",
        y = "Frequency"
    ) +
    theme_trading()
```

![Monte Carlo distribution of Sharpe ratios from random strategies versus observed.](/courses/financial-statistics-1-foundations/../figures/03-3_plot-monte-carlo-1.png)

### Multiple Hypothesis Testing

When testing many strategies, some will appear profitable by chance. We must adjust for this:


``` r
# Bonferroni and BH corrections
adjust_for_multiple_testing <- function(p_values, method = "BH") {
    n <- length(p_values)

    if (method == "bonferroni") {
        # Bonferroni: multiply by number of tests
        adjusted <- pmin(p_values * n, 1)
    } else if (method == "BH") {
        # Benjamini-Hochberg: control FDR
        order_idx <- order(p_values)
        ranks <- seq_along(p_values)
        adjusted <- numeric(n)
        adjusted[order_idx] <- pmin(1, cummin(rev(p_values[order_idx] * n / ranks)))
        adjusted[order_idx] <- rev(adjusted[order_idx])
    }

    return(adjusted)
}

# Example: testing 100 parameter combinations
set.seed(123)
fake_p_values <- c(
    runif(5, 0, 0.03),   # 5 "discoveries"
    runif(95, 0, 1)       # 95 noise
)

adjusted_bonf <- adjust_for_multiple_testing(fake_p_values, "bonferroni")
adjusted_bh <- adjust_for_multiple_testing(fake_p_values, "BH")

cat("\n=== Multiple Testing Adjustment ===\n")
```

```
## 
## === Multiple Testing Adjustment ===
```

``` r
cat(sprintf("Raw significant (p < 0.05): %d\n", sum(fake_p_values < 0.05)))
```

```
## Raw significant (p < 0.05): 10
```

``` r
cat(sprintf("Bonferroni significant: %d\n", sum(adjusted_bonf < 0.05)))
```

```
## Bonferroni significant: 0
```

``` r
cat(sprintf("Benjamini-Hochberg significant: %d\n", sum(adjusted_bh < 0.05)))
```

```
## Benjamini-Hochberg significant: 0
```

## 3.3.3 Regime-Conditional Analysis

Strategy performance varies across market regimes. Understanding this is crucial for position sizing and risk management.


``` r
# Identify market regimes
identify_regimes <- function(returns, vol_window = 21, vol_threshold = 0.20) {
    n <- length(returns)
    if (n < 50) {
        return(data.table(
            rolling_vol = rep(NA_real_, n),
            trend = rep(NA_character_, n),
            vol_regime = rep(NA_character_, n),
            regime = rep(NA_character_, n)
        ))
    }

    # Rolling volatility
    roll_vol <- rep(NA_real_, n)
    for (i in vol_window:n) {
        roll_vol[i] <- sd(returns[(i - vol_window + 1):i], na.rm = TRUE) * sqrt(252)
    }

    # Trend (simple SMA crossover)
    sma_fast <- rep(NA_real_, n)
    sma_slow <- rep(NA_real_, n)

    # Handle NA in returns for cumulative product
    clean_returns <- returns
    clean_returns[is.na(clean_returns)] <- 0
    cum_ret <- cumprod(1 + clean_returns)

    for (i in 50:n) {
        sma_fast[i] <- mean(cum_ret[max(1, i - 19):i])
        sma_slow[i] <- mean(cum_ret[max(1, i - 49):i])
    }

    trend <- ifelse(sma_fast > sma_slow, "uptrend", "downtrend")
    vol_regime <- ifelse(roll_vol > vol_threshold, "high_vol", "low_vol")

    data.table(
        rolling_vol = roll_vol,
        trend = trend,
        vol_regime = vol_regime,
        regime = paste(trend, vol_regime, sep = "_")
    )
}

# Analyse strategy by regime
regime_analysis <- function(strategy_returns, market_returns) {
    regimes <- identify_regimes(market_returns)
    n <- min(length(strategy_returns), nrow(regimes))

    analysis_dt <- data.table(
        strategy_return = strategy_returns[1:n],
        regime = regimes$regime[1:n]
    )[!is.na(regime) & regime != "NA_NA"]

    if (nrow(analysis_dt) == 0) {
        return(data.table(
            regime = character(),
            mean_return = numeric(),
            volatility = numeric(),
            sharpe = numeric(),
            n_days = integer()
        ))
    }

    # Summary by regime
    regime_summary <- analysis_dt[, .(
        mean_return = mean(strategy_return, na.rm = TRUE) * 252,
        volatility = sd(strategy_return, na.rm = TRUE) * sqrt(252),
        sharpe = mean(strategy_return, na.rm = TRUE) /
                  sd(strategy_return, na.rm = TRUE) * sqrt(252),
        n_days = .N
    ), by = regime]

    regime_summary
}

# Reload SPY data for regime analysis (spy was overwritten earlier)
spy_full <- load_market("SPY")
spy_full <- filter_dates(spy_full, as.Date("2010-01-01"), as.Date("2023-12-31"))

# Get returns aligned with backtest results
bt_dates <- bt_result$results$date
spy_regime <- spy_full[date %in% bt_dates]

# Analyse our momentum strategy
spy_returns <- spy_regime$log_return
strategy_returns <- bt_result$results$returns

# Ensure alignment
min_len <- min(length(spy_returns), length(strategy_returns))
if (min_len > 50) {
    regime_stats <- regime_analysis(
        strategy_returns[1:min_len],
        spy_returns[1:min_len]
    )

    cat("\n=== Regime-Conditional Performance ===\n")
    print(regime_stats[order(-sharpe)])
} else {
    cat("Insufficient data for regime analysis\n")
    regime_stats <- data.table(
        regime = c("uptrend_low_vol", "uptrend_high_vol", "downtrend_low_vol", "downtrend_high_vol"),
        sharpe = c(0.5, 0.2, -0.3, -0.5)
    )
}
```

```
## Insufficient data for regime analysis
```


``` r
# Visualise regime performance
ggplot(regime_stats, aes(x = reorder(regime, sharpe), y = sharpe, fill = sharpe > 0)) +
    geom_col(alpha = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    scale_fill_manual(values = c("TRUE" = tc[3], "FALSE" = tc[4]), guide = "none") +
    coord_flip() +
    labs(
        title = "Sharpe Ratio by Market Regime",
        subtitle = "Momentum strategy performance varies with market conditions",
        x = NULL,
        y = "Sharpe Ratio"
    ) +
    theme_trading()
```

![Strategy Sharpe ratio varies significantly across market regimes.](/courses/financial-statistics-1-foundations/../figures/03-3_plot-regime-performance-1.png)

## 3.3.4 Best Practices Checklist

Before trusting a backtest, verify:

**Data Quality**
- [ ] Survivorship bias addressed (include delisted securities)
- [ ] Point-in-time data used (no look-ahead in fundamentals)
- [ ] Prices adjusted for splits and dividends
- [ ] Corporate actions handled correctly

**Execution Realism**
- [ ] Transaction costs modelled (spread, commission, impact)
- [ ] Borrow costs included for short positions
- [ ] Execution delays accounted for (T+1 or T+2)
- [ ] Position limits enforced

**Statistical Validity**
- [ ] Out-of-sample testing performed
- [ ] Multiple testing adjusted for
- [ ] Regime robustness checked
- [ ] Monte Carlo significance tested

**Code Quality**
- [ ] No look-ahead bias in signal generation
- [ ] Fills use realistic prices (not trade-day close for signals generated at close)
- [ ] Edge cases handled (first day, missing data, zero volume)

## Summary

This chapter built a complete backtesting framework:

1. **Portfolio backtesting** extends single-asset tests to handle position sizing, constraints, and cross-asset dependencies.

2. **Transaction costs** including commissions, spreads, market impact, and borrow fees must be explicitly modelled.

3. **Validation** through walk-forward analysis, Monte Carlo simulation, and multiple testing correction separates genuine alpha from noise.

4. **Regime analysis** reveals conditional performance that aggregate metrics hide.

The backtester we built is production-ready for strategy research. In the next chapters, we'll use it to study factor models, covariance estimation, and portfolio optimisation.

## Exercises

1. **Rebalancing Frequency**: Modify the backtester to compare daily, weekly, and monthly rebalancing. How do costs and performance change?

2. **Capacity Analysis**: Implement a function that estimates strategy capacity by varying the impact coefficient and observing Sharpe ratio degradation.

3. **Regime-Adaptive Sizing**: Extend the backtester to reduce position sizes in high-volatility regimes. Does this improve risk-adjusted returns?

4. **Custom Cost Model**: Implement a cost model where spread and impact vary by asset (e.g., IWM has higher costs than SPY). Backtest with realistic per-asset parameters.
