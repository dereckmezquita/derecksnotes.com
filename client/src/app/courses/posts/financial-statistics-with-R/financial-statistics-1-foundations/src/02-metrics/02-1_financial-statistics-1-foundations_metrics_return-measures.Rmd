---
title: "Algorithmic Trading with R"
chapter: "Chapter 2: Risk and Performance Metrics"
part: "Part 1: Measuring Returns"
section: "02-1"
coverImage: 13
author: "Dereck Mezquita"
date: 2026-01-21
tags: [algorithmic-trading, quantitative-finance, R, statistics, risk, performance]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# HTML5 figure hook for accessibility
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(
    dpi = 300,
    fig.width = 10,
    fig.height = 7,
    comment = "",
    warning = FALSE,
    collapse = FALSE,
    results = 'hold'
)

# box::use() must be called in each chunk that needs it
# Set working directory context for box modules
options(box.path = getwd())
```

# Part 1: Measuring Returns

Before you can evaluate whether a trading strategy is any good, you need to know what "good" means. This chapter establishes the metrics that separate winning strategies from losing ones, profitable traders from fooling-themselves traders.

We start with measuring returns—the raw material of performance analysis. The core problem is deceptively simple: you have a series of returns, and you want to estimate what you can expect going forward. The difficulty is that this estimation is fundamentally uncertain, and that uncertainty is larger than most practitioners realise.

```{r load-modules, message=FALSE}
# Load our course modules
box::use(
    ../modules/data[load_market, load_factors, filter_dates],
    ../modules/stats[sharpe_ratio, annualised_return, annualised_vol, max_drawdown],
    ../modules/viz[theme_trading, trading_colors]
)

box::use(
    data.table[...],
    ggplot2[...]
)

# Helper: convert trading_colors list to vector for indexed access
tc <- unlist(trading_colors)

set.seed(42)
```

---

## 2.1 Expected Return Estimation

The expected return is the single most important—and hardest to estimate—parameter in finance. Every investment decision ultimately depends on beliefs about future returns, yet the statistical properties of return estimation make precise prediction nearly impossible.

### 2.1.1 Prose/Intuition

What return can you realistically expect from a strategy? This seemingly simple question conceals the fundamental challenge of quantitative finance.

The natural approach is to compute the sample mean of historical returns and use that as an estimate of future expected returns. If a stock returned 10% per year over the past decade, perhaps it will return roughly 10% going forward. This is the **plug-in principle**: estimate the population parameter with the sample statistic.

The problem is that returns are extremely noisy. Stock returns have annualised volatility around 15-25%, while expected risk premia are only 4-8% per year. The signal-to-noise ratio is abysmal. To estimate a mean return of 6% per year with annualised volatility of 20%, you need:

$$\text{Standard Error} = \frac{\sigma}{\sqrt{T}} = \frac{20\%}{\sqrt{T}}$$

For a standard error of 1%, you would need $T = 400$ years of data. For a standard error of 2%, you still need 100 years. With a realistic 10 years of data, your standard error is about 6.3%—meaning your 95% confidence interval for expected return spans roughly ±12 percentage points.

This is the **mean estimation problem**: the most important parameter in finance is estimated with enormous uncertainty. Volatility, by contrast, is estimated much more precisely because it depends on the magnitude of returns (which are large) rather than their direction (which is nearly random).

The practical implication: **never trust a backtest expected return at face value**. A strategy showing 15% annual returns over 5 years could easily have a true expected return anywhere from -5% to +35% at 95% confidence. The signal-to-noise ratio is simply too low for precision.

### 2.1.2 Visual Evidence

Let's demonstrate the instability of mean estimation with real market data.

```{r rolling-mean-instability, fig.cap="Rolling 3-year mean returns for S&P 500 show enormous variation, ranging from -20% to +30% annualised. The true expected return is obscured by noise."}
# Load S&P 500 data
spy <- load_market("SPY")
spy <- spy[date >= "2000-01-01"]

# Calculate daily log returns
spy[, returns := c(NA, diff(log(adjusted)))]
spy <- spy[!is.na(returns)]

# Calculate rolling 3-year annualised mean return
# 252 trading days per year, 3 years = 756 days
window <- 756

spy[, rolling_mean := frollapply(
    returns,
    n = window,
    FUN = function(x) mean(x) * 252,  # Annualise
    align = "right"
)]

# Plot rolling mean
ggplot(spy[!is.na(rolling_mean)], aes(x = date, y = rolling_mean * 100)) +
    geom_hline(yintercept = 0, colour = "grey50", linetype = "dashed") +
    geom_line(colour = tc[1], linewidth = 0.8) +
    geom_hline(
        yintercept = mean(spy$returns, na.rm = TRUE) * 252 * 100,
        colour = tc[2],
        linetype = "dashed",
        linewidth = 1
    ) +
    annotate(
        "text",
        x = as.Date("2005-01-01"),
        y = mean(spy$returns, na.rm = TRUE) * 252 * 100 + 3,
        label = "Full sample mean",
        colour = tc[2],
        size = 4
    ) +
    labs(
        title = "Rolling 3-Year Mean Return: S&P 500",
        subtitle = "Mean estimation is extremely unstable—3 years of data tells you little",
        x = NULL,
        y = "Annualised Return (%)"
    ) +
    theme_trading() +
    scale_y_continuous(labels = function(x) paste0(x, "%"))
```

The chart shows rolling 3-year mean returns oscillating wildly. At various points, the "expected return" appears to be -20%, 0%, 15%, or 30%—depending entirely on which 3-year window you examine.

```{r confidence-bands, fig.cap="95% confidence intervals for expected return remain wide even with 10 years of data. The uncertainty in mean estimation is irreducible with reasonable sample sizes."}
# Calculate confidence intervals for different sample sizes
sample_years <- c(1, 2, 3, 5, 10, 20, 30, 50)
sample_days <- sample_years * 252

# Use full sample statistics
mu <- mean(spy$returns, na.rm = TRUE) * 252  # Annualised mean
sigma <- sd(spy$returns, na.rm = TRUE) * sqrt(252)  # Annualised vol

# Calculate standard errors and confidence intervals
ci_data <- data.table(
    years = sample_years,
    se = sigma / sqrt(sample_years),
    mu = mu
)
ci_data[, `:=`(
    lower_95 = mu - 1.96 * se,
    upper_95 = mu + 1.96 * se
)]

# Melt for plotting
ci_long <- melt(ci_data,
                id.vars = c("years", "mu", "se"),
                measure.vars = c("lower_95", "upper_95"),
                variable.name = "bound",
                value.name = "return")

ggplot(ci_data, aes(x = years)) +
    geom_ribbon(
        aes(ymin = lower_95 * 100, ymax = upper_95 * 100),
        fill = tc[1],
        alpha = 0.3
    ) +
    geom_hline(yintercept = mu * 100, colour = tc[2], linewidth = 1) +
    geom_hline(yintercept = 0, colour = "grey50", linetype = "dashed") +
    geom_point(aes(y = mu * 100), size = 3, colour = tc[2]) +
    labs(
        title = "95% Confidence Intervals for Expected Return",
        subtitle = sprintf("Point estimate: %.1f%% annualised, volatility: %.1f%%",
                          mu * 100, sigma * 100),
        x = "Years of Data",
        y = "Expected Return (%)"
    ) +
    scale_x_continuous(breaks = sample_years) +
    scale_y_continuous(labels = function(x) paste0(x, "%")) +
    theme_trading() +
    annotate(
        "text",
        x = 35,
        y = -10,
        label = "Even 30 years of data\nleaves substantial uncertainty",
        size = 3.5,
        colour = "grey40"
    )
```

The confidence interval narrows slowly—proportional to $1/\sqrt{T}$. With 10 years of data and 20% volatility, the 95% CI spans roughly 25 percentage points. This is not a failure of your analysis; it is an irreducible feature of noisy return data.

### 2.1.3 Mathematical Derivation

Let $r_1, r_2, \ldots, r_T$ be a sequence of returns. The **sample mean** is:

$$\hat{\mu} = \frac{1}{T} \sum_{t=1}^{T} r_t$$

Under the assumption that returns are i.i.d. (independently and identically distributed) with true mean $\mu$ and variance $\sigma^2$, the sample mean is an unbiased estimator:

$$E[\hat{\mu}] = \mu$$

**Derivation of standard error:**

The variance of the sample mean is:
$$\text{Var}(\hat{\mu}) = \text{Var}\left(\frac{1}{T} \sum_{t=1}^{T} r_t\right)$$

Since returns are assumed independent:
$$= \frac{1}{T^2} \sum_{t=1}^{T} \text{Var}(r_t) = \frac{1}{T^2} \cdot T \cdot \sigma^2 = \frac{\sigma^2}{T}$$

Therefore, the **standard error** of the mean is:
$$SE(\hat{\mu}) = \sqrt{\text{Var}(\hat{\mu})} = \frac{\sigma}{\sqrt{T}}$$

**Confidence interval:**

For large $T$, by the Central Limit Theorem, $\hat{\mu}$ is approximately normally distributed:
$$\hat{\mu} \sim N\left(\mu, \frac{\sigma^2}{T}\right)$$

A 95% confidence interval is:
$$\hat{\mu} \pm 1.96 \cdot \frac{\sigma}{\sqrt{T}}$$

**Annualisation:**

For daily returns with $T_{\text{days}}$ observations over $T_{\text{years}}$ years (where $T_{\text{days}} \approx 252 \cdot T_{\text{years}}$):

- Annualised mean: $\hat{\mu}_{\text{annual}} = 252 \cdot \hat{\mu}_{\text{daily}}$
- Annualised volatility: $\sigma_{\text{annual}} = \sqrt{252} \cdot \sigma_{\text{daily}}$
- Standard error of annualised mean: $SE = \frac{\sigma_{\text{annual}}}{\sqrt{T_{\text{years}}}}$

**Why mean estimation is hard:**

The information content for mean estimation grows with $\sqrt{T}$, not $T$. To halve your standard error, you need four times as much data. Meanwhile, volatility estimation converges much faster because it uses the squared magnitude of each observation.

Consider the signal-to-noise ratio for mean estimation:
$$\text{SNR} = \frac{\mu}{\sigma/\sqrt{T}} = \frac{\mu \sqrt{T}}{\sigma} = \sqrt{T} \cdot \text{Sharpe}$$

For a strategy with Sharpe ratio 0.5, you need $T = 16$ years just to get SNR = 2 (roughly equivalent to a t-statistic of 2).

### 2.1.4 Implementation & Application

```{r mean-estimation-functions}
# Function to calculate sample mean with standard error and confidence interval
estimate_mean_return <- function(returns, conf_level = 0.95, annualise = TRUE, periods_per_year = 252) {
    # Remove NA values
    returns <- returns[!is.na(returns)]
    n <- length(returns)

    # Sample mean and standard deviation
    mu_hat <- mean(returns)
    sigma_hat <- sd(returns)

    # Annualise if requested
    if (annualise) {
        mu_hat <- mu_hat * periods_per_year
        sigma_hat <- sigma_hat * sqrt(periods_per_year)
        n_years <- n / periods_per_year
    } else {
        n_years <- n
    }

    # Standard error (using annualised values)
    se <- sigma_hat / sqrt(n_years)

    # Confidence interval
    z <- qnorm(1 - (1 - conf_level) / 2)
    ci_lower <- mu_hat - z * se
    ci_upper <- mu_hat + z * se

    # t-statistic (is the mean significantly different from zero?)
    t_stat <- mu_hat / se
    p_value <- 2 * (1 - pnorm(abs(t_stat)))

    list(
        mean = mu_hat,
        se = se,
        ci_lower = ci_lower,
        ci_upper = ci_upper,
        t_statistic = t_stat,
        p_value = p_value,
        n_observations = n,
        n_years = n_years,
        volatility = sigma_hat
    )
}

# Test on S&P 500 data
spy_estimate <- estimate_mean_return(spy$returns)

cat("S&P 500 Expected Return Estimation:\n")
cat(sprintf("  Point estimate: %.2f%% annualised\n", spy_estimate$mean * 100))
cat(sprintf("  Standard error: %.2f%%\n", spy_estimate$se * 100))
cat(sprintf("  95%% CI: [%.2f%%, %.2f%%]\n",
            spy_estimate$ci_lower * 100, spy_estimate$ci_upper * 100))
cat(sprintf("  t-statistic: %.2f (p = %.3f)\n",
            spy_estimate$t_statistic, spy_estimate$p_value))
cat(sprintf("  Based on %.1f years of data\n", spy_estimate$n_years))
```

**Bootstrap confidence intervals:**

Bootstrapping provides a non-parametric alternative that doesn't assume normality:

```{r bootstrap-ci, fig.cap="Bootstrap distribution of mean return estimates. The wide spread reflects genuine uncertainty in expected returns."}
# Bootstrap confidence interval for mean return
bootstrap_mean_ci <- function(returns, n_boot = 10000, conf_level = 0.95,
                              annualise = TRUE, periods_per_year = 252) {
    returns <- returns[!is.na(returns)]
    n <- length(returns)

    # Bootstrap: resample with replacement
    boot_means <- numeric(n_boot)
    for (i in 1:n_boot) {
        boot_sample <- sample(returns, n, replace = TRUE)
        boot_means[i] <- mean(boot_sample)
    }

    # Annualise
    if (annualise) {
        boot_means <- boot_means * periods_per_year
    }

    # Percentile confidence interval
    alpha <- 1 - conf_level
    ci <- quantile(boot_means, c(alpha/2, 1 - alpha/2))

    list(
        mean = mean(boot_means),
        ci_lower = ci[1],
        ci_upper = ci[2],
        boot_distribution = boot_means
    )
}

# Run bootstrap
boot_result <- bootstrap_mean_ci(spy$returns, n_boot = 10000)

# Plot bootstrap distribution
boot_dt <- data.table(mean_return = boot_result$boot_distribution * 100)

ggplot(boot_dt, aes(x = mean_return)) +
    geom_histogram(
        bins = 50,
        fill = tc[1],
        colour = "white",
        alpha = 0.8
    ) +
    geom_vline(
        xintercept = mean(boot_dt$mean_return),
        colour = tc[2],
        linewidth = 1.2
    ) +
    geom_vline(
        xintercept = boot_result$ci_lower * 100,
        colour = tc[3],
        linetype = "dashed",
        linewidth = 1
    ) +
    geom_vline(
        xintercept = boot_result$ci_upper * 100,
        colour = tc[3],
        linetype = "dashed",
        linewidth = 1
    ) +
    labs(
        title = "Bootstrap Distribution of Mean Return Estimate",
        subtitle = sprintf("95%% CI: [%.1f%%, %.1f%%]",
                          boot_result$ci_lower * 100, boot_result$ci_upper * 100),
        x = "Annualised Mean Return (%)",
        y = "Frequency"
    ) +
    theme_trading()
```

**Practical implications for trading:**

1. **Backtest returns are estimates with wide uncertainty.** A strategy with 15% backtest returns and 20% volatility over 5 years has a 95% CI roughly [−3%, 33%].

2. **Don't optimise on expected returns.** The estimation error is too large. Optimise on Sharpe ratio or risk-adjusted metrics, which are more stable.

3. **Diversify across uncorrelated strategies.** The mean of means is estimated with lower error than any single mean if strategies are independent.

4. **Use Bayesian shrinkage.** Shrink extreme mean estimates toward a prior (like the market return) to reduce estimation error.

---

## 2.2 Trade-Level Statistics

Not all strategies generate continuous exposure. Many systematic strategies take discrete trades with clear entry and exit points. For these, trade-level statistics provide an alternative lens into performance.

### 2.2.1 Prose/Intuition

Return-based analysis treats your P&L as a continuous stream. Trade-based analysis treats it as a sequence of bets. Each trade has a win/loss outcome and a magnitude. The psychology is different: traders naturally think in terms of "winning trades" and "losing trades."

The key statistics are:

- **Win rate**: What percentage of trades are profitable?
- **Average win**: When you win, how much do you win on average?
- **Average loss**: When you lose, how much do you lose on average?
- **Profit factor**: Gross profits divided by gross losses.
- **Expectancy**: Expected profit per trade.

The relationship between these metrics determines profitability:

$$\text{Expectancy} = (\text{Win Rate} \times \text{Avg Win}) - (\text{Loss Rate} \times \text{Avg Loss})$$

A strategy with 40% win rate can be highly profitable if average wins are 3× average losses. Conversely, a strategy with 70% win rate can lose money if average losses are 3× average wins.

**When to use trade-based analysis:**

- Strategies with discrete entry/exit points (mean reversion, breakouts)
- High-frequency strategies with many trades
- Diagnosing strategy behaviour (why is it losing?)

**When return-based analysis is better:**

- Continuous exposure strategies (trend following with gradual position changes)
- Multi-asset portfolios with rebalancing
- Comparison across strategies with different trade frequencies

### 2.2.2 Visual Evidence

```{r simulate-trades, fig.cap="Trade P&L distributions for two strategies with identical expected returns but different win rates. The psychological experience differs dramatically."}
# Simulate two strategies with same expectancy but different profiles
set.seed(123)

# Strategy A: High win rate, small average win, larger average loss
# Win rate: 70%, Avg win: 1%, Avg loss: 2%
# Expectancy: 0.7 * 1% - 0.3 * 2% = 0.7% - 0.6% = 0.1%
n_trades_a <- 200
wins_a <- rbinom(1, n_trades_a, 0.70)
losses_a <- n_trades_a - wins_a
trades_a <- c(
    rnorm(wins_a, mean = 0.01, sd = 0.003),
    rnorm(losses_a, mean = -0.02, sd = 0.006)
)
trades_a <- sample(trades_a)  # Randomise order

# Strategy B: Low win rate, large average win, small average loss
# Win rate: 35%, Avg win: 2.5%, Avg loss: 0.8%
# Expectancy: 0.35 * 2.5% - 0.65 * 0.8% = 0.875% - 0.52% = 0.355%
n_trades_b <- 200
wins_b <- rbinom(1, n_trades_b, 0.35)
losses_b <- n_trades_b - wins_b
trades_b <- c(
    rnorm(wins_b, mean = 0.025, sd = 0.008),
    rnorm(losses_b, mean = -0.008, sd = 0.003)
)
trades_b <- sample(trades_b)

# Combine for plotting
trade_dt <- rbind(
    data.table(strategy = "A: High Win Rate (70%)", pnl = trades_a * 100),
    data.table(strategy = "B: Low Win Rate (35%)", pnl = trades_b * 100)
)

ggplot(trade_dt, aes(x = pnl, fill = strategy)) +
    geom_histogram(bins = 40, alpha = 0.7, position = "identity") +
    geom_vline(xintercept = 0, linetype = "dashed", colour = "grey50") +
    facet_wrap(~strategy, ncol = 1, scales = "free_y") +
    scale_fill_manual(values = tc[1:2]) +
    labs(
        title = "Trade P&L Distributions: High vs Low Win Rate Strategies",
        subtitle = "Both strategies can be profitable—psychology differs",
        x = "Trade P&L (%)",
        y = "Frequency"
    ) +
    theme_trading() +
    theme(legend.position = "none")
```

```{r trade-stats-comparison}
# Function to calculate trade statistics
calculate_trade_stats <- function(trades) {
    wins <- trades[trades > 0]
    losses <- trades[trades < 0]

    n_trades <- length(trades)
    n_wins <- length(wins)
    n_losses <- length(losses)

    win_rate <- n_wins / n_trades
    avg_win <- if (n_wins > 0) mean(wins) else 0
    avg_loss <- if (n_losses > 0) mean(losses) else 0

    gross_profit <- sum(wins)
    gross_loss <- abs(sum(losses))
    profit_factor <- if (gross_loss > 0) gross_profit / gross_loss else Inf

    expectancy <- mean(trades)

    list(
        n_trades = n_trades,
        win_rate = win_rate,
        avg_win = avg_win,
        avg_loss = avg_loss,
        profit_factor = profit_factor,
        expectancy = expectancy,
        total_return = sum(trades)
    )
}

# Calculate stats for both strategies
stats_a <- calculate_trade_stats(trades_a)
stats_b <- calculate_trade_stats(trades_b)

cat("Strategy A (High Win Rate):\n")
cat(sprintf("  Win rate: %.1f%%\n", stats_a$win_rate * 100))
cat(sprintf("  Average win: %.2f%%\n", stats_a$avg_win * 100))
cat(sprintf("  Average loss: %.2f%%\n", stats_a$avg_loss * 100))
cat(sprintf("  Profit factor: %.2f\n", stats_a$profit_factor))
cat(sprintf("  Expectancy per trade: %.3f%%\n", stats_a$expectancy * 100))
cat(sprintf("  Total return: %.2f%%\n\n", stats_a$total_return * 100))

cat("Strategy B (Low Win Rate):\n")
cat(sprintf("  Win rate: %.1f%%\n", stats_b$win_rate * 100))
cat(sprintf("  Average win: %.2f%%\n", stats_b$avg_win * 100))
cat(sprintf("  Average loss: %.2f%%\n", stats_b$avg_loss * 100))
cat(sprintf("  Profit factor: %.2f\n", stats_b$profit_factor))
cat(sprintf("  Expectancy per trade: %.3f%%\n", stats_b$expectancy * 100))
cat(sprintf("  Total return: %.2f%%\n", stats_b$total_return * 100))
```

```{r cumulative-trades, fig.cap="Cumulative P&L for high vs low win rate strategies. Strategy B (low win rate) has more losing streaks but larger eventual gains."}
# Calculate cumulative P&L
cum_dt <- rbind(
    data.table(
        strategy = "A: High Win Rate (70%)",
        trade = 1:length(trades_a),
        cum_pnl = cumsum(trades_a) * 100
    ),
    data.table(
        strategy = "B: Low Win Rate (35%)",
        trade = 1:length(trades_b),
        cum_pnl = cumsum(trades_b) * 100
    )
)

ggplot(cum_dt, aes(x = trade, y = cum_pnl, colour = strategy)) +
    geom_line(linewidth = 1) +
    geom_hline(yintercept = 0, linetype = "dashed", colour = "grey50") +
    scale_colour_manual(values = tc[1:2]) +
    labs(
        title = "Cumulative P&L by Trade Number",
        subtitle = "Low win rate doesn't mean unprofitable",
        x = "Trade Number",
        y = "Cumulative P&L (%)"
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

### 2.2.3 Mathematical Derivation

**Expectancy derivation:**

Let $W$ be the indicator for a winning trade (1 if win, 0 if loss), and let $X$ be the P&L of a trade. Define:

- $p = P(W = 1)$ = win rate
- $\mu_w = E[X | W = 1]$ = average win
- $\mu_l = E[X | W = 0]$ = average loss (negative)

By the law of total expectation:

$$E[X] = E[X | W = 1] P(W = 1) + E[X | W = 0] P(W = 0)$$
$$= \mu_w \cdot p + \mu_l \cdot (1 - p)$$
$$= p \cdot \mu_w - (1 - p) \cdot |\mu_l|$$

This is the **expectancy**: the expected profit per trade.

**Break-even win rate:**

For a strategy to be profitable, $E[X] > 0$:
$$p \cdot \mu_w > (1 - p) \cdot |\mu_l|$$
$$p > \frac{|\mu_l|}{\mu_w + |\mu_l|}$$

If your average win is £2 and average loss is £1, you need a win rate above $1/(2+1) = 33.3\%$ to break even.

**Profit factor:**

$$\text{Profit Factor} = \frac{\sum \text{wins}}{|\sum \text{losses}|} = \frac{n_w \cdot \mu_w}{n_l \cdot |\mu_l|}$$

where $n_w$ and $n_l$ are the number of winning and losing trades.

This can be related to win rate and average win/loss:
$$\text{Profit Factor} = \frac{p \cdot \mu_w}{(1-p) \cdot |\mu_l|}$$

A profit factor > 1 means the strategy is profitable.

**Relationship to Sharpe ratio:**

For a strategy with $N$ trades over $T$ years, the approximate relationship is:

$$\text{Sharpe} \approx \frac{E[X] \cdot N/T}{\sigma_X \cdot \sqrt{N/T}} = \frac{E[X] \cdot \sqrt{N/T}}{\sigma_X}$$

where $\sigma_X$ is the standard deviation of trade P&L.

The Sharpe ratio depends on both expectancy and the volatility of individual trades, as well as trade frequency.

### 2.2.4 Implementation & Application

```{r trade-extraction}
# Function to extract trades from a position and return series
# A "trade" is defined as a round-trip: entry to exit
extract_trades <- function(positions, returns, prices = NULL) {
    # Ensure data.table
    dt <- data.table(
        position = positions,
        returns = returns
    )
    if (!is.null(prices)) dt$price <- prices

    dt[, idx := .I]

    # Identify position changes
    dt[, pos_change := c(0, diff(sign(position)))]

    # Mark trade entry points (position goes from 0 to non-zero)
    dt[, trade_entry := position != 0 & shift(position, 1, fill = 0) == 0]

    # Mark trade exit points (position goes to 0)
    dt[, trade_exit := position == 0 & shift(position, 1, fill = 0) != 0]

    # Assign trade IDs
    dt[, trade_id := cumsum(trade_entry)]
    dt[trade_id == 0, trade_id := NA]

    # Calculate P&L for each day in position
    dt[, daily_pnl := shift(position, 1, fill = 0) * returns]

    # Aggregate to trade level
    trades <- dt[!is.na(trade_id), .(
        entry_date = min(idx),
        exit_date = max(idx),
        duration = .N,
        pnl = sum(daily_pnl, na.rm = TRUE)
    ), by = trade_id]

    trades$pnl
}

# Comprehensive trade statistics function
comprehensive_trade_stats <- function(trades) {
    if (length(trades) == 0) return(NULL)

    wins <- trades[trades > 0]
    losses <- trades[trades < 0]

    n_trades <- length(trades)
    n_wins <- length(wins)
    n_losses <- length(losses)
    n_flat <- n_trades - n_wins - n_losses

    # Basic stats
    win_rate <- n_wins / n_trades
    avg_win <- if (n_wins > 0) mean(wins) else NA
    avg_loss <- if (n_losses > 0) mean(losses) else NA

    # Profit factor
    gross_profit <- sum(wins)
    gross_loss <- abs(sum(losses))
    profit_factor <- if (gross_loss > 0) gross_profit / gross_loss else Inf

    # Expectancy and its standard error
    expectancy <- mean(trades)
    expectancy_se <- sd(trades) / sqrt(n_trades)

    # Max consecutive wins/losses
    signs <- sign(trades)
    rle_result <- rle(signs)

    max_consec_wins <- max(c(0, rle_result$lengths[rle_result$values == 1]))
    max_consec_losses <- max(c(0, rle_result$lengths[rle_result$values == -1]))

    # Largest win/loss
    max_win <- if (n_wins > 0) max(wins) else NA
    max_loss <- if (n_losses > 0) min(losses) else NA

    list(
        n_trades = n_trades,
        n_wins = n_wins,
        n_losses = n_losses,
        win_rate = win_rate,
        avg_win = avg_win,
        avg_loss = avg_loss,
        profit_factor = profit_factor,
        expectancy = expectancy,
        expectancy_se = expectancy_se,
        total_return = sum(trades),
        max_consecutive_wins = max_consec_wins,
        max_consecutive_losses = max_consec_losses,
        largest_win = max_win,
        largest_loss = max_loss
    )
}

# Example: Generate a simple mean-reversion signal and extract trades
spy[, ma_20 := frollmean(adjusted, 20)]
spy[, zscore := (adjusted - ma_20) / frollapply(adjusted, 20, sd)]
spy[, signal := fifelse(zscore < -2, 1, fifelse(zscore > 0, 0, NA_real_))]
spy[, signal := nafill(signal, type = "locf")]
spy[is.na(signal), signal := 0]

# Extract trades
mean_rev_trades <- extract_trades(spy$signal, spy$returns)

# Calculate stats
if (length(mean_rev_trades) > 0) {
    mean_rev_stats <- comprehensive_trade_stats(mean_rev_trades)

    cat("\nMean Reversion Strategy Trade Statistics:\n")
    cat(sprintf("  Number of trades: %d\n", mean_rev_stats$n_trades))
    cat(sprintf("  Win rate: %.1f%%\n", mean_rev_stats$win_rate * 100))
    cat(sprintf("  Average win: %.2f%%\n", mean_rev_stats$avg_win * 100))
    cat(sprintf("  Average loss: %.2f%%\n", mean_rev_stats$avg_loss * 100))
    cat(sprintf("  Profit factor: %.2f\n", mean_rev_stats$profit_factor))
    cat(sprintf("  Expectancy: %.3f%% ± %.3f%%\n",
                mean_rev_stats$expectancy * 100, mean_rev_stats$expectancy_se * 100))
    cat(sprintf("  Max consecutive wins: %d\n", mean_rev_stats$max_consecutive_wins))
    cat(sprintf("  Max consecutive losses: %d\n", mean_rev_stats$max_consecutive_losses))
}
```

**Using trade stats for diagnosis:**

```{r trade-diagnosis}
# Function to diagnose strategy issues from trade statistics
diagnose_strategy <- function(stats) {
    issues <- character(0)

    # Low win rate with small wins = problem
    if (stats$win_rate < 0.35 && stats$avg_win < abs(stats$avg_loss) * 1.5) {
        issues <- c(issues, "Low win rate without compensating large wins")
    }

    # High win rate with large losses = problem
    if (stats$win_rate > 0.65 && abs(stats$avg_loss) > stats$avg_win * 2) {
        issues <- c(issues, "High win rate undermined by occasional large losses")
    }

    # Profit factor close to 1
    if (stats$profit_factor < 1.3 && stats$profit_factor > 0.7) {
        issues <- c(issues, "Profit factor near breakeven - fragile to costs/slippage")
    }

    # Large expectancy standard error
    if (stats$expectancy_se > abs(stats$expectancy) * 0.5) {
        issues <- c(issues, "Expectancy highly uncertain - need more trades")
    }

    if (length(issues) == 0) {
        "Strategy trade profile looks healthy"
    } else {
        paste("Potential issues:", paste(issues, collapse = "; "))
    }
}

if (exists("mean_rev_stats")) {
    cat("\nDiagnosis:\n", diagnose_strategy(mean_rev_stats), "\n")
}
```

---

## Quick Reference: Return Metrics

### Expected Return Estimation

| Metric | Formula | Notes |
|--------|---------|-------|
| Sample mean | $\hat{\mu} = \frac{1}{T} \sum r_t$ | Unbiased estimator |
| Standard error | $SE = \frac{\sigma}{\sqrt{T}}$ | Annualised: $\frac{\sigma_{\text{ann}}}{\sqrt{T_{\text{years}}}}$ |
| 95% CI | $\hat{\mu} \pm 1.96 \cdot SE$ | Wide! Often ±6-12% |
| t-statistic | $t = \frac{\hat{\mu}}{SE}$ | Need $t > 2$ for significance |

### Trade-Level Statistics

| Metric | Formula | Interpretation |
|--------|---------|---------------|
| Win rate | $\frac{n_{\text{wins}}}{n_{\text{trades}}}$ | Higher isn't always better |
| Expectancy | $p \cdot \mu_w - (1-p) \cdot |\mu_l|$ | Expected profit per trade |
| Profit factor | $\frac{\sum \text{wins}}{|\sum \text{losses}|}$ | > 1 means profitable |
| Break-even win rate | $\frac{|\mu_l|}{\mu_w + |\mu_l|}$ | Minimum win rate for profit |

### R Code Snippets

```r
# Sample mean with confidence interval
mean_return <- mean(returns, na.rm = TRUE)
se <- sd(returns, na.rm = TRUE) / sqrt(length(returns))
ci <- mean_return + c(-1, 1) * 1.96 * se

# Annualise (daily to annual)
annual_mean <- mean_return * 252
annual_vol <- sd(returns, na.rm = TRUE) * sqrt(252)
annual_se <- annual_vol / sqrt(length(returns) / 252)

# Bootstrap mean (non-parametric)
boot_means <- replicate(10000, mean(sample(returns, replace = TRUE)))
boot_ci <- quantile(boot_means, c(0.025, 0.975))

# Trade statistics
wins <- trades[trades > 0]
losses <- trades[trades < 0]
win_rate <- length(wins) / length(trades)
expectancy <- mean(trades)
profit_factor <- sum(wins) / abs(sum(losses))
```

### Key Takeaways

1. **Mean estimation is extremely uncertain.** A 95% CI for expected return with 10 years of data and 20% volatility spans about 25 percentage points.

2. **Don't over-interpret backtest returns.** The point estimate is one draw from a wide distribution.

3. **Trade-based and return-based analysis give different perspectives.** Use trade analysis for discrete strategies, return analysis for continuous exposure.

4. **Win rate alone means nothing.** A 30% win rate can be highly profitable; a 70% win rate can lose money. Expectancy is what matters.

5. **Profit factor > 1 is necessary but not sufficient.** A profit factor of 1.1 can easily become < 1 after transaction costs.
