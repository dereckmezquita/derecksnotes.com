---
title: "Algorithmic Trading with R"
chapter: "Chapter 2: Risk and Performance Metrics"
part: "Part 2: Measuring Risk"
section: "02-2"
coverImage: 13
author: "Dereck Mezquita"
date: 2026-01-21
tags: [algorithmic-trading, quantitative-finance, R, statistics, risk, volatility, VaR]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# HTML5 figure hook for accessibility
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(
    dpi = 300,
    fig.width = 10,
    fig.height = 7,
    comment = "",
    warning = FALSE,
    collapse = FALSE,
    results = 'hold'
)

options(box.path = getwd())
```

# Part 2: Measuring Risk

Returns tell you how much you made; risk measures tell you how much pain you endured to get there. A strategy returning 20% annually is worthless if you had to suffer 60% drawdowns to achieve it—you would have abandoned it long before reaching profitability.

This chapter covers the standard risk measures used in quantitative finance: volatility, downside measures, drawdown, Value-at-Risk, and Expected Shortfall. Each captures a different aspect of risk, and understanding their relationships helps you evaluate strategies honestly.

```{r load-modules, message=FALSE}
box::use(
    ./modules/data[load_market, load_factors, filter_dates],
    ./modules/stats[sharpe_ratio, annualised_return, annualised_vol, max_drawdown],
    ./modules/viz[theme_trading, trading_colors]
)

box::use(
    data.table[...],
    ggplot2[...]
)

# Helper: convert trading_colors list to vector for indexed access
tc <- unlist(trading_colors)

set.seed(42)

# Load data for examples
spy <- load_market("SPY")
spy <- spy[date >= "2000-01-01"]
spy[, returns := c(NA, diff(log(adjusted)))]
spy <- spy[!is.na(returns)]
```

---

## 2.3 Volatility

Volatility is the standard deviation of returns—the most widely used risk measure in finance. It captures dispersion around the mean and forms the denominator of the Sharpe ratio.

### 2.3.1 Prose/Intuition

When Harry Markowitz developed modern portfolio theory in 1952, he needed a single number to represent risk. He chose variance (or its square root, standard deviation), which became enshrined as the default measure.

Why standard deviation? Several reasons:

1. **Mathematical tractability.** Variance has nice properties under addition: $\text{Var}(A + B) = \text{Var}(A) + \text{Var}(B) + 2\text{Cov}(A, B)$. This makes portfolio optimisation possible.

2. **Estimation stability.** Unlike mean returns, volatility can be estimated reasonably precisely with modest data. With daily returns, a year of data gives a reasonably stable volatility estimate.

3. **Interpretation.** Under normality, volatility tells you the range of typical outcomes. Roughly 68% of observations fall within ±1σ, 95% within ±2σ.

The limitation is that volatility treats upside and downside equally. A strategy that frequently returns +5% contributes the same to volatility as one that frequently returns -5%. Investors don't actually treat these symmetrically—they love upside volatility and hate downside volatility.

Despite this limitation, volatility remains the industry standard because:
- It's what everyone else uses, enabling comparison
- It's embedded in regulation and risk limits
- Alternatives (like semi-deviation) are less tractable mathematically

### 2.3.2 Visual Evidence

```{r vol-comparison, fig.cap="High volatility vs low volatility return series. Both have similar cumulative returns, but the paths—and the investor experience—differ dramatically."}
# Generate two return series with different volatility
n <- 1000
set.seed(123)

# Low vol: 10% annual vol
low_vol_returns <- rnorm(n, mean = 0.0004, sd = 0.10 / sqrt(252))

# High vol: 25% annual vol, same expected return
high_vol_returns <- rnorm(n, mean = 0.0004, sd = 0.25 / sqrt(252))

# Create data.table for plotting
vol_dt <- data.table(
    day = rep(1:n, 2),
    returns = c(low_vol_returns, high_vol_returns),
    strategy = rep(c("Low Volatility (10%)", "High Volatility (25%)"), each = n)
)

vol_dt[, cumulative := cumsum(returns), by = strategy]

ggplot(vol_dt, aes(x = day, y = cumulative * 100, colour = strategy)) +
    geom_line(linewidth = 0.8) +
    scale_colour_manual(values = tc[1:2]) +
    labs(
        title = "Cumulative Returns: Low vs High Volatility Paths",
        subtitle = "Same expected return, different investor experience",
        x = "Trading Day",
        y = "Cumulative Return (%)",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

```{r rolling-vol, fig.cap="Rolling 60-day volatility of S&P 500 shows distinct regimes: calm periods around 10% and crisis periods exceeding 50%."}
# Calculate rolling volatility
spy[, vol_20 := frollapply(returns, 20, sd) * sqrt(252)]
spy[, vol_60 := frollapply(returns, 60, sd) * sqrt(252)]

# Plot rolling volatility
ggplot(spy[!is.na(vol_60)], aes(x = date, y = vol_60 * 100)) +
    geom_line(colour = tc[1], linewidth = 0.5) +
    geom_hline(yintercept = c(15, 25), linetype = "dashed", colour = "grey50") +
    annotate("rect", xmin = as.Date("2008-09-01"), xmax = as.Date("2009-06-01"),
             ymin = 0, ymax = 90, alpha = 0.2, fill = tc[3]) +
    annotate("rect", xmin = as.Date("2020-03-01"), xmax = as.Date("2020-06-01"),
             ymin = 0, ymax = 90, alpha = 0.2, fill = tc[3]) +
    annotate("text", x = as.Date("2009-01-01"), y = 85, label = "2008\nCrisis",
             size = 3, colour = "grey40") +
    annotate("text", x = as.Date("2020-04-15"), y = 85, label = "COVID",
             size = 3, colour = "grey40") +
    labs(
        title = "Rolling 60-Day Volatility: S&P 500",
        subtitle = "Volatility clusters—calm and stormy regimes are persistent",
        x = NULL,
        y = "Annualised Volatility (%)"
    ) +
    theme_trading() +
    scale_y_continuous(labels = function(x) paste0(x, "%"))
```

### 2.3.3 Mathematical Derivation

**Sample variance definition:**

For returns $r_1, r_2, \ldots, r_T$, the sample variance is:

$$\hat{\sigma}^2 = \frac{1}{T-1} \sum_{t=1}^{T} (r_t - \bar{r})^2$$

where $\bar{r} = \frac{1}{T}\sum r_t$ is the sample mean.

**Why $T-1$ (Bessel's correction)?**

We want an unbiased estimator: $E[\hat{\sigma}^2] = \sigma^2$. With divisor $T$, the estimator is biased downward. Here's the proof:

Let $S = \sum_{t=1}^T (r_t - \bar{r})^2$. We can rewrite this as:
$$S = \sum_{t=1}^T (r_t - \mu + \mu - \bar{r})^2 = \sum_{t=1}^T (r_t - \mu)^2 - T(\bar{r} - \mu)^2$$

Taking expectations:
$$E[S] = \sum_{t=1}^T E[(r_t - \mu)^2] - T \cdot E[(\bar{r} - \mu)^2]$$
$$= T\sigma^2 - T \cdot \frac{\sigma^2}{T} = T\sigma^2 - \sigma^2 = (T-1)\sigma^2$$

Therefore:
$$E\left[\frac{S}{T-1}\right] = \sigma^2$$

Division by $T-1$ gives an unbiased estimator.

**Annualisation:**

If daily returns are i.i.d., the variance of the sum of $n$ daily returns is:
$$\text{Var}\left(\sum_{i=1}^n r_i\right) = n \cdot \sigma^2_{\text{daily}}$$

For annual volatility (252 trading days):
$$\sigma^2_{\text{annual}} = 252 \cdot \sigma^2_{\text{daily}}$$
$$\sigma_{\text{annual}} = \sqrt{252} \cdot \sigma_{\text{daily}}$$

The $\sqrt{252}$ scaling assumes returns are independent. In practice, volatility clustering causes slight positive autocorrelation in squared returns, which can make this approximation imperfect.

**EWMA (Exponentially Weighted Moving Average) volatility:**

Simple historical volatility weights all observations equally. EWMA gives more weight to recent observations:

$$\sigma^2_t = \lambda \sigma^2_{t-1} + (1 - \lambda) r^2_{t-1}$$

where $\lambda \in (0, 1)$ is the decay factor. Common choices are $\lambda = 0.94$ (RiskMetrics standard) or $\lambda = 0.97$.

The effective sample size is approximately $\frac{1}{1-\lambda}$. With $\lambda = 0.94$, this is about 17 days.

### 2.3.4 Implementation & Application

```{r volatility-functions}
# Sample volatility with annualisation
sample_volatility <- function(returns, annualise = TRUE, periods_per_year = 252) {
    returns <- returns[!is.na(returns)]
    vol <- sd(returns)
    if (annualise) vol <- vol * sqrt(periods_per_year)
    vol
}

# EWMA volatility
ewma_volatility <- function(returns, lambda = 0.94, annualise = TRUE,
                            periods_per_year = 252) {
    returns <- returns[!is.na(returns)]
    n <- length(returns)

    # Initialise with sample variance of first 20 observations
    init_var <- var(returns[1:min(20, n)])
    ewma_var <- numeric(n)
    ewma_var[1] <- init_var

    for (t in 2:n) {
        ewma_var[t] <- lambda * ewma_var[t-1] + (1 - lambda) * returns[t-1]^2
    }

    ewma_vol <- sqrt(ewma_var)
    if (annualise) ewma_vol <- ewma_vol * sqrt(periods_per_year)
    ewma_vol
}

# Rolling volatility
rolling_volatility <- function(returns, window = 20, annualise = TRUE,
                               periods_per_year = 252) {
    vol <- frollapply(returns, window, sd, align = "right")
    if (annualise) vol <- vol * sqrt(periods_per_year)
    vol
}

# Calculate and compare
spy[, hist_vol := sample_volatility(returns, annualise = TRUE)]
spy[, ewma_vol := ewma_volatility(returns, lambda = 0.94)]
spy[, roll_vol := rolling_volatility(returns, window = 20)]

cat("S&P 500 Volatility Measures:\n")
cat(sprintf("  Full-sample historical: %.1f%%\n", spy$hist_vol[1] * 100))
cat(sprintf("  Current EWMA (λ=0.94): %.1f%%\n", tail(spy$ewma_vol, 1) * 100))
cat(sprintf("  Current 20-day rolling: %.1f%%\n", tail(spy$roll_vol, 1) * 100))
```

```{r vol-measures-comparison, fig.cap="Comparison of volatility measures: rolling vs EWMA. EWMA reacts more quickly to new information."}
# Compare rolling and EWMA
vol_compare <- spy[date >= "2018-01-01", .(date, roll_vol, ewma_vol)]
vol_compare <- melt(vol_compare, id.vars = "date",
                    measure.vars = c("roll_vol", "ewma_vol"),
                    variable.name = "method", value.name = "volatility")

vol_compare[, method := fifelse(method == "roll_vol",
                                "20-Day Rolling",
                                "EWMA (λ=0.94)")]

ggplot(vol_compare, aes(x = date, y = volatility * 100, colour = method)) +
    geom_line(linewidth = 0.8) +
    scale_colour_manual(values = tc[1:2]) +
    labs(
        title = "Volatility Measures Comparison",
        subtitle = "EWMA responds faster to changes in market conditions",
        x = NULL,
        y = "Annualised Volatility (%)",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

**Volatility targeting for position sizing:**

A common application is volatility targeting: scaling position size to maintain constant risk.

```{r vol-targeting}
# Volatility targeting example
# Target: 15% annual volatility
target_vol <- 0.15

# Calculate position scaling based on recent volatility
spy[, vol_forecast := shift(ewma_vol, 1)]  # Use yesterday's forecast
spy[, scaling := target_vol / vol_forecast]
spy[scaling > 3, scaling := 3]  # Cap leverage at 3x
spy[scaling < 0.25, scaling := 0.25]  # Floor at 0.25x

# Calculate vol-targeted returns
spy[, targeted_returns := returns * scaling]

# Compare
unscaled_vol <- sample_volatility(spy$returns)
scaled_vol <- sample_volatility(spy[!is.na(targeted_returns), targeted_returns])

cat("\nVolatility Targeting Results:\n")
cat(sprintf("  Unscaled volatility: %.1f%%\n", unscaled_vol * 100))
cat(sprintf("  Vol-targeted volatility: %.1f%%\n", scaled_vol * 100))
cat(sprintf("  Target: %.1f%%\n", target_vol * 100))
```

---

## 2.4 Downside Risk Measures

Standard volatility treats upside and downside equally. Downside risk measures focus on what investors actually dislike: losses.

### 2.4.1 Prose/Intuition

Consider two strategies:
- Strategy A: Returns +10% or -10% with equal probability
- Strategy B: Returns +20% or 0% with equal probability

Both have the same expected return (0% and 10% excess respectively... well not quite, but the point is about volatility). Strategy A has higher volatility, yet Strategy B is clearly preferable—it never loses money.

Standard volatility penalises both strategies equally for their dispersion. Downside measures recognise that dispersion above a target is desirable ("good volatility") while dispersion below the target is undesirable ("bad volatility").

The key downside measures are:

1. **Semi-deviation**: Standard deviation computed only on returns below the mean (or below zero, or below a target).

2. **Lower Partial Moments (LPM)**: Generalisation of semi-deviation that allows different powers and different targets.

3. **Downside deviation**: Semi-deviation relative to zero or to the risk-free rate.

These measures are particularly relevant for:
- Strategies with asymmetric returns (options, trend following)
- Risk management where losses matter more than gains
- Utility-based portfolio optimisation

### 2.4.2 Visual Evidence

```{r asymmetric-strategies, fig.cap="Two distributions with identical volatility but different downside profiles. Semi-deviation distinguishes them."}
set.seed(456)

# Strategy A: Symmetric (normal) returns
returns_symmetric <- rnorm(5000, mean = 0, sd = 0.02)

# Strategy B: Positive skew (truncated normal + gamma)
# Match the same volatility but with positive skew
returns_skewed <- c(
    rnorm(3500, mean = -0.005, sd = 0.01),  # Small losses, frequent
    rgamma(1500, shape = 2, rate = 100)  # Occasional large gains
)
# Scale to match volatility
returns_skewed <- returns_skewed * (sd(returns_symmetric) / sd(returns_skewed))

# Create data for plotting
dist_dt <- rbind(
    data.table(strategy = "Symmetric (Normal)", returns = returns_symmetric * 100),
    data.table(strategy = "Positive Skew", returns = returns_skewed * 100)
)

ggplot(dist_dt, aes(x = returns, fill = strategy)) +
    geom_histogram(bins = 60, alpha = 0.6, position = "identity") +
    geom_vline(xintercept = 0, linetype = "dashed", colour = "grey50") +
    scale_fill_manual(values = tc[1:2]) +
    facet_wrap(~strategy, ncol = 1) +
    labs(
        title = "Same Volatility, Different Downside Profiles",
        subtitle = "Standard deviation is identical; semi-deviation reveals the difference",
        x = "Daily Return (%)",
        y = "Frequency"
    ) +
    theme_trading() +
    theme(legend.position = "none")

# Calculate statistics
cat("\nSymmetric Returns:\n")
cat(sprintf("  Volatility: %.2f%%\n", sd(returns_symmetric) * 100))
cat(sprintf("  Semi-deviation (below 0): %.2f%%\n",
            sd(returns_symmetric[returns_symmetric < 0]) * 100))
cat(sprintf("  Skewness: %.2f\n", moments::skewness(returns_symmetric)))

cat("\nPositively Skewed Returns:\n")
cat(sprintf("  Volatility: %.2f%%\n", sd(returns_skewed) * 100))
cat(sprintf("  Semi-deviation (below 0): %.2f%%\n",
            sd(returns_skewed[returns_skewed < 0]) * 100))
cat(sprintf("  Skewness: %.2f\n", moments::skewness(returns_skewed)))
```

### 2.4.3 Mathematical Derivation

**Semi-deviation:**

The semi-deviation relative to a threshold $\tau$ is:

$$\sigma^-_\tau = \sqrt{E[\min(r - \tau, 0)^2]} = \sqrt{\frac{1}{T}\sum_{t=1}^T \min(r_t - \tau, 0)^2}$$

Common choices for $\tau$:
- $\tau = 0$: Downside deviation relative to zero
- $\tau = \bar{r}$: Downside deviation relative to mean
- $\tau = r_f$: Downside deviation relative to risk-free rate

**Lower Partial Moments:**

The $n$th lower partial moment relative to threshold $\tau$ is:

$$\text{LPM}_n(\tau) = E[\max(\tau - r, 0)^n] = \frac{1}{T}\sum_{t=1}^T \max(\tau - r_t, 0)^n$$

Special cases:
- $n = 0$: Shortfall probability $P(r < \tau)$
- $n = 1$: Expected shortfall below $\tau$
- $n = 2$: Semi-variance (squared semi-deviation)

**Connection to utility theory:**

Lower partial moments arise naturally from mean-lower partial moment utility:

$$U = E[r] - \lambda \cdot \text{LPM}_n(\tau)$$

The parameter $n$ reflects risk aversion in the tails. Higher $n$ penalises large losses more severely.

**Relationship to standard deviation:**

Under normality, semi-deviation equals $\sigma/\sqrt{2} \approx 0.707\sigma$ (for $\tau = \mu$). The ratio of downside deviation to total standard deviation indicates asymmetry. Ratios significantly below 0.707 suggest positive skew; ratios above suggest negative skew.

### 2.4.4 Implementation & Application

```{r downside-functions}
# Semi-deviation
semi_deviation <- function(returns, threshold = 0, annualise = TRUE,
                           periods_per_year = 252) {
    returns <- returns[!is.na(returns)]

    # Keep only returns below threshold
    downside <- returns[returns < threshold]

    if (length(downside) == 0) return(0)

    semi_dev <- sqrt(mean((downside - threshold)^2))
    if (annualise) semi_dev <- semi_dev * sqrt(periods_per_year)
    semi_dev
}

# Lower Partial Moment
lower_partial_moment <- function(returns, threshold = 0, n = 2,
                                  annualise = TRUE, periods_per_year = 252) {
    returns <- returns[!is.na(returns)]

    # Calculate LPM
    shortfalls <- pmax(threshold - returns, 0)
    lpm <- mean(shortfalls^n)

    # For n=2, we can annualise like variance
    if (annualise && n == 2) lpm <- lpm * periods_per_year

    lpm
}

# Downside deviation (square root of LPM_2)
downside_deviation <- function(returns, threshold = 0, annualise = TRUE,
                               periods_per_year = 252) {
    sqrt(lower_partial_moment(returns, threshold, n = 2,
                              annualise = annualise,
                              periods_per_year = periods_per_year))
}

# Calculate for S&P 500
cat("S&P 500 Downside Risk Measures:\n")
cat(sprintf("  Standard deviation: %.2f%%\n",
            sample_volatility(spy$returns) * 100))
cat(sprintf("  Semi-deviation (below 0): %.2f%%\n",
            semi_deviation(spy$returns, threshold = 0) * 100))
cat(sprintf("  Downside deviation: %.2f%%\n",
            downside_deviation(spy$returns, threshold = 0) * 100))
cat(sprintf("  LPM(0,1) - Expected shortfall: %.4f%%\n",
            lower_partial_moment(spy$returns, 0, n = 1, annualise = FALSE) * 100))
cat(sprintf("  LPM(0,0) - Shortfall probability: %.2f%%\n",
            lower_partial_moment(spy$returns, 0, n = 0, annualise = FALSE) * 100))
```

---

## 2.5 Drawdown

Drawdown measures the decline from a running peak. It's the risk measure that matters most psychologically—it determines whether investors stick with a strategy.

### 2.5.1 Prose/Intuition

Imagine two strategies with identical Sharpe ratios:
- Strategy A: Steady gains, maximum 10% peak-to-trough decline
- Strategy B: Volatile path, 40% peak-to-trough decline before recovery

Most investors would abandon Strategy B during the drawdown, even if it eventually recovers. Drawdown captures this "pain" that volatility-based measures miss.

Drawdown statistics include:
- **Maximum drawdown (MDD)**: Largest peak-to-trough decline ever observed
- **Average drawdown**: Mean of all drawdown observations
- **Drawdown duration**: How long until recovery to previous peak
- **Underwater curve**: Time series of current drawdown

The challenge with drawdown is that it's **path-dependent** and has an awkward statistical distribution. MDD, in particular, almost always gets worse with more data—you're computing a running maximum of a random walk.

### 2.5.2 Visual Evidence

```{r drawdown-chart, fig.cap="S&P 500 cumulative return (top) and underwater chart (bottom). Major drawdowns—2008, 2020, 2022—are clearly visible."}
# Calculate cumulative return and drawdown
spy[, cum_return := cumprod(1 + returns) - 1]
spy[, high_water_mark := cummax(1 + cum_return)]
spy[, drawdown := (1 + cum_return) / high_water_mark - 1]

# Two-panel plot
library(patchwork)

p1 <- ggplot(spy, aes(x = date, y = cum_return * 100)) +
    geom_line(colour = tc[1], linewidth = 0.5) +
    labs(
        title = "S&P 500 Cumulative Return",
        x = NULL,
        y = "Cumulative Return (%)"
    ) +
    theme_trading()

p2 <- ggplot(spy, aes(x = date, y = drawdown * 100)) +
    geom_area(fill = tc[3], alpha = 0.7) +
    geom_hline(yintercept = 0, colour = "grey50") +
    labs(
        title = "Underwater Chart (Drawdown)",
        x = NULL,
        y = "Drawdown (%)"
    ) +
    theme_trading() +
    scale_y_continuous(labels = function(x) paste0(x, "%"))

p1 / p2
```

```{r drawdown-duration, fig.cap="Distribution of drawdown durations. Most drawdowns are short, but some last years."}
# Calculate drawdown durations
# Find start and end of each drawdown episode
spy[, in_drawdown := drawdown < 0]
spy[, dd_start := in_drawdown & !shift(in_drawdown, fill = FALSE)]
spy[, dd_episode := cumsum(dd_start)]
spy[in_drawdown == FALSE, dd_episode := 0]

# Calculate duration of each episode
dd_durations <- spy[dd_episode > 0, .(
    duration_days = .N,
    max_dd = min(drawdown)
), by = dd_episode]

# Convert to months for better interpretation
dd_durations[, duration_months := duration_days / 21]

ggplot(dd_durations, aes(x = duration_months)) +
    geom_histogram(bins = 50, fill = tc[1], alpha = 0.8) +
    geom_vline(xintercept = median(dd_durations$duration_months),
               colour = tc[2], linewidth = 1, linetype = "dashed") +
    annotate("text", x = median(dd_durations$duration_months) + 2, y = 50,
             label = sprintf("Median: %.1f months", median(dd_durations$duration_months)),
             colour = tc[2]) +
    labs(
        title = "Distribution of Drawdown Durations",
        subtitle = "Most drawdowns are brief, but some persist for years",
        x = "Duration (Months)",
        y = "Frequency"
    ) +
    theme_trading()
```

### 2.5.3 Mathematical Derivation

**Definitions:**

Let $W_t$ be wealth at time $t$, starting from $W_0 = 1$. The **high-water mark** at time $t$ is:

$$\text{HWM}_t = \max_{s \leq t} W_s$$

The **drawdown** at time $t$ is:

$$\text{DD}_t = \frac{\text{HWM}_t - W_t}{\text{HWM}_t} = 1 - \frac{W_t}{\text{HWM}_t}$$

The **maximum drawdown** over the period is:

$$\text{MDD} = \max_t \text{DD}_t = 1 - \min_t \frac{W_t}{\text{HWM}_t}$$

**Expected MDD under random walk:**

For a geometric random walk with drift $\mu$ and volatility $\sigma$, the expected maximum drawdown over $T$ periods has an approximate closed form (Magdon-Ismail et al., 2004):

$$E[\text{MDD}] \approx \begin{cases}
\sigma\sqrt{T} \cdot f\left(\frac{\mu\sqrt{T}}{\sigma}\right) & \text{if } \mu > 0 \\
\sigma\sqrt{T} \cdot g\left(\frac{|\mu|\sqrt{T}}{\sigma}\right) & \text{if } \mu < 0
\end{cases}$$

where $f$ and $g$ are tabulated functions. For practical purposes, the key insight is:

- MDD scales roughly with $\sigma\sqrt{T}$ for strategies with zero or low Sharpe
- MDD increases with more data even for profitable strategies
- Higher Sharpe ratios reduce expected MDD relative to volatility

**Calmar ratio:**

$$\text{Calmar} = \frac{\text{CAGR}}{\text{MDD}}$$

This ratio measures return per unit of worst-case risk. A Calmar of 0.5 means your annual return equals half your worst drawdown.

### 2.5.4 Implementation & Application

```{r drawdown-functions}
# Comprehensive drawdown analysis
drawdown_analysis <- function(returns, wealth_start = 1) {
    returns <- returns[!is.na(returns)]
    n <- length(returns)

    # Calculate wealth path
    wealth <- wealth_start * cumprod(1 + returns)

    # High-water mark and drawdown
    hwm <- cummax(wealth)
    dd <- 1 - wealth / hwm

    # Maximum drawdown
    mdd <- max(dd)
    mdd_end_idx <- which.max(dd)
    mdd_start_idx <- which.max(wealth[1:mdd_end_idx])

    # Recovery time from MDD (if any)
    recovery_idx <- which(wealth[mdd_end_idx:n] >= hwm[mdd_end_idx])[1]
    recovery_time <- if (!is.na(recovery_idx)) recovery_idx else NA

    # Average drawdown
    avg_dd <- mean(dd)

    # Longest drawdown duration
    in_dd <- dd > 0
    dd_rle <- rle(in_dd)
    longest_dd <- if (any(dd_rle$values)) max(dd_rle$lengths[dd_rle$values]) else 0

    # Time underwater (percentage)
    pct_underwater <- mean(dd > 0) * 100

    list(
        max_drawdown = mdd,
        mdd_start_idx = mdd_start_idx,
        mdd_end_idx = mdd_end_idx,
        mdd_recovery_days = recovery_time,
        avg_drawdown = avg_dd,
        longest_drawdown_days = longest_dd,
        pct_time_underwater = pct_underwater,
        drawdown_series = dd
    )
}

# Analyse S&P 500
spy_dd <- drawdown_analysis(spy$returns)

cat("S&P 500 Drawdown Analysis:\n")
cat(sprintf("  Maximum drawdown: %.1f%%\n", spy_dd$max_drawdown * 100))
cat(sprintf("  Average drawdown: %.1f%%\n", spy_dd$avg_drawdown * 100))
cat(sprintf("  Longest drawdown: %d days (%.1f years)\n",
            spy_dd$longest_drawdown_days, spy_dd$longest_drawdown_days / 252))
cat(sprintf("  Time underwater: %.1f%%\n", spy_dd$pct_time_underwater))

# Calculate Calmar ratio
spy_cagr <- (prod(1 + spy$returns))^(252 / length(spy$returns)) - 1
calmar <- spy_cagr / spy_dd$max_drawdown

cat(sprintf("  CAGR: %.2f%%\n", spy_cagr * 100))
cat(sprintf("  Calmar ratio: %.2f\n", calmar))
```

---

## 2.6 Value-at-Risk (VaR)

Value-at-Risk answers: "What's the most I can lose with a given probability?" It's a quantile-based measure that became the regulatory standard after the Basel Accords.

### 2.6.1 Prose/Intuition

VaR answers a simple question: "What's my worst-case loss at the 95th (or 99th) percentile?"

For example, a 1-day 95% VaR of 2% means: "On 95% of days, I will lose less than 2%. On the worst 5% of days, I could lose more."

VaR became dominant because:
1. **Regulators love it.** Capital requirements are based on VaR.
2. **Simple to communicate.** A single number captures risk.
3. **Aggregation.** Can (with caveats) aggregate across portfolios.

VaR's weaknesses:
1. **Says nothing about tail shape.** A 95% VaR tells you nothing about what happens on the worst 5% of days.
2. **Not subadditive.** VaR(A + B) can exceed VaR(A) + VaR(B).
3. **Estimation sensitivity.** Small data changes can cause large VaR changes.

### 2.6.2 Visual Evidence

```{r var-distribution, fig.cap="VaR on the return distribution. VaR marks a specific quantile; it says nothing about losses beyond that point."}
# Plot VaR on S&P 500 returns
returns_for_var <- spy$returns * 100  # Percentage

var_95 <- quantile(returns_for_var, 0.05)
var_99 <- quantile(returns_for_var, 0.01)

ggplot(data.table(returns = returns_for_var), aes(x = returns)) +
    geom_histogram(bins = 100, fill = "grey80", colour = "white") +
    geom_vline(xintercept = var_95, colour = tc[1],
               linewidth = 1.2, linetype = "dashed") +
    geom_vline(xintercept = var_99, colour = tc[3],
               linewidth = 1.2, linetype = "dashed") +
    annotate("text", x = var_95 - 0.5, y = 400,
             label = sprintf("95%% VaR: %.2f%%", abs(var_95)),
             colour = tc[1], hjust = 1) +
    annotate("text", x = var_99 - 0.5, y = 350,
             label = sprintf("99%% VaR: %.2f%%", abs(var_99)),
             colour = tc[3], hjust = 1) +
    geom_area(data = data.table(
        returns = density(returns_for_var, n = 1000)$x,
        density = density(returns_for_var, n = 1000)$y * 20000
    )[returns < var_95],
    aes(x = returns, y = density), fill = tc[1], alpha = 0.3) +
    labs(
        title = "Value-at-Risk on S&P 500 Return Distribution",
        subtitle = "VaR marks a quantile—it doesn't describe what happens beyond",
        x = "Daily Return (%)",
        y = "Frequency"
    ) +
    theme_trading()
```

```{r var-exceedances, fig.cap="VaR exceedances over time. In practice, exceedances cluster during crises—violating the i.i.d. assumption."}
# Calculate rolling VaR and mark exceedances
spy[, var_95_roll := frollapply(returns, 250, function(x) quantile(x, 0.05))]
spy[, var_95_roll := shift(var_95_roll, 1)]  # Use yesterday's VaR
spy[, exceedance := returns < var_95_roll]

# Plot
spy_plot <- spy[date >= "2005-01-01"]

ggplot(spy_plot, aes(x = date)) +
    geom_line(aes(y = returns * 100), colour = "grey70", linewidth = 0.3) +
    geom_line(aes(y = var_95_roll * 100), colour = tc[1],
              linewidth = 0.8) +
    geom_point(data = spy_plot[exceedance == TRUE],
               aes(y = returns * 100), colour = tc[3], size = 1.5) +
    labs(
        title = "Rolling 250-Day 95% VaR with Exceedances",
        subtitle = "Red points are VaR breaches—note the clustering",
        x = NULL,
        y = "Daily Return (%)"
    ) +
    theme_trading()

# Count exceedances
n_exceedances <- sum(spy_plot$exceedance, na.rm = TRUE)
n_total <- sum(!is.na(spy_plot$exceedance))
exceedance_rate <- n_exceedances / n_total

cat(sprintf("\nVaR Exceedance Analysis:\n"))
cat(sprintf("  Expected exceedance rate: 5.0%%\n"))
cat(sprintf("  Actual exceedance rate: %.1f%%\n", exceedance_rate * 100))
```

### 2.6.3 Mathematical Derivation

**Definition:**

VaR at confidence level $\alpha$ (e.g., 95%) is defined as:

$$\text{VaR}_\alpha = -F^{-1}(1 - \alpha)$$

where $F^{-1}$ is the inverse CDF (quantile function) of returns. The negative sign converts the return quantile to a positive loss amount.

Equivalently:
$$P(r \leq -\text{VaR}_\alpha) = 1 - \alpha$$

**Historical VaR:**

Simply use the empirical quantile of historical returns:
$$\text{VaR}_\alpha^{\text{hist}} = -\hat{F}^{-1}(1 - \alpha) = -r_{(\lfloor(1-\alpha)T\rfloor)}$$

where $r_{(k)}$ is the $k$th order statistic (sorted returns).

**Parametric (Normal) VaR:**

Assuming returns are normally distributed: $r \sim N(\mu, \sigma^2)$

$$\text{VaR}_\alpha = -\mu + \sigma \cdot z_\alpha$$

where $z_\alpha$ is the $\alpha$-quantile of the standard normal. For $\alpha = 0.95$, $z_{0.95} = 1.645$. For $\alpha = 0.99$, $z_{0.99} = 2.326$.

**Derivation:**

We want $P(r \leq -\text{VaR}) = 1 - \alpha$.

Standardising: $P\left(\frac{r - \mu}{\sigma} \leq \frac{-\text{VaR} - \mu}{\sigma}\right) = 1 - \alpha$

So: $\Phi\left(\frac{-\text{VaR} - \mu}{\sigma}\right) = 1 - \alpha$

Therefore: $\frac{-\text{VaR} - \mu}{\sigma} = z_{1-\alpha} = -z_\alpha$

Solving: $\text{VaR} = -\mu + \sigma \cdot z_\alpha$

**Scaling VaR:**

Under the assumption of i.i.d. returns, $T$-day VaR scales as:
$$\text{VaR}_T = \text{VaR}_1 \cdot \sqrt{T}$$

This follows from $\sigma_T = \sigma_1\sqrt{T}$ and the parametric form.

### 2.6.4 Implementation & Application

```{r var-functions}
# Historical VaR
var_historical <- function(returns, alpha = 0.95, holding_period = 1) {
    returns <- returns[!is.na(returns)]
    var_1 <- -quantile(returns, 1 - alpha)

    # Scale for holding period (under i.i.d. assumption)
    var_T <- var_1 * sqrt(holding_period)
    var_T
}

# Parametric (Normal) VaR
var_normal <- function(returns, alpha = 0.95, holding_period = 1) {
    returns <- returns[!is.na(returns)]

    mu <- mean(returns)
    sigma <- sd(returns)
    z <- qnorm(alpha)

    var_1 <- -mu + sigma * z
    var_T <- var_1 * sqrt(holding_period)
    var_T
}

# Cornish-Fisher VaR (adjusts for skewness and kurtosis)
var_cornish_fisher <- function(returns, alpha = 0.95, holding_period = 1) {
    returns <- returns[!is.na(returns)]

    mu <- mean(returns)
    sigma <- sd(returns)
    skew <- moments::skewness(returns)
    kurt <- moments::kurtosis(returns) - 3  # Excess kurtosis

    z <- qnorm(alpha)

    # Cornish-Fisher expansion
    z_cf <- z +
        (z^2 - 1) * skew / 6 +
        (z^3 - 3*z) * kurt / 24 -
        (2*z^3 - 5*z) * skew^2 / 36

    var_1 <- -mu + sigma * z_cf
    var_T <- var_1 * sqrt(holding_period)
    var_T
}

# Calculate all three for S&P 500
var_h <- var_historical(spy$returns)
var_n <- var_normal(spy$returns)
var_cf <- var_cornish_fisher(spy$returns)

cat("S&P 500 1-Day 95% VaR:\n")
cat(sprintf("  Historical: %.2f%%\n", var_h * 100))
cat(sprintf("  Parametric (Normal): %.2f%%\n", var_n * 100))
cat(sprintf("  Cornish-Fisher: %.2f%%\n", var_cf * 100))

# 10-day VaR (typical regulatory horizon)
cat("\n10-Day 95% VaR:\n")
cat(sprintf("  Historical: %.2f%%\n", var_historical(spy$returns, holding_period = 10) * 100))
cat(sprintf("  Parametric: %.2f%%\n", var_normal(spy$returns, holding_period = 10) * 100))
```

**VaR backtesting:**

A valid VaR model should have exceedances at the expected rate. The Kupiec test checks this:

```{r var-backtest}
# Kupiec test for VaR exceedances
kupiec_test <- function(returns, var_series, alpha = 0.05) {
    # var_series should be the previous day's VaR (no look-ahead)
    exceedances <- returns < -var_series
    exceedances <- exceedances[!is.na(exceedances)]

    n <- length(exceedances)
    x <- sum(exceedances)  # Number of exceedances
    p_hat <- x / n  # Observed exceedance rate

    # Likelihood ratio test
    # H0: p = alpha, H1: p != alpha
    # LR = -2 * log(L0/L1)

    L0 <- dbinom(x, n, alpha, log = TRUE)
    L1 <- dbinom(x, n, p_hat, log = TRUE)

    lr_stat <- -2 * (L0 - L1)
    p_value <- 1 - pchisq(lr_stat, df = 1)

    list(
        n_observations = n,
        n_exceedances = x,
        expected_exceedances = n * alpha,
        exceedance_rate = p_hat,
        expected_rate = alpha,
        lr_statistic = lr_stat,
        p_value = p_value
    )
}

# Backtest our rolling VaR
# Use lagged VaR (no look-ahead)
spy[, var_95_lagged := shift(var_95_roll, 1)]

test_result <- kupiec_test(
    spy[!is.na(var_95_lagged), returns],
    spy[!is.na(var_95_lagged), -var_95_lagged],  # Negative because VaR is positive
    alpha = 0.05
)

cat("\nKupiec Test for 95% VaR:\n")
cat(sprintf("  Observations: %d\n", test_result$n_observations))
cat(sprintf("  Exceedances: %d (expected: %.0f)\n",
            test_result$n_exceedances, test_result$expected_exceedances))
cat(sprintf("  Exceedance rate: %.2f%% (expected: %.2f%%)\n",
            test_result$exceedance_rate * 100, test_result$expected_rate * 100))
cat(sprintf("  LR statistic: %.2f\n", test_result$lr_statistic))
cat(sprintf("  p-value: %.4f\n", test_result$p_value))
```

---

## 2.7 Expected Shortfall (CVaR)

Expected Shortfall (ES), also called Conditional VaR (CVaR), answers: "When things go wrong, how bad on average?" It's the mean loss conditional on exceeding VaR.

### 2.7.1 Prose/Intuition

VaR tells you the threshold—"you'll lose more than X on 5% of days." Expected Shortfall tells you the average loss when you exceed that threshold—"on those bad 5% of days, your average loss is Y."

ES is superior to VaR because:

1. **Captures tail severity.** Two portfolios with identical VaR can have very different ES if their tail shapes differ.

2. **Coherent risk measure.** ES satisfies subadditivity: ES(A+B) ≤ ES(A) + ES(B). Diversification always reduces ES-based risk.

3. **Regulatory preference.** Basel III moved from VaR to ES for market risk capital.

The downside is that ES is harder to backtest because it's a conditional expectation, not a quantile.

### 2.7.2 Visual Evidence

```{r es-visualisation, fig.cap="Expected Shortfall vs VaR. ES is the average of all losses beyond VaR, capturing tail severity."}
# Visualise ES vs VaR
returns_pct <- spy$returns * 100

var_95_pct <- quantile(returns_pct, 0.05)
es_95_pct <- mean(returns_pct[returns_pct <= var_95_pct])

# Create density for shading
dens <- density(returns_pct, n = 1000)
dens_dt <- data.table(x = dens$x, y = dens$y)

ggplot() +
    geom_histogram(data = data.table(returns = returns_pct),
                   aes(x = returns, y = after_stat(density)),
                   bins = 100, fill = "grey80", colour = "white") +
    geom_line(data = dens_dt, aes(x = x, y = y), colour = "grey40", linewidth = 1) +
    geom_area(data = dens_dt[x <= var_95_pct],
              aes(x = x, y = y), fill = tc[3], alpha = 0.5) +
    geom_vline(xintercept = var_95_pct, colour = tc[1],
               linewidth = 1.2, linetype = "dashed") +
    geom_vline(xintercept = es_95_pct, colour = tc[2],
               linewidth = 1.2) +
    annotate("text", x = var_95_pct + 0.3, y = 0.3,
             label = sprintf("95%% VaR\n%.2f%%", abs(var_95_pct)),
             colour = tc[1], hjust = 0, size = 4) +
    annotate("text", x = es_95_pct + 0.3, y = 0.25,
             label = sprintf("95%% ES\n%.2f%%", abs(es_95_pct)),
             colour = tc[2], hjust = 0, size = 4) +
    annotate("text", x = -6, y = 0.15,
             label = "ES = average\nloss in tail",
             colour = tc[3], size = 3.5) +
    labs(
        title = "Expected Shortfall vs Value-at-Risk",
        subtitle = "ES captures the average severity of tail losses",
        x = "Daily Return (%)",
        y = "Density"
    ) +
    theme_trading() +
    coord_cartesian(xlim = c(-10, 5))
```

### 2.7.3 Mathematical Derivation

**Definition:**

Expected Shortfall at confidence level $\alpha$ is:

$$\text{ES}_\alpha = E[r \mid r \leq -\text{VaR}_\alpha] = \frac{1}{1-\alpha} \int_{-\infty}^{-\text{VaR}_\alpha} r \cdot f(r) \, dr$$

where $f(r)$ is the probability density function of returns.

Equivalently, ES is the mean of the worst $(1-\alpha)$ fraction of returns.

**Historical ES:**

For empirical data with $T$ observations:

$$\text{ES}_\alpha^{\text{hist}} = \frac{1}{\lfloor(1-\alpha)T\rfloor} \sum_{t: r_t \leq -\text{VaR}_\alpha} r_t$$

**Parametric (Normal) ES:**

For $r \sim N(\mu, \sigma^2)$:

$$\text{ES}_\alpha = -\mu + \sigma \cdot \frac{\phi(z_\alpha)}{1-\alpha}$$

where $\phi$ is the standard normal PDF and $z_\alpha = \Phi^{-1}(\alpha)$.

**Derivation:**

We need $E[r | r \leq q]$ where $q = -\text{VaR}_\alpha$.

By definition of conditional expectation:
$$E[r | r \leq q] = \frac{1}{P(r \leq q)} \int_{-\infty}^q r \cdot f(r) \, dr$$

For normal distribution with $r = \mu + \sigma Z$ where $Z \sim N(0,1)$:
$$\int_{-\infty}^q r \cdot f(r) \, dr = \int_{-\infty}^{(q-\mu)/\sigma} (\mu + \sigma z) \cdot \phi(z) \, dz$$

Using the identity $\int_{-\infty}^a z\phi(z)dz = -\phi(a)$:
$$= \mu \cdot \Phi\left(\frac{q-\mu}{\sigma}\right) - \sigma \cdot \phi\left(\frac{q-\mu}{\sigma}\right)$$

Dividing by $P(r \leq q) = \Phi((q-\mu)/\sigma) = 1-\alpha$:
$$\text{ES}_\alpha = \mu - \sigma \cdot \frac{\phi(z_{1-\alpha})}{1-\alpha} = -\mu + \sigma \cdot \frac{\phi(z_\alpha)}{1-\alpha}$$

**Subadditivity proof:**

ES is subadditive: for any portfolios $A$ and $B$:
$$\text{ES}(A + B) \leq \text{ES}(A) + \text{ES}(B)$$

This is a consequence of the superadditivity of the expectation operator and the definition of ES as a conditional expectation. The formal proof uses the representation of ES as an optimisation problem.

### 2.7.4 Implementation & Application

```{r es-functions}
# Historical Expected Shortfall
es_historical <- function(returns, alpha = 0.95) {
    returns <- returns[!is.na(returns)]
    var_threshold <- quantile(returns, 1 - alpha)
    tail_returns <- returns[returns <= var_threshold]
    -mean(tail_returns)
}

# Parametric (Normal) Expected Shortfall
es_normal <- function(returns, alpha = 0.95) {
    returns <- returns[!is.na(returns)]

    mu <- mean(returns)
    sigma <- sd(returns)
    z <- qnorm(alpha)

    es <- -mu + sigma * dnorm(z) / (1 - alpha)
    es
}

# Cornish-Fisher Expected Shortfall
es_cornish_fisher <- function(returns, alpha = 0.95) {
    returns <- returns[!is.na(returns)]

    mu <- mean(returns)
    sigma <- sd(returns)
    skew <- moments::skewness(returns)
    kurt <- moments::kurtosis(returns) - 3

    # Use the CF-adjusted quantile
    z <- qnorm(alpha)
    z_cf <- z +
        (z^2 - 1) * skew / 6 +
        (z^3 - 3*z) * kurt / 24 -
        (2*z^3 - 5*z) * skew^2 / 36

    # Approximate ES using the CF quantile
    # This is a simplification; proper CF ES requires integration
    es <- -mu + sigma * dnorm(z_cf) / (1 - alpha)
    es
}

# Calculate all three for S&P 500
es_h <- es_historical(spy$returns)
es_n <- es_normal(spy$returns)

cat("S&P 500 95% Expected Shortfall:\n")
cat(sprintf("  Historical: %.2f%%\n", es_h * 100))
cat(sprintf("  Parametric (Normal): %.2f%%\n", es_n * 100))
cat(sprintf("  Ratio ES/VaR (hist): %.2f\n", es_h / var_historical(spy$returns)))

# Compare ES to VaR
cat("\nRisk Measure Comparison:\n")
cat(sprintf("  95%% VaR (hist): %.2f%%\n", var_historical(spy$returns) * 100))
cat(sprintf("  95%% ES (hist): %.2f%%\n", es_h * 100))
cat(sprintf("  99%% VaR (hist): %.2f%%\n", var_historical(spy$returns, alpha = 0.99) * 100))
cat(sprintf("  99%% ES (hist): %.2f%%\n", es_historical(spy$returns, alpha = 0.99) * 100))
```

---

## Quick Reference: Risk Metrics

### Volatility

| Measure | Formula | Notes |
|---------|---------|-------|
| Sample vol | $\sigma = \sqrt{\frac{1}{T-1}\sum(r_t - \bar{r})^2}$ | Bessel corrected |
| Annualised | $\sigma_{\text{ann}} = \sigma_{\text{daily}} \times \sqrt{252}$ | Assumes i.i.d. |
| EWMA | $\sigma^2_t = \lambda\sigma^2_{t-1} + (1-\lambda)r^2_{t-1}$ | λ ≈ 0.94 typical |

### Downside Measures

| Measure | Formula | Notes |
|---------|---------|-------|
| Semi-deviation | $\sigma^- = \sqrt{E[\min(r-\tau,0)^2]}$ | τ often 0 or r_f |
| LPM(τ,n) | $E[\max(\tau-r,0)^n]$ | n=2 is semi-variance |

### Drawdown

| Measure | Formula | Notes |
|---------|---------|-------|
| Drawdown | $DD_t = 1 - W_t/\text{HWM}_t$ | Path-dependent |
| Max DD | $\max_t(DD_t)$ | Gets worse with more data |
| Calmar | CAGR / MDD | Return per unit max risk |

### VaR and ES

| Measure | Historical | Normal |
|---------|------------|--------|
| 95% VaR | Quantile(0.05) | $-\mu + 1.645\sigma$ |
| 99% VaR | Quantile(0.01) | $-\mu + 2.326\sigma$ |
| ES | Mean of tail | $-\mu + \sigma\frac{\phi(z)}{1-\alpha}$ |

### R Code Snippets

```r
# Volatility
vol_daily <- sd(returns, na.rm = TRUE)
vol_annual <- vol_daily * sqrt(252)

# EWMA (lambda = 0.94)
ewma_var <- numeric(n); ewma_var[1] <- var(returns[1:20])
for (t in 2:n) ewma_var[t] <- 0.94 * ewma_var[t-1] + 0.06 * returns[t-1]^2

# Downside deviation
downside_returns <- returns[returns < 0]
downside_dev <- sqrt(mean(downside_returns^2)) * sqrt(252)

# Drawdown
wealth <- cumprod(1 + returns)
hwm <- cummax(wealth)
drawdown <- 1 - wealth / hwm
max_dd <- max(drawdown)

# VaR
var_95 <- -quantile(returns, 0.05)
var_95_normal <- -mean(returns) + sd(returns) * qnorm(0.95)

# Expected Shortfall
es_95 <- -mean(returns[returns <= quantile(returns, 0.05)])
```
