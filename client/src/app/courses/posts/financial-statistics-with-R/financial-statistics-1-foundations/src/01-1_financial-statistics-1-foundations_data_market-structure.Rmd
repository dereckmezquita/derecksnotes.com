---
title: "Algorithmic Trading with R"
chapter: "Chapter 1: Financial Data and Returns"
part: "Part 1: Market Structure and Data"
section: "01-1"
coverImage: 13
author: "Dereck Mezquita"
date: 2026-01-20
tags: [algorithmic-trading, quantitative-finance, R, statistics]
published: true
comments: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# HTML5 figure hook for accessibility
if (knitr::is_html_output()) knitr::knit_hooks$set(
    plot = function(x, options) {
        cap  <- options$fig.cap
        as.character(htmltools::tag(
            "Figure", list(src = x, alt = cap, paste("\n\t", cap, "\n", sep = ""))
        ))
    }
)

knitr::knit_hooks$set(optipng = knitr::hook_optipng)
knitr::opts_chunk$set(
    dpi = 300,
    fig.width = 10,
    fig.height = 7,
    comment = "",
    warning = FALSE,
    collapse = FALSE,
    results = 'hold'
)

# Set working directory for data access
course_root <- dirname(knitr::current_input(dir = TRUE))
knitr::opts_knit$set(root.dir = course_root)
setwd(course_root)

# Load modules
box::use(
    ./modules/data[load_market, load_factors, list_market_symbols, filter_dates],
    ./modules/stats[sharpe_ratio, annualised_return, annualised_vol],
    ./modules/viz[plot_price, plot_cumulative_returns, theme_trading, trading_colors]
)

# Load core packages
library(data.table)
library(ggplot2)
```

# Chapter 1: Financial Data and Returns

Every trading strategy begins with data. Before we can build models, test hypotheses, or deploy capital, we must understand precisely what the numbers represent and how to interpret them correctly.

This chapter establishes the foundation. We examine market microstructure to understand how prices form, explore the conventions for recording price data, and derive the mathematics of returns. We then document the empirical regularities—the "stylised facts"—that characterise real financial data and distinguish it from the idealised models of textbooks.

---

# Part 1: Market Structure and Data

The data you download from Yahoo Finance or Bloomberg represents the outcome of a complex process involving millions of market participants, each with different information, objectives, and constraints. Understanding this process is essential for interpreting what the data can and cannot tell you.

---

## 1.1 How Markets Work

### 1.1.1 Prose/Intuition

A financial market is fundamentally an information aggregation mechanism. Prices emerge from the collective actions of buyers and sellers, each acting on their own beliefs about value. When a new piece of information enters the market—an earnings announcement, a macroeconomic report, a rumour—prices adjust as traders revise their expectations.

**The order book** sits at the heart of modern electronic markets. It is a queue of limit orders: offers to buy or sell at specified prices. Market participants can either:

1. **Submit limit orders**: "I will buy 100 shares at £45.00"—these orders wait in the queue until matched.
2. **Submit market orders**: "I want to buy 100 shares now, at the best available price"—these orders execute immediately against waiting limit orders.

The interaction between patient traders (who provide liquidity via limit orders) and impatient traders (who demand liquidity via market orders) determines prices.

**Market makers** are specialised firms that continuously quote both bid (buy) and ask (sell) prices. They profit from the spread—the difference between what they buy at and what they sell at—in exchange for bearing the risk of holding inventory. Without market makers, you might find no one willing to take the other side of your trade.

**Price discovery** refers to the process by which markets incorporate information into prices. In efficient markets, this happens quickly—within milliseconds for major assets. The key insight for quantitative trading is that *price movements contain information*, but distinguishing genuine information from noise is extraordinarily difficult.

**Exchanges vs OTC markets**: Exchanges (NYSE, NASDAQ, LSE) are centralised venues with standardised rules and transparent order books. Over-the-counter (OTC) markets involve direct negotiation between parties, common for bonds, derivatives, and foreign exchange. Most equity trading now occurs on exchanges, while most fixed income trading remains OTC.

### 1.1.2 Visual Evidence

Let us examine what actual order book data looks like and how bid-ask spreads behave in practice.

```{r order-book-simulation, fig.cap="Simulated order book showing bid and ask queues. The spread is the gap between the best bid and best ask."}
# Simulate a simple order book snapshot
set.seed(42)

# Generate order book levels
n_levels <- 10
mid_price <- 100.00
tick_size <- 0.01

# Bid side (buyers) - prices below mid
bid_prices <- mid_price - (1:n_levels) * tick_size
bid_sizes <- round(runif(n_levels, 100, 1000))

# Ask side (sellers) - prices above mid
ask_prices <- mid_price + (1:n_levels) * tick_size
ask_sizes <- round(runif(n_levels, 100, 1000))

# Create order book data.table
order_book <- data.table(
    price = c(rev(bid_prices), ask_prices),
    size = c(rev(bid_sizes), ask_sizes),
    side = c(rep("Bid", n_levels), rep("Ask", n_levels))
)

# Calculate cumulative depth
order_book[side == "Bid", cum_size := cumsum(size)]
order_book[side == "Ask", cum_size := cumsum(size)]

# Visualise order book
ggplot(order_book, aes(x = price, y = size, fill = side)) +
    geom_col(width = 0.008, alpha = 0.8) +
    geom_vline(xintercept = mid_price, linetype = "dashed", colour = "grey50") +
    annotate("text", x = mid_price, y = max(order_book$size) * 0.9,
             label = sprintf("Mid: £%.2f\nSpread: £%.2f (%.0f bps)",
                           mid_price, 2 * tick_size, 2 * tick_size / mid_price * 10000),
             hjust = 0.5, size = 3.5) +
    scale_fill_manual(values = c("Bid" = trading_colors$positive,
                                  "Ask" = trading_colors$negative)) +
    labs(
        title = "Order Book Snapshot",
        subtitle = "Bid orders (green) vs ask orders (red) at each price level",
        x = "Price (£)",
        y = "Order Size (shares)",
        fill = "Side"
    ) +
    theme_trading() +
    coord_cartesian(xlim = c(99.85, 100.15))
```

The spread represents the cost of immediacy. If you want to buy right now, you pay the ask price. If you want to sell right now, you receive the bid price. The spread compensates market makers for providing this service.

```{r spread-by-size, fig.cap="Bid-ask spread as percentage of price across different market capitalisations. Smaller stocks have wider spreads."}
# Load sample market data to examine spreads by market cap
# For this demonstration, we'll create realistic spread estimates
set.seed(123)

# Market cap quintiles with typical spread characteristics
spread_data <- data.table(
    market_cap = c("Mega Cap\n(>$200B)", "Large Cap\n($10-200B)",
                   "Mid Cap\n($2-10B)", "Small Cap\n($300M-2B)",
                   "Micro Cap\n(<$300M)"),
    typical_spread_bps = c(1, 3, 8, 25, 100),
    spread_sd = c(0.5, 1, 3, 10, 50)
)

# Maintain ordering
spread_data[, market_cap := factor(market_cap, levels = market_cap)]

ggplot(spread_data, aes(x = market_cap, y = typical_spread_bps)) +
    geom_col(fill = trading_colors$primary, alpha = 0.8) +
    geom_errorbar(aes(ymin = typical_spread_bps - spread_sd,
                      ymax = typical_spread_bps + spread_sd),
                  width = 0.2, colour = trading_colors$dark) +
    geom_text(aes(label = sprintf("%.0f bps", typical_spread_bps)),
              vjust = -0.5, size = 3.5) +
    labs(
        title = "Typical Bid-Ask Spread by Market Capitalisation",
        subtitle = "Spread in basis points (1 bp = 0.01%)",
        x = NULL,
        y = "Spread (basis points)"
    ) +
    theme_trading() +
    scale_y_continuous(expand = expansion(mult = c(0, 0.15)))
```

This pattern has profound implications for strategy design. A strategy that trades frequently in micro-cap stocks faces transaction costs 100 times higher than one trading mega-caps. Many theoretically profitable strategies fail in practice because they cannot overcome these costs.

### 1.1.3 Mathematical Derivation

**Price impact: Kyle's Lambda**

When a trader submits a market order, the price moves. This price impact reflects the market's inference about the trader's information. Albert Kyle (1985) formalised this in a classic model.

Consider a market with:
- An informed trader who knows the asset's true value $v$
- Noise traders whose aggregate order flow is $u \sim N(0, \sigma_u^2)$
- A market maker who observes total order flow $x + u$ but cannot distinguish informed from noise trading

The market maker sets price equal to expected value given observed order flow:

$$P = E[v | x + u]$$

Kyle showed that in equilibrium, the informed trader's strategy and the market maker's pricing rule take linear forms:

$$x = \beta(v - P_0)$$

where $\beta$ is the informed trader's trading intensity, and:

$$P = P_0 + \lambda(x + u)$$

where $\lambda$ is **Kyle's lambda**—the price impact coefficient.

**Derivation of $\lambda$:**

The market maker's inference problem uses Bayes' rule. Let the total order flow be $y = x + u$. The market maker computes:

$$E[v | y] = P_0 + \frac{Cov(v, y)}{Var(y)}(y - E[y])$$

Since $x = \beta(v - P_0)$ and $E[y] = 0$:

$$Cov(v, y) = Cov(v, \beta(v - P_0) + u) = \beta \cdot Var(v) = \beta \sigma_v^2$$

$$Var(y) = Var(\beta(v - P_0) + u) = \beta^2 \sigma_v^2 + \sigma_u^2$$

Therefore:

$$\lambda = \frac{\beta \sigma_v^2}{\beta^2 \sigma_v^2 + \sigma_u^2}$$

In equilibrium, the informed trader optimally chooses $\beta$ to maximise expected profits, yielding:

$$\lambda = \frac{\sigma_v}{2\sigma_u}$$

**Key insight**: Price impact $\lambda$ increases with the variance of fundamental value ($\sigma_v$) and decreases with noise trading ($\sigma_u$). Markets with more informed trading are harder to trade in without moving prices.

### 1.1.4 Implementation and Application

Let us implement a simple order book simulator and estimate effective spreads from trade data.

```{r order-book-simulator}
#' Simulate order book dynamics
#' @param n_periods Number of time periods
#' @param initial_mid Initial mid price
#' @param tick_size Minimum price increment
#' @param volatility Price volatility per period
#' @param lambda Kyle's lambda (price impact)
#' @return data.table of order book states
simulate_order_book <- function(n_periods = 1000,
                                 initial_mid = 100,
                                 tick_size = 0.01,
                                 volatility = 0.001,
                                 lambda = 0.1) {
    set.seed(42)

    # Initialise
    mid <- initial_mid
    bid <- mid - tick_size
    ask <- mid + tick_size

    # Storage
    results <- data.table(
        t = 1:n_periods,
        mid = numeric(n_periods),
        bid = numeric(n_periods),
        ask = numeric(n_periods),
        spread = numeric(n_periods),
        trade_price = numeric(n_periods),
        trade_direction = integer(n_periods)  # 1 = buy, -1 = sell
    )

    for (i in 1:n_periods) {
        # Random order flow: positive = buy pressure, negative = sell pressure
        order_flow <- rnorm(1, 0, 1)

        # Price impact (Kyle's lambda)
        price_impact <- lambda * order_flow

        # Information arrival (random walk component)
        news <- rnorm(1, 0, volatility) * mid

        # Update mid price
        mid <- mid + price_impact + news

        # Update quotes with some spread
        half_spread <- max(tick_size, abs(volatility * mid * 0.5))
        bid <- mid - half_spread
        ask <- mid + half_spread

        # Record trade (at bid if sell, at ask if buy)
        direction <- sign(order_flow)
        trade_price <- if (direction > 0) ask else bid

        # Store results
        results[i, `:=`(
            mid = mid,
            bid = bid,
            ask = ask,
            spread = ask - bid,
            trade_price = trade_price,
            trade_direction = direction
        )]
    }

    return(results)
}

# Run simulation
ob_sim <- simulate_order_book(n_periods = 500)

cat("Order book simulation summary:\n")
cat("  Mean spread: £", round(mean(ob_sim$spread), 4), "\n")
cat("  Mean spread (bps):", round(mean(ob_sim$spread / ob_sim$mid) * 10000, 1), "\n")
cat("  Price range: £", round(min(ob_sim$mid), 2), "to £", round(max(ob_sim$mid), 2), "\n")
```

```{r plot-ob-simulation, fig.cap="Simulated order book dynamics showing mid price, bid, and ask over time."}
# Plot the simulation
ggplot(ob_sim, aes(x = t)) +
    geom_ribbon(aes(ymin = bid, ymax = ask), fill = trading_colors$light, alpha = 0.5) +
    geom_line(aes(y = mid), colour = trading_colors$primary, linewidth = 0.5) +
    geom_line(aes(y = bid), colour = trading_colors$positive, linewidth = 0.3, alpha = 0.7) +
    geom_line(aes(y = ask), colour = trading_colors$negative, linewidth = 0.3, alpha = 0.7) +
    labs(
        title = "Simulated Order Book Dynamics",
        subtitle = "Blue line = mid price, green = bid, red = ask, shaded area = spread",
        x = "Time",
        y = "Price (£)"
    ) +
    theme_trading()
```

**Estimating effective spread from trade data:**

When we only have trade prices (not the full order book), we can estimate the effective spread using the Roll (1984) model. If the true price follows a random walk and trades alternate between bid and ask:

$$\text{Effective Spread} = 2\sqrt{-Cov(r_t, r_{t-1})}$$

where $r_t$ is the return from trade $t-1$ to trade $t$.

```{r effective-spread}
#' Estimate effective spread using Roll (1984) method
#' @param prices Vector of trade prices
#' @return Estimated effective spread
estimate_effective_spread <- function(prices) {
    # Calculate returns
    returns <- diff(log(prices))

    # Remove NAs
    returns <- returns[!is.na(returns)]
    if (length(returns) < 3) return(NA_real_)

    # Calculate autocovariance at lag 1
    autocov <- cov(returns[-length(returns)], returns[-1], use = "complete.obs")

    # Handle NA result
    if (is.na(autocov)) return(NA_real_)

    # Roll estimator: spread = 2 * sqrt(-autocov)
    # Only valid if autocov is negative
    if (autocov < 0) {
        spread <- 2 * sqrt(-autocov)
        return(spread)
    } else {
        # If autocov is non-negative, spread is effectively zero
        # (or model assumptions are violated)
        return(NA_real_)
    }
}

# Apply to our simulated trade prices
effective_spread <- estimate_effective_spread(ob_sim$trade_price)

cat("Effective spread estimation:\n")
cat("  True spread (mean):", round(mean(ob_sim$spread / ob_sim$mid) * 10000, 1), "bps\n")
if (!is.na(effective_spread)) {
    cat("  Roll estimate:", round(effective_spread * 10000, 1), "bps\n")
} else {
    cat("  Roll estimate: NA (positive autocovariance)\n")
}
```

**Trading application:**

Understanding market microstructure directly affects strategy profitability:

1. **Cost estimation**: Before backtesting, estimate realistic transaction costs based on the assets you plan to trade. Use spread + impact, not just spread.

2. **Capacity limits**: Kyle's lambda tells us that impact grows with trade size. A strategy's capacity is limited by how much you can trade before impact destroys the alpha.

3. **Execution timing**: Spreads widen at market open and close. Patient traders can reduce costs by avoiding these periods.

4. **Market selection**: The same strategy may be profitable in liquid markets but unprofitable in illiquid ones purely due to transaction cost differences.

---

## 1.2 Price Data: OHLCV

### 1.2.1 Prose/Intuition

Financial databases do not store every tick. Instead, they aggregate continuous trading activity into summary statistics for fixed intervals—typically daily, but sometimes hourly, minutely, or weekly.

**OHLCV** stands for:

- **Open**: The first traded price in the interval
- **High**: The maximum price during the interval
- **Low**: The minimum price during the interval
- **Close**: The last traded price in the interval
- **Volume**: Total shares (or contracts) traded during the interval

These five numbers compress potentially thousands of individual trades into a manageable summary. The close price receives the most attention—it is used for most return calculations and serves as the benchmark for end-of-day valuations.

But OHLCV data loses information. We cannot reconstruct the intraday price path from OHLCV alone. We don't know whether the high occurred before the low, or how long prices spent near each extreme. This information loss matters for some applications (volatility estimation, order execution) but not for others (daily momentum strategies).

**Why the close matters**: Many institutional investors (mutual funds, pension funds) calculate their net asset value (NAV) using closing prices. This creates real economic importance—mutual fund redemptions settle at the NAV price. Some strategies explicitly target the close-to-close return, while others use alternative reference prices (VWAP, open-to-open).

### 1.2.2 Visual Evidence

```{r load-spy-data}
# Load S&P 500 ETF (SPY) data
spy <- load_market("spy")

# Filter to recent period for clearer visualisation
spy_recent <- filter_dates(spy, "2024-01-01", "2024-06-30")

cat("SPY data structure:\n")
cat("  Columns:", paste(names(spy), collapse = ", "), "\n")
cat("  Date range:", as.character(min(spy$date)), "to", as.character(max(spy$date)), "\n")
cat("  Total observations:", nrow(spy), "\n")

head(spy_recent)
```

```{r candlestick-chart, fig.cap="Candlestick chart showing OHLC price data. Green candles indicate close > open (up days), red indicate close < open (down days)."}
# Create candlestick-style visualisation
spy_candle <- copy(spy_recent)
spy_candle[, direction := ifelse(close >= open, "Up", "Down")]
spy_candle[, body_low := pmin(open, close)]
spy_candle[, body_high := pmax(open, close)]

ggplot(spy_candle, aes(x = date)) +
    # High-low wicks
    geom_segment(aes(xend = date, y = low, yend = high),
                 colour = "grey40", linewidth = 0.3) +
    # Open-close bodies
    geom_rect(aes(xmin = date - 0.4, xmax = date + 0.4,
                  ymin = body_low, ymax = body_high, fill = direction),
              colour = "grey40", linewidth = 0.1) +
    scale_fill_manual(values = c("Up" = trading_colors$positive,
                                  "Down" = trading_colors$negative)) +
    labs(
        title = "SPY Candlestick Chart (H1 2024)",
        subtitle = "Each candle summarises one trading day's price action",
        x = NULL,
        y = "Price ($)",
        fill = "Direction"
    ) +
    theme_trading() +
    theme(legend.position = "right")
```

```{r ohlc-info-loss, fig.cap="Information loss from OHLCV aggregation. The same OHLC values can arise from very different intraday paths."}
# Demonstrate information loss: same OHLC, different paths
set.seed(42)

# Generate two different intraday paths with same OHLC
n_points <- 100
open_price <- 100
high_price <- 102
low_price <- 98
close_price <- 101

# Path 1: Goes up then down then up
t <- seq(0, 1, length.out = n_points)
path1 <- open_price +
    2 * sin(pi * t) +  # Main oscillation
    (close_price - open_price) * t  # Drift to close

# Rescale to match OHLC exactly
path1 <- (path1 - min(path1)) / (max(path1) - min(path1)) * (high_price - low_price) + low_price
path1[1] <- open_price
path1[n_points] <- close_price

# Path 2: Goes down then up
path2 <- open_price +
    -2 * sin(pi * t) +  # Opposite oscillation
    (close_price - open_price) * t  # Same drift to close

path2 <- (path2 - min(path2)) / (max(path2) - min(path2)) * (high_price - low_price) + low_price
path2[1] <- open_price
path2[n_points] <- close_price

# Combine for plotting
paths_dt <- data.table(
    t = rep(t, 2),
    price = c(path1, path2),
    path = rep(c("Path A: Rally first, then pullback",
                 "Path B: Selloff first, then recovery"), each = n_points)
)

# Create OHLC reference box
ohlc_box <- data.table(
    xmin = 0, xmax = 1,
    ymin = low_price, ymax = high_price
)

ggplot(paths_dt, aes(x = t, y = price, colour = path)) +
    geom_hline(yintercept = c(open_price, close_price),
               linetype = "dashed", colour = "grey60", linewidth = 0.5) +
    geom_hline(yintercept = c(high_price, low_price),
               linetype = "dotted", colour = "grey60", linewidth = 0.5) +
    geom_line(linewidth = 0.8) +
    annotate("text", x = 0.02, y = open_price, label = "Open",
             hjust = 0, vjust = -0.5, size = 3) +
    annotate("text", x = 0.98, y = close_price, label = "Close",
             hjust = 1, vjust = -0.5, size = 3) +
    annotate("text", x = 0.5, y = high_price, label = "High",
             hjust = 0.5, vjust = -0.5, size = 3) +
    annotate("text", x = 0.5, y = low_price, label = "Low",
             hjust = 0.5, vjust = 1.5, size = 3) +
    scale_colour_manual(values = c(trading_colors$primary, trading_colors$secondary)) +
    labs(
        title = "Same OHLC, Different Stories",
        subtitle = sprintf("Both paths have O=%.0f, H=%.0f, L=%.0f, C=%.0f",
                          open_price, high_price, low_price, close_price),
        x = "Time (fraction of day)",
        y = "Price",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

This example illustrates why some applications require tick data: if you're estimating volatility, the path matters. If you're calculating daily returns, it doesn't.

### 1.2.3 Mathematical Derivation

**OHLC as sufficient statistics for range-based volatility**

The high-low range contains information about intraday volatility. Several estimators exploit this:

**Parkinson (1980) estimator:**

Assumes prices follow geometric Brownian motion: $dS = \mu S \, dt + \sigma S \, dW$

For a single day, the range $H - L$ (in log terms) relates to volatility by:

$$\hat{\sigma}^2_{Parkinson} = \frac{(\ln H - \ln L)^2}{4 \ln 2}$$

**Derivation sketch:** Under GBM, the expected squared range $E[(\ln H - \ln L)^2]$ can be derived from the distribution of the range of a Brownian bridge. The factor $4 \ln 2 \approx 2.77$ arises from this distribution.

**Garman-Klass (1980) estimator:**

Incorporates all four OHLC prices for greater efficiency:

$$\hat{\sigma}^2_{GK} = 0.511(\ln H - \ln L)^2 - 0.019[(\ln C - \ln O)(\ln H + \ln L - 2\ln O) - 2(\ln H - \ln O)(\ln L - \ln O)] - 0.383(\ln C - \ln O)^2$$

This estimator is approximately 8 times more efficient than the close-to-close estimator (uses information from the full trading range, not just endpoints).

**Rogers-Satchell (1991) estimator:**

Handles non-zero drift (trending markets):

$$\hat{\sigma}^2_{RS} = (\ln H - \ln C)(\ln H - \ln O) + (\ln L - \ln C)(\ln L - \ln O)$$

This estimator is unbiased even when the true drift $\mu \neq 0$.

### 1.2.4 Implementation and Application

```{r volatility-estimators}
#' Calculate Parkinson volatility estimator
#' @param high Vector of high prices
#' @param low Vector of low prices
#' @return Annualised volatility estimate
parkinson_vol <- function(high, low, periods_per_year = 252) {
    log_range_sq <- (log(high) - log(low))^2
    daily_var <- mean(log_range_sq, na.rm = TRUE) / (4 * log(2))
    sqrt(daily_var * periods_per_year)
}

#' Calculate Garman-Klass volatility estimator
#' @param open Vector of open prices
#' @param high Vector of high prices
#' @param low Vector of low prices
#' @param close Vector of close prices
#' @return Annualised volatility estimate
garman_klass_vol <- function(open, high, low, close, periods_per_year = 252) {
    log_hl <- log(high) - log(low)
    log_co <- log(close) - log(open)

    daily_var <- mean(
        0.5 * log_hl^2 - (2 * log(2) - 1) * log_co^2,
        na.rm = TRUE
    )

    sqrt(daily_var * periods_per_year)
}

#' Calculate Rogers-Satchell volatility estimator (drift-adjusted)
#' @param open Vector of open prices
#' @param high Vector of high prices
#' @param low Vector of low prices
#' @param close Vector of close prices
#' @return Annualised volatility estimate
rogers_satchell_vol <- function(open, high, low, close, periods_per_year = 252) {
    log_h <- log(high)
    log_l <- log(low)
    log_o <- log(open)
    log_c <- log(close)

    daily_var <- mean(
        (log_h - log_c) * (log_h - log_o) + (log_l - log_c) * (log_l - log_o),
        na.rm = TRUE
    )

    sqrt(daily_var * periods_per_year)
}

#' Standard close-to-close volatility
#' @param close Vector of close prices
#' @return Annualised volatility estimate
close_to_close_vol <- function(close, periods_per_year = 252) {
    returns <- diff(log(close))
    sqrt(var(returns, na.rm = TRUE) * periods_per_year)
}

# Compare estimators on SPY data
vol_comparison <- data.table(
    Estimator = c("Close-to-Close", "Parkinson", "Garman-Klass", "Rogers-Satchell"),
    Volatility = c(
        close_to_close_vol(spy$close),
        parkinson_vol(spy$high, spy$low),
        garman_klass_vol(spy$open, spy$high, spy$low, spy$close),
        rogers_satchell_vol(spy$open, spy$high, spy$low, spy$close)
    )
)

vol_comparison[, Volatility := paste0(round(Volatility * 100, 2), "%")]

cat("SPY Volatility Estimates (Annualised):\n")
print(vol_comparison)
```

```{r rolling-vol-comparison, fig.cap="Rolling 20-day volatility using different estimators. Range-based estimators react more quickly to volatility changes."}
# Calculate rolling volatilities
window <- 20

spy_vol <- copy(spy_recent)
spy_vol[, row_id := .I]

# Close-to-close rolling volatility
spy_vol[, returns := c(NA, diff(log(close)))]
spy_vol[row_id >= window, vol_cc := {
    sapply(row_id[row_id >= window], function(i) {
        sd(returns[(i - window + 1):i], na.rm = TRUE) * sqrt(252)
    })
}]

# Parkinson rolling volatility
spy_vol[row_id >= window, vol_park := {
    sapply(row_id[row_id >= window], function(i) {
        parkinson_vol(high[(i - window + 1):i], low[(i - window + 1):i])
    })
}]

# Melt for plotting
vol_long <- melt(spy_vol[!is.na(vol_cc), .(date, vol_cc, vol_park)],
                 id.vars = "date",
                 variable.name = "estimator",
                 value.name = "volatility")
vol_long[, estimator := ifelse(estimator == "vol_cc", "Close-to-Close", "Parkinson")]

ggplot(vol_long, aes(x = date, y = volatility, colour = estimator)) +
    geom_line(linewidth = 0.6, alpha = 0.8) +
    scale_colour_manual(values = c("Close-to-Close" = trading_colors$primary,
                                    "Parkinson" = trading_colors$secondary)) +
    scale_y_continuous(labels = scales::percent) +
    labs(
        title = "Rolling 20-Day Volatility Comparison",
        subtitle = "Range-based (Parkinson) vs return-based (close-to-close) estimators",
        x = NULL,
        y = "Annualised Volatility",
        colour = "Estimator"
    ) +
    theme_trading()
```

**Detecting data quality issues:**

Real-world data contains errors. Common problems include:

```{r data-quality-checks}
#' Check for common data quality issues
#' @param dt data.table with OHLCV data
#' @return List of potential issues
check_data_quality <- function(dt) {
    issues <- list()

    # 1. Missing values
    na_counts <- sapply(dt, function(x) sum(is.na(x)))
    if (any(na_counts > 0)) {
        issues$missing <- na_counts[na_counts > 0]
    }

    # 2. OHLC consistency: High should be >= Open, Close, Low
    #                      Low should be <= Open, Close, High
    ohlc_errors <- dt[high < pmax(open, close) | low > pmin(open, close)]
    if (nrow(ohlc_errors) > 0) {
        issues$ohlc_inconsistent <- nrow(ohlc_errors)
    }

    # 3. Suspicious returns (potential splits or errors)
    dt[, returns := c(NA, diff(log(close)))]
    extreme_returns <- dt[abs(returns) > 0.25, .(date, close, returns)]
    if (nrow(extreme_returns) > 0) {
        issues$extreme_returns <- extreme_returns
    }

    # 4. Gaps in dates (weekends/holidays are expected, but check for unexpected gaps)
    dt[, date_diff := c(NA, diff(date))]
    long_gaps <- dt[date_diff > 5, .(date, date_diff)]  # More than 5 days gap
    if (nrow(long_gaps) > 0) {
        issues$long_gaps <- long_gaps
    }

    # 5. Zero or negative prices
    bad_prices <- dt[open <= 0 | high <= 0 | low <= 0 | close <= 0]
    if (nrow(bad_prices) > 0) {
        issues$bad_prices <- nrow(bad_prices)
    }

    # 6. Zero volume (possible data issue unless market was closed)
    zero_vol <- dt[volume == 0]
    if (nrow(zero_vol) > 0) {
        issues$zero_volume <- nrow(zero_vol)
    }

    return(issues)
}

# Run quality checks on SPY data
spy_issues <- check_data_quality(copy(spy))
cat("Data quality report for SPY:\n")
if (length(spy_issues) == 0) {
    cat("  No issues detected\n")
} else {
    for (name in names(spy_issues)) {
        cat("  ", name, ":",
            if (is.data.table(spy_issues[[name]])) nrow(spy_issues[[name]]) else spy_issues[[name]],
            "\n")
    }
}
```

---

## 1.3 Adjusted Prices

### 1.3.1 Prose/Intuition

The closing price you see in historical data is often not the price that actually traded on that date. This is because **corporate actions** create discontinuities that, if not handled, make the data misleading or useless.

**Stock splits** multiply the number of shares while proportionally reducing the price. A 2-for-1 split means each shareholder receives an additional share, and the price halves. Without adjustment, the data shows a 50% "return" that represents no real economic change.

**Dividends** transfer cash from the company to shareholders, reducing the stock price by approximately the dividend amount on the ex-dividend date. Without adjustment, dividend-paying stocks appear to underperform their true total return.

**Adjusted prices** back-adjust historical prices so that percentage changes in the adjusted series correctly represent total returns. Most data providers offer adjusted close prices that account for both splits and dividends.

**Which price to use?**

- **Adjusted close**: Use for return calculations, backtesting, performance analysis
- **Unadjusted close**: Use for actual order prices, displaying current values, some technical analysis

The adjustment is typically applied backwards in time. Today's actual price equals the adjusted price. Historical adjusted prices are modified to preserve the correct return series.

### 1.3.2 Visual Evidence

```{r load-aapl-example}
# Load Apple data to show corporate action effects
aapl <- load_market("aapl")

cat("AAPL data loaded:\n")
cat("  Date range:", as.character(min(aapl$date)), "to", as.character(max(aapl$date)), "\n")
cat("  Has adjusted column:", "adjusted" %in% names(aapl), "\n")
```

```{r aapl-split-viz, fig.cap="Apple stock price showing the effect of adjustment for the 4-for-1 split in August 2020."}
# Focus on period around the 2020 split
aapl_split_period <- filter_dates(aapl, "2020-06-01", "2020-10-31")

# Find the approximate split date (price dropped ~75% overnight)
aapl_split_period[, returns := c(NA, diff(log(close)))]

# The split was 4:1 on August 31, 2020 - close dropped from ~500 to ~130
split_date <- as.Date("2020-08-31")

# Calculate what unadjusted prices would look like
# After split date, unadjusted = adjusted
# Before split date, unadjusted = adjusted * 4
aapl_split_period[, unadjusted := ifelse(date < split_date, adjusted * 4, adjusted)]

# Melt for plotting
price_comparison <- melt(
    aapl_split_period[, .(date, adjusted, unadjusted)],
    id.vars = "date",
    variable.name = "type",
    value.name = "price"
)
price_comparison[, type := ifelse(type == "adjusted", "Adjusted (correct)", "Unadjusted (misleading)")]

ggplot(price_comparison, aes(x = date, y = price, colour = type)) +
    geom_vline(xintercept = split_date, linetype = "dashed", colour = "grey50") +
    geom_line(linewidth = 0.7) +
    annotate("text", x = split_date, y = 350, label = "4-for-1 Split\n(Aug 31, 2020)",
             hjust = -0.05, size = 3.5) +
    scale_colour_manual(values = c("Adjusted (correct)" = trading_colors$primary,
                                    "Unadjusted (misleading)" = trading_colors$negative)) +
    labs(
        title = "Apple: Adjusted vs Unadjusted Prices Around 2020 Stock Split",
        subtitle = "Without adjustment, the split appears as a 75% crash",
        x = NULL,
        y = "Price ($)",
        colour = NULL
    ) +
    theme_trading()
```

```{r dividend-effect, fig.cap="Effect of dividend adjustments on cumulative returns. Unadjusted prices understate total return for dividend-paying stocks."}
# Demonstrate dividend adjustment effect on a high-dividend stock
# We'll simulate this since we need to show the concept clearly

set.seed(42)

# Simulate 2 years of a dividend-paying stock
n_days <- 504  # ~2 years
dates <- seq.Date(as.Date("2022-01-01"), by = "day", length.out = n_days)
dates <- dates[!weekdays(dates) %in% c("Saturday", "Sunday")]
n_days <- length(dates)

# Start at $100, modest growth, quarterly dividends
true_returns <- rnorm(n_days, mean = 0.0003, sd = 0.01)
dividend_dates <- seq(1, n_days, by = 63)  # Quarterly dividends
dividend_yield <- 0.01  # 1% quarterly dividend (4% annual)

# Build price series
price_adjusted <- 100
prices_adj <- numeric(n_days)
prices_unadj <- numeric(n_days)

for (i in 1:n_days) {
    # Apply daily return
    price_adjusted <- price_adjusted * exp(true_returns[i])
    prices_adj[i] <- price_adjusted

    # Unadjusted drops on ex-dividend dates
    if (i %in% dividend_dates && i > 1) {
        prices_unadj[i] <- prices_unadj[i-1] * exp(true_returns[i]) * (1 - dividend_yield)
    } else if (i == 1) {
        prices_unadj[i] <- 100
    } else {
        prices_unadj[i] <- prices_unadj[i-1] * exp(true_returns[i])
    }
}

div_demo <- data.table(
    date = dates,
    adjusted = prices_adj,
    unadjusted = prices_unadj
)

# Melt for plotting
div_long <- melt(div_demo, id.vars = "date", variable.name = "type", value.name = "price")
div_long[, type := ifelse(type == "adjusted", "Adjusted (Total Return)", "Unadjusted (Price Only)")]

ggplot(div_long, aes(x = date, y = price, colour = type)) +
    geom_line(linewidth = 0.6) +
    scale_colour_manual(values = c("Adjusted (Total Return)" = trading_colors$positive,
                                    "Unadjusted (Price Only)" = trading_colors$negative)) +
    labs(
        title = "Impact of Dividend Adjustments on Returns",
        subtitle = "Simulated stock with 4% annual dividend yield; unadjusted price drops at each ex-date",
        x = NULL,
        y = "Price Index (starting = $100)",
        colour = NULL
    ) +
    theme_trading()

# Calculate the difference in apparent returns
adj_return <- (tail(prices_adj, 1) / head(prices_adj, 1) - 1) * 100
unadj_return <- (tail(prices_unadj, 1) / head(prices_unadj, 1) - 1) * 100

cat("\nReturn comparison over 2 years:\n")
cat("  Adjusted (true total return):", round(adj_return, 1), "%\n")
cat("  Unadjusted (price only):", round(unadj_return, 1), "%\n")
cat("  Difference (missed dividends):", round(adj_return - unadj_return, 1), "%\n")
```

### 1.3.3 Mathematical Derivation

**Adjustment factor calculation:**

For a stock split with ratio $n:1$ (receive $n$ shares for every 1 held), the adjustment factor is:

$$f_{split} = \frac{1}{n}$$

All prices before the split date are multiplied by this factor.

For a cash dividend of amount $D$ when the pre-ex price is $P$:

$$f_{dividend} = \frac{P - D}{P} = 1 - \frac{D}{P}$$

**Chain multiplication for multiple events:**

When multiple corporate actions occur, adjustment factors multiply:

$$P_{adjusted}(t) = P_{actual}(t) \times \prod_{i: t < t_i} f_i$$

where $f_i$ is the adjustment factor for event $i$ occurring at time $t_i$.

**Example:** A stock has:
- 2-for-1 split on date $t_1$ ($f_1 = 0.5$)
- $2 dividend when price is $100 on date $t_2$ ($f_2 = 0.98$)
- 3-for-1 split on date $t_3$ ($f_3 = 0.333$)

For prices before $t_1$: $P_{adj} = P_{actual} \times 0.5 \times 0.98 \times 0.333 = P_{actual} \times 0.163$

### 1.3.4 Implementation and Application

```{r adjustment-functions}
#' Apply split adjustment to price series
#' @param prices Vector of prices (oldest to newest)
#' @param split_ratio Split ratio (e.g., 4 for 4-for-1 split)
#' @param split_idx Index of the split date (prices before this are adjusted)
#' @return Adjusted price vector
apply_split_adjustment <- function(prices, split_ratio, split_idx) {
    adjusted <- prices
    if (split_idx > 1) {
        adjusted[1:(split_idx - 1)] <- prices[1:(split_idx - 1)] / split_ratio
    }
    return(adjusted)
}

#' Apply dividend adjustment to price series
#' @param prices Vector of prices (oldest to newest)
#' @param dividend Dividend amount
#' @param ex_idx Index of ex-dividend date
#' @return Adjusted price vector
apply_dividend_adjustment <- function(prices, dividend, ex_idx) {
    adjusted <- prices
    if (ex_idx > 1) {
        # Adjustment factor based on price just before ex-date
        adj_factor <- (prices[ex_idx - 1] - dividend) / prices[ex_idx - 1]
        adjusted[1:(ex_idx - 1)] <- prices[1:(ex_idx - 1)] * adj_factor
    }
    return(adjusted)
}

#' Calculate adjustment factors from known corporate actions
#' @param dt data.table with price data
#' @param splits List of splits: list(date = "YYYY-MM-DD", ratio = n)
#' @param dividends List of dividends: list(date = "YYYY-MM-DD", amount = d)
#' @return data.table with adjusted prices
calculate_adjustments <- function(dt, splits = list(), dividends = list()) {
    result <- copy(dt)
    result[, adjusted := close]

    # Apply splits (most recent first)
    for (split in rev(splits)) {
        split_date <- as.Date(split$date)
        split_idx <- which(result$date == split_date)
        if (length(split_idx) > 0) {
            result[, adjusted := apply_split_adjustment(adjusted, split$ratio, split_idx)]
        }
    }

    # Apply dividends (most recent first)
    for (div in rev(dividends)) {
        ex_date <- as.Date(div$date)
        ex_idx <- which(result$date == ex_date)
        if (length(ex_idx) > 0) {
            result[, adjusted := apply_dividend_adjustment(adjusted, div$amount, ex_idx)]
        }
    }

    return(result)
}

# Example: Demonstrate with synthetic data
synthetic <- data.table(
    date = seq.Date(as.Date("2024-01-01"), as.Date("2024-01-10"), by = "day"),
    close = c(100, 102, 104, 52, 53, 54, 55, 56, 57, 58)  # Split on day 4
)

# Apply 2-for-1 split adjustment
adjusted <- calculate_adjustments(
    synthetic,
    splits = list(list(date = "2024-01-04", ratio = 2))
)

cat("Split adjustment example:\n")
print(adjusted[, .(date, close, adjusted, adj_factor = round(adjusted/close, 3))])
```

**When to use adjusted vs unadjusted prices:**

| Use Case | Price Type | Reason |
|----------|------------|--------|
| Return calculation | Adjusted | Correct total return |
| Backtesting | Adjusted | Realistic P&L simulation |
| Current order price | Unadjusted | Actual tradeable price |
| Price charts for reporting | Both | Context-dependent |
| Technical analysis levels | Unadjusted | Historical support/resistance at actual prices |
| Ratio analysis (P/E, etc.) | Depends | Use consistent adjustment |

**Common pitfalls:**

1. **Mixing adjusted and unadjusted**: Never calculate returns using a mix.
2. **Forward vs backward adjustment**: Most providers use backward adjustment (today's price = actual price). Ensure consistency.
3. **Stale adjustment factors**: If you download data periodically, the adjustment factors for old data may change as new dividends are declared.
4. **Split detection**: Large single-day moves (>25%) often indicate splits. Always verify.

---

## Quick Reference

### Data Loading

```r
# Load market data with our module
box::use(./src/modules/data[load_market, load_returns, list_market_symbols])

# Single asset
spy <- load_market("spy")

# Multiple assets (returns already calculated)
returns <- load_returns(c("spy", "qqq", "tlt"))

# List available symbols
symbols <- list_market_symbols()
```

### Volatility Estimators

| Estimator | Formula | Best For |
|-----------|---------|----------|
| Close-to-Close | $\sigma = \sqrt{Var(r_t)}$ | Default, widely understood |
| Parkinson | $\sigma = \sqrt{\frac{(\ln H - \ln L)^2}{4\ln 2}}$ | Higher efficiency, intraday info |
| Garman-Klass | See section 1.2.3 | Maximum efficiency with OHLC |
| Rogers-Satchell | See section 1.2.3 | Non-zero drift (trending markets) |

### Price Impact

$$\lambda = \frac{\sigma_v}{2\sigma_u}$$

where $\sigma_v$ = fundamental value uncertainty, $\sigma_u$ = noise trader volume.

### Adjustment Factors

| Corporate Action | Adjustment Factor |
|-----------------|-------------------|
| $n$-for-1 split | $f = 1/n$ |
| Cash dividend $D$ at price $P$ | $f = (P-D)/P$ |
| Multiple events | $f_{total} = \prod f_i$ |

### Quality Checks Checklist

- [ ] OHLC consistency: $L \leq O, C \leq H$
- [ ] No extreme single-day returns (>25% suggests split)
- [ ] No unexplained gaps in dates
- [ ] No zero or negative prices
- [ ] Adjusted prices available and consistent
- [ ] Volume reasonable (not all zeros)

---

*Next: Part 2 covers the mathematics of returns—simple vs log returns, compounding, and excess returns.*
