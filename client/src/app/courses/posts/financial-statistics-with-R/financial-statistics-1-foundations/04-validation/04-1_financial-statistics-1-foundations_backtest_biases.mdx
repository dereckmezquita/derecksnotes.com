---
title: "Backtesting Biases"
---



# Backtesting Biases

A backtest is a simulation of what *would have happened* if you had traded a strategy in the past. But this simulation can lie to you in systematic ways. Understanding these biases is the difference between a profitable strategy and an expensive lesson.

The biases we cover here are not edge cases—they are the primary reason most backtested strategies fail in live trading.

---

## 4.1 Look-Ahead Bias

Look-ahead bias occurs when your backtest uses information that wouldn't have been available at the time of the trading decision. It's the most common and most deadly bias.

### 4.1.1 Prose/Intuition

Imagine you're backtesting a strategy on January 15th, 2020. Your signal uses the 20-day moving average of prices. The question is: which prices?

If you use prices including January 15th to generate a signal that trades on January 15th, you've committed look-ahead bias. You're using today's close to make a decision that had to happen before the close.

**Common sources of look-ahead bias:**

1. **Using close prices for same-day signals** — You can't know the close until the market closes.

2. **Point-in-time data issues** — Financial statements are filed weeks after the quarter ends. Using Q4 data in January assumes you knew it in October.

3. **Index composition changes** — The S&P 500 today includes companies that weren't in it 10 years ago. Backtesting "buy the S&P 500" using today's constituents is look-ahead.

4. **Adjusted prices without care** — Split adjustments are applied retroactively. If your signal triggered before a split, you need the pre-split price.

### 4.1.2 Visual Evidence


``` r
# Load data
spy <- load_market("SPY")
spy <- filter_dates(spy, as.Date("2010-01-01"), as.Date("2023-12-31"))

# Simple momentum strategy: buy when price > 50-day SMA
spy[, sma_50 := frollmean(adjusted, 50)]

# WRONG: Signal uses today's close, trades at today's close (look-ahead!)
spy[, signal_wrong := fifelse(adjusted > sma_50, 1, 0)]
spy[, returns_wrong := signal_wrong * shift(log(adjusted / shift(adjusted)), -1)]

# CORRECT: Signal uses yesterday's close, trades at today's close
spy[, signal_correct := shift(fifelse(adjusted > sma_50, 1, 0), 1)]
spy[, returns_correct := signal_correct * log(adjusted / shift(adjusted))]

# Calculate cumulative returns
spy <- spy[!is.na(returns_wrong) & !is.na(returns_correct)]
spy[, cum_wrong := exp(cumsum(returns_wrong))]
spy[, cum_correct := exp(cumsum(returns_correct))]

# Plot comparison
plot_dt <- melt(spy[, .(date,
                        `Look-Ahead (Wrong)` = cum_wrong,
                        `Properly Lagged (Correct)` = cum_correct)],
                id.vars = "date",
                variable.name = "Method",
                value.name = "Wealth")

ggplot(plot_dt, aes(x = date, y = Wealth, colour = Method)) +
    geom_line(linewidth = 0.8) +
    geom_hline(yintercept = 1, linetype = "dashed", colour = "grey50") +
    scale_colour_manual(values = c(tc[4], tc[3])) +
    labs(
        title = "Look-Ahead Bias: The Silent Strategy Killer",
        subtitle = "Same strategy, different data alignment—vastly different results",
        x = NULL,
        y = "Wealth Index",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Look-ahead bias creates artificial alpha. The same strategy with proper lagging shows dramatically different results.](/courses/financial-statistics-1-foundations/../figures/04-1_lookahead-demonstration-1.png)


``` r
# Compare Sharpe ratios
sharpe_wrong <- sharpe_ratio(spy$returns_wrong[!is.na(spy$returns_wrong)])
sharpe_correct <- sharpe_ratio(spy$returns_correct[!is.na(spy$returns_correct)])

cat("=== Look-Ahead Bias Impact ===\n")
```

```
## === Look-Ahead Bias Impact ===
```

``` r
cat(sprintf("Sharpe with look-ahead (WRONG): %.2f\n", sharpe_wrong))
```

```
## Sharpe with look-ahead (WRONG): 0.53
```

``` r
cat(sprintf("Sharpe properly lagged (CORRECT): %.2f\n", sharpe_correct))
```

```
## Sharpe properly lagged (CORRECT): 0.53
```

``` r
cat(sprintf("Sharpe inflation: %.1fx\n", sharpe_wrong / sharpe_correct))
```

```
## Sharpe inflation: 1.0x
```

### 4.1.3 Mathematical Derivation

**Information set formalisation:**

Let $\Omega_t$ denote the information set available at time $t$. This includes all prices, volumes, and other data known before time $t$.

A valid trading signal $s_t$ must be $\Omega_t$-measurable:
$$s_t \in \sigma(\Omega_t)$$

where $\sigma(\Omega_t)$ is the sigma-algebra generated by the information set.

**The point-in-time constraint:**

For daily data with close prices $P_t$:
- $P_t$ is known only after market close on day $t$
- A signal generated at end of day $t$ can only use $\{P_1, P_2, ..., P_t\}$
- A trade executed at the open of day $t+1$ should use a signal based on information through day $t$

**Mathematical test for look-ahead:**

If your signal $s_t$ has correlation with future returns $r_{t+k}$ for $k > 0$ that exceeds what's achievable with lagged signals, you likely have look-ahead bias:

$$\text{Cor}(s_t, r_{t+1}) >> \text{Cor}(s_{t-1}, r_t)$$

### 4.1.4 Implementation & Application


``` r
# Function to detect potential look-ahead bias
detect_lookahead <- function(signal, returns, max_lag = 5) {
    n <- length(signal)
    correlations <- data.table(
        lag = -max_lag:max_lag,
        correlation = NA_real_
    )

    for (lag in -max_lag:max_lag) {
        if (lag < 0) {
            # Signal leads returns (potential look-ahead)
            sig <- signal[1:(n + lag)]
            ret <- returns[(1 - lag):n]
        } else if (lag > 0) {
            # Signal lags returns (normal)
            sig <- signal[(1 + lag):n]
            ret <- returns[1:(n - lag)]
        } else {
            sig <- signal
            ret <- returns
        }

        valid <- !is.na(sig) & !is.na(ret)
        if (sum(valid) > 30) {
            correlations[lag == (lag), correlation := cor(sig[valid], ret[valid])]
        }
    }

    correlations
}

# Test our signals
signal_test <- spy$signal_wrong
returns_test <- log(spy$adjusted / shift(spy$adjusted))

lookahead_check <- detect_lookahead(
    signal_test[!is.na(signal_test)],
    returns_test[!is.na(returns_test)]
)

cat("\n=== Look-Ahead Detection ===\n")
```

```
## 
## === Look-Ahead Detection ===
```

``` r
cat("Correlation at each lag (negative lag = signal uses future info):\n")
```

```
## Correlation at each lag (negative lag = signal uses future info):
```

``` r
print(lookahead_check)
```

```
##       lag correlation
##     <int>       <num>
##  1:    -5   0.1495933
##  2:    -4   0.1495933
##  3:    -3   0.1495933
##  4:    -2   0.1495933
##  5:    -1   0.1495933
##  6:     0   0.1495933
##  7:     1   0.1495933
##  8:     2   0.1495933
##  9:     3   0.1495933
## 10:     4   0.1495933
## 11:     5   0.1495933
```


``` r
ggplot(lookahead_check[!is.na(correlation)],
       aes(x = lag, y = correlation, fill = lag < 0)) +
    geom_col(alpha = 0.8) +
    geom_hline(yintercept = 0, colour = "grey50") +
    geom_vline(xintercept = 0, linetype = "dashed", colour = "grey30") +
    scale_fill_manual(values = c("TRUE" = tc[4], "FALSE" = tc[3]),
                      labels = c("TRUE" = "Potential Look-Ahead", "FALSE" = "Valid"),
                      name = NULL) +
    labs(
        title = "Look-Ahead Bias Detection via Lag Analysis",
        subtitle = "Signal correlation with returns at each lag",
        x = "Lag (negative = signal uses future returns)",
        y = "Correlation"
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Look-ahead detection: correlation between signal and returns at various lags. Correlation at negative lags suggests look-ahead bias.](/courses/financial-statistics-1-foundations/../figures/04-1_lookahead-test-plot-1.png)

**Common look-ahead pitfalls and fixes:**


``` r
# WRONG: Using today's data for today's decision
# spy[, signal := adjusted > frollmean(adjusted, 20)]

# CORRECT: Lag the signal by 1 day
# spy[, signal := shift(adjusted > frollmean(adjusted, 20), 1)]

# WRONG: Using point-in-time adjusted close (includes future splits)
# For precise work, use adjustment factors calculated only from known events

# CORRECT: Track adjustment factor chain
calculate_adjustment_factor <- function(prices, split_dates, split_ratios) {
    # Only apply adjustments known at each point in time
    # This is more complex but eliminates look-ahead in adjustments
    n <- length(prices)
    adj_factor <- rep(1, n)

    for (i in seq_along(split_dates)) {
        split_idx <- which(names(prices) == as.character(split_dates[i]))
        if (length(split_idx) > 0 && split_idx > 1) {
            # Apply adjustment only to prices BEFORE the split date
            # and only after we know about the split
            adj_factor[1:(split_idx - 1)] <- adj_factor[1:(split_idx - 1)] * split_ratios[i]
        }
    }

    adj_factor
}

cat("\nKey look-ahead avoidance rules:\n")
```

```
## 
## Key look-ahead avoidance rules:
```

``` r
cat("1. Always shift signals by at least 1 period\n")
```

```
## 1. Always shift signals by at least 1 period
```

``` r
cat("2. Use point-in-time databases for fundamentals\n")
```

```
## 2. Use point-in-time databases for fundamentals
```

``` r
cat("3. Track index composition changes historically\n")
```

```
## 3. Track index composition changes historically
```

``` r
cat("4. Be explicit about when information becomes available\n")
```

```
## 4. Be explicit about when information becomes available
```

---

## 4.2 Survivorship Bias

Survivorship bias occurs when your backtest only includes securities that survived to the present day, ignoring those that were delisted, went bankrupt, or were acquired.

### 4.2.1 Prose/Intuition

Imagine backtesting a strategy that buys distressed stocks—companies trading below book value with high debt. Many such companies in 2005 no longer exist today. If your database only contains currently-trading stocks, you're missing all the bankruptcies.

**The result:** Your backtest shows you buying cheap companies that recovered, when in reality many went to zero.

**Where survivorship bias hides:**

1. **Stock databases** — Yahoo Finance, most free data sources only include current listings.

2. **Index backtests** — The S&P 500 in 2010 included companies that were later removed (often after declines).

3. **Mutual fund databases** — Funds that closed due to poor performance disappear from returns.

4. **ETF databases** — Failed ETFs are delisted and their history is lost.

### 4.2.2 Visual Evidence


``` r
# Simulate survivorship bias effect
# Create a universe of 100 stocks, some of which will "die"

set.seed(123)
n_stocks <- 100
n_days <- 2520  # 10 years

# Simulate stock returns
# Some stocks will have negative drift and eventually delist
simulate_stock_universe <- function(n_stocks, n_days, death_rate = 0.05) {
    returns_matrix <- matrix(NA, n_days, n_stocks)
    alive <- rep(TRUE, n_stocks)

    # Assign drift to each stock (some are bad)
    drifts <- rnorm(n_stocks, mean = 0.0003, sd = 0.001)  # Daily drift
    drifts[1:20] <- drifts[1:20] - 0.002  # 20% of stocks have negative drift

    vols <- runif(n_stocks, 0.01, 0.03)  # Daily volatility

    for (i in 1:n_stocks) {
        if (!alive[i]) next

        # Simulate price path
        returns <- rnorm(n_days, mean = drifts[i], sd = vols[i])
        price <- cumprod(c(100, 1 + returns[-1]))

        # Check for delisting (price drops below $1)
        delist_idx <- which(price < 1)
        if (length(delist_idx) > 0) {
            first_delist <- min(delist_idx)
            returns[first_delist:n_days] <- NA
            returns[first_delist] <- -1  # Total loss at delisting
        }

        returns_matrix[, i] <- returns
    }

    returns_matrix
}

returns_full <- simulate_stock_universe(n_stocks, n_days)

# Count survivors
survivors <- apply(returns_full, 2, function(x) !any(is.na(x)))
n_survivors <- sum(survivors)

cat(sprintf("Starting stocks: %d\n", n_stocks))
```

```
## Starting stocks: 100
```

``` r
cat(sprintf("Survivors after 10 years: %d (%.1f%%)\n",
            n_survivors, n_survivors / n_stocks * 100))
```

```
## Survivors after 10 years: 89 (89.0%)
```

``` r
# Compare equal-weight portfolio returns
# Survivorship-biased: Only use stocks that survived the whole period
returns_survivors <- returns_full[, survivors]
portfolio_biased <- rowMeans(returns_survivors, na.rm = TRUE)

# Survivorship-free: Use all stocks, dropping them when they delist
portfolio_unbiased <- rowMeans(returns_full, na.rm = TRUE)

# Create comparison data
compare_dt <- data.table(
    day = 1:n_days,
    date = seq.Date(as.Date("2013-01-01"), by = "day", length.out = n_days),
    Biased = exp(cumsum(portfolio_biased)),
    Unbiased = exp(cumsum(portfolio_unbiased))
)

compare_long <- melt(compare_dt, id.vars = c("day", "date"),
                     variable.name = "Portfolio", value.name = "Wealth")

ggplot(compare_long, aes(x = date, y = Wealth, colour = Portfolio)) +
    geom_line(linewidth = 0.8) +
    geom_hline(yintercept = 1, linetype = "dashed", colour = "grey50") +
    scale_colour_manual(values = c(tc[4], tc[3]),
                        labels = c("Survivorship Biased", "Survivorship Free")) +
    labs(
        title = "Survivorship Bias: The Invisible Performance Boost",
        subtitle = sprintf("Equal-weight portfolio simulation: %d of %d stocks delisted",
                          n_stocks - n_survivors, n_stocks),
        x = NULL,
        y = "Wealth Index",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Survivorship bias inflates returns. The same strategy run on survivorship-free data shows much lower performance.](/courses/financial-statistics-1-foundations/../figures/04-1_survivorship-simulation-1.png)


``` r
# Calculate the magnitude of bias
sharpe_biased <- mean(portfolio_biased, na.rm = TRUE) / sd(portfolio_biased, na.rm = TRUE) * sqrt(252)
sharpe_unbiased <- mean(portfolio_unbiased, na.rm = TRUE) / sd(portfolio_unbiased, na.rm = TRUE) * sqrt(252)

ann_ret_biased <- mean(portfolio_biased, na.rm = TRUE) * 252
ann_ret_unbiased <- mean(portfolio_unbiased, na.rm = TRUE) * 252

cat("\n=== Survivorship Bias Magnitude ===\n")
```

```
## 
## === Survivorship Bias Magnitude ===
```

``` r
cat(sprintf("Biased annual return: %.2f%%\n", ann_ret_biased * 100))
```

```
## Biased annual return: 8.90%
```

``` r
cat(sprintf("Unbiased annual return: %.2f%%\n", ann_ret_unbiased * 100))
```

```
## Unbiased annual return: 2.26%
```

``` r
cat(sprintf("Bias magnitude: %.2f%% annual\n", (ann_ret_biased - ann_ret_unbiased) * 100))
```

```
## Bias magnitude: 6.64% annual
```

``` r
cat(sprintf("\nBiased Sharpe: %.2f\n", sharpe_biased))
```

```
## 
## Biased Sharpe: 2.53
```

``` r
cat(sprintf("Unbiased Sharpe: %.2f\n", sharpe_unbiased))
```

```
## Unbiased Sharpe: 0.64
```

### 4.2.3 Mathematical Derivation

**Selection bias formalisation:**

Let $S_T$ denote the set of securities that survive to time $T$ (present).

The survivorship-biased estimator of expected return is:
$$\hat{\mu}_{biased} = \frac{1}{|S_T|} \sum_{i \in S_T} \bar{r}_i$$

The unbiased estimator includes all securities that existed at each point:
$$\hat{\mu}_{unbiased} = \frac{1}{T} \sum_{t=1}^{T} \frac{1}{|S_t|} \sum_{i \in S_t} r_{i,t}$$

where $S_t$ is the set of securities alive at time $t$.

**Expected magnitude of bias:**

If securities delist with probability $p$ per year, and delisting is associated with below-average returns $\mu_{delist}$:

$$E[\hat{\mu}_{biased} - \hat{\mu}_{unbiased}] \approx p \cdot (\bar{\mu} - \mu_{delist})$$

Empirically:
- US equity attrition rate: ~5-10% per year (including mergers, bankruptcies, delistings)
- Delistings due to poor performance: average return of -30% to -50% in final year
- **Estimated survivorship bias: 1-2% annual inflation**

### 4.2.4 Implementation & Application


``` r
# Strategies for survivorship-free backtesting

# 1. Use point-in-time databases
# Examples: CRSP, Compustat with delisting returns, QuantQuote delisting files

# 2. Implement proper delisting return handling
handle_delisting <- function(returns_dt, delisting_dt) {
    # delisting_dt should have: date, symbol, delisting_return
    # Merge delisting returns into main dataset

    # When a stock delists:
    # - Use the delisting return for the final observation
    # - Set all subsequent returns to NA
    # - Do NOT simply drop the stock retroactively

    result <- copy(returns_dt)

    for (i in 1:nrow(delisting_dt)) {
        symbol <- delisting_dt$symbol[i]
        delist_date <- delisting_dt$date[i]
        delist_return <- delisting_dt$delisting_return[i]

        # Find the stock's rows
        stock_rows <- which(result$symbol == symbol)

        if (length(stock_rows) > 0) {
            # Find the delisting date row
            delist_row <- which(result$symbol == symbol & result$date == delist_date)

            if (length(delist_row) > 0) {
                # Apply delisting return
                result[delist_row, return := delist_return]

                # Mark subsequent rows as NA (stock no longer tradeable)
                later_rows <- which(result$symbol == symbol & result$date > delist_date)
                if (length(later_rows) > 0) {
                    result[later_rows, return := NA]
                }
            }
        }
    }

    result
}

# 3. Use historical index constituents
# Don't backtest "S&P 500" using today's constituents
# Use point-in-time constituent lists

cat("\nSurvivorship-Free Data Sources:\n")
```

```
## 
## Survivorship-Free Data Sources:
```

``` r
cat("1. CRSP (US equities, includes delisting returns)\n")
```

```
## 1. CRSP (US equities, includes delisting returns)
```

``` r
cat("2. Compustat with point-in-time fundamentals\n")
```

```
## 2. Compustat with point-in-time fundamentals
```

``` r
cat("3. QuantQuote delisting supplement\n")
```

```
## 3. QuantQuote delisting supplement
```

``` r
cat("4. Bloomberg point-in-time index constituents\n")
```

```
## 4. Bloomberg point-in-time index constituents
```

``` r
cat("5. SEC EDGAR filings with original file dates\n")
```

```
## 5. SEC EDGAR filings with original file dates
```


``` r
# Demonstrate the concept (simplified simulation)
# In practice, you'd use actual historical constituent data

# Simulate index with turnover
n_years <- 10
n_constituents <- 500
annual_turnover <- 0.05  # 5% annual turnover

# Stocks that leave the index typically underperform before removal
# Stocks that join typically outperform before addition

simulate_index_bias <- function(n_years, n_constituents, turnover) {
    n_days <- n_years * 252

    # Current constituents: all positive drift
    current_returns <- rnorm(n_days, mean = 0.0004, sd = 0.01)

    # Historical constituents: include underperformers that were removed
    # Simulate the drag from stocks that were in the index but later removed
    removed_drag <- rnorm(n_days, mean = -0.0001 * turnover * n_constituents, sd = 0.001)
    historical_returns <- current_returns + removed_drag

    data.table(
        day = 1:n_days,
        current_only = exp(cumsum(current_returns)),
        historical = exp(cumsum(historical_returns))
    )
}

index_sim <- simulate_index_bias(10, 500, 0.05)
index_long <- melt(index_sim, id.vars = "day",
                   variable.name = "Method", value.name = "Wealth")
index_long[, Method := fifelse(Method == "current_only",
                               "Current Constituents (Biased)",
                               "Historical Constituents (Correct)")]

ggplot(index_long, aes(x = day, y = Wealth, colour = Method)) +
    geom_line(linewidth = 0.8) +
    scale_colour_manual(values = c(tc[4], tc[3])) +
    labs(
        title = "Index Constituent Survivorship Bias",
        subtitle = "Backtesting with today's S&P 500 vs historical constituents",
        x = "Trading Days",
        y = "Wealth Index",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![S&P 500 backtest: using current constituents vs historical constituents shows different results.](/courses/financial-statistics-1-foundations/../figures/04-1_survivorship-index-example-1.png)

---

## 4.3 Overfitting

Overfitting occurs when your strategy captures noise rather than signal. It's the inevitable consequence of testing many configurations on finite data.

### 4.3.1 Prose/Intuition

You test a moving average crossover strategy with lookback periods from 5 to 200 days. That's nearly 40,000 combinations (200 × 200). Some combination will look amazing—by pure chance.

**The fundamental problem:** Financial returns have low signal-to-noise ratios. With enough parameters and tests, you can fit any historical pattern. But that pattern probably won't persist.

**Signs of overfitting:**

1. In-sample performance much better than simple benchmarks
2. Strategy requires very specific parameter values
3. Small parameter changes dramatically affect results
4. Strategy "worked" only in specific time periods
5. No economic rationale for why the strategy should work

### 4.3.2 Visual Evidence


``` r
# Demonstrate overfitting with increasing model complexity

spy_data <- load_market("SPY")
spy_data <- filter_dates(spy_data, as.Date("2000-01-01"), as.Date("2023-12-31"))
spy_data[, returns := c(NA, diff(log(adjusted)))]
spy_data <- spy_data[!is.na(returns)]

# Split data
split_date <- as.Date("2015-01-01")
train <- spy_data[date < split_date]
test <- spy_data[date >= split_date]

# Function to fit polynomial model and calculate Sharpe
fit_polynomial_strategy <- function(train_data, test_data, degree) {
    # Create lagged features
    train_dt <- copy(train_data)
    test_dt <- copy(test_data)

    for (i in 1:min(degree, 20)) {
        train_dt[, paste0("lag_", i) := shift(returns, i)]
        test_dt[, paste0("lag_", i) := shift(returns, i)]
    }

    train_dt <- train_dt[complete.cases(train_dt)]
    test_dt <- test_dt[complete.cases(test_dt)]

    # Fit model: predict return from lagged returns
    if (degree <= 20) {
        formula_str <- paste("returns ~",
                             paste(paste0("lag_", 1:degree), collapse = " + "))
    } else {
        # Add interaction terms for higher complexity
        formula_str <- paste("returns ~",
                             paste(paste0("lag_", 1:20), collapse = " + "),
                             "+",
                             paste(paste0("I(lag_1^", 2:min(degree-19, 5), ")"), collapse = " + "))
    }

    model <- tryCatch(
        lm(as.formula(formula_str), data = train_dt),
        error = function(e) NULL
    )

    if (is.null(model)) {
        return(list(train_sharpe = NA, test_sharpe = NA))
    }

    # Generate signals
    train_pred <- predict(model, train_dt)
    test_pred <- predict(model, test_dt)

    # Simple signal: go long if predicted return > 0
    train_signal <- ifelse(train_pred > 0, 1, -1)
    test_signal <- ifelse(test_pred > 0, 1, -1)

    # Strategy returns
    train_strat_ret <- train_signal * train_dt$returns
    test_strat_ret <- test_signal * test_dt$returns

    list(
        train_sharpe = mean(train_strat_ret) / sd(train_strat_ret) * sqrt(252),
        test_sharpe = mean(test_strat_ret) / sd(test_strat_ret) * sqrt(252)
    )
}

# Test various complexity levels
complexity_levels <- c(1, 2, 3, 5, 10, 15, 20)
results <- lapply(complexity_levels, function(d) {
    res <- fit_polynomial_strategy(train, test, d)
    data.table(
        complexity = d,
        train_sharpe = res$train_sharpe,
        test_sharpe = res$test_sharpe
    )
})
results_dt <- rbindlist(results)

# Plot
results_long <- melt(results_dt, id.vars = "complexity",
                     variable.name = "Sample", value.name = "Sharpe")
results_long[, Sample := fifelse(Sample == "train_sharpe", "In-Sample", "Out-of-Sample")]

ggplot(results_long, aes(x = complexity, y = Sharpe, colour = Sample)) +
    geom_line(linewidth = 1) +
    geom_point(size = 3) +
    geom_hline(yintercept = 0, linetype = "dashed", colour = "grey50") +
    scale_colour_manual(values = c(tc[3], tc[4])) +
    labs(
        title = "Overfitting: The Complexity Trap",
        subtitle = "In-sample performance rises with complexity; out-of-sample collapses",
        x = "Model Complexity (Number of Parameters)",
        y = "Sharpe Ratio",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Overfitting in action: in-sample Sharpe increases with complexity, but out-of-sample Sharpe collapses.](/courses/financial-statistics-1-foundations/../figures/04-1_overfitting-demonstration-1.png)


``` r
# Show parameter sensitivity for moving average crossover
calculate_ma_sharpe <- function(data, fast, slow) {
    if (fast >= slow) return(NA)

    dt <- copy(data)
    dt[, ma_fast := frollmean(adjusted, fast)]
    dt[, ma_slow := frollmean(adjusted, slow)]
    dt[, signal := shift(fifelse(ma_fast > ma_slow, 1, -1), 1)]
    dt[, strat_ret := signal * c(NA, diff(log(adjusted)))]

    dt <- dt[!is.na(strat_ret)]
    if (nrow(dt) < 100) return(NA)

    mean(dt$strat_ret) / sd(dt$strat_ret) * sqrt(252)
}

# Test parameter grid
fast_range <- seq(5, 50, by = 5)
slow_range <- seq(20, 200, by = 10)

param_grid <- expand.grid(fast = fast_range, slow = slow_range)
param_grid$sharpe <- mapply(function(f, s) calculate_ma_sharpe(train, f, s),
                             param_grid$fast, param_grid$slow)

param_dt <- as.data.table(param_grid)

ggplot(param_dt[!is.na(sharpe)], aes(x = fast, y = slow, fill = sharpe)) +
    geom_tile() +
    scale_fill_gradient2(low = tc[4], mid = "white", high = tc[3], midpoint = 0,
                         name = "Sharpe") +
    labs(
        title = "Parameter Sensitivity Heatmap: Moving Average Crossover",
        subtitle = "Robust strategies show gradual performance transitions; overfitted show isolated peaks",
        x = "Fast MA Period",
        y = "Slow MA Period"
    ) +
    theme_trading()
```

![Parameter sensitivity analysis: overfitted strategies show 'islands' of good performance.](/courses/financial-statistics-1-foundations/../figures/04-1_overfitting-parameter-sensitivity-1.png)

### 4.3.3 Mathematical Derivation

**Bias-variance trade-off:**

The expected out-of-sample error can be decomposed:
$$E[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

For trading strategies:
- **Bias:** Error from oversimplified models that miss real patterns
- **Variance:** Error from models that fit noise in training data
- **Irreducible:** Inherent unpredictability of returns

As model complexity increases:
- Bias ↓ (model can capture more patterns)
- Variance ↑ (model fits more noise)

**Effective degrees of freedom:**

A model with $p$ parameters fitted to $T$ observations has approximately $p$ degrees of freedom. A rule of thumb:
$$\text{max parameters} \approx \sqrt{T}$$

For 10 years of daily data (T ≈ 2,520):
$$p_{max} \approx \sqrt{2520} \approx 50$$

More than ~50 free parameters risk severe overfitting.

**Bailey-de Prado deflated Sharpe ratio:**

When you test $K$ strategies, the expected maximum Sharpe under the null (no skill) is:
$$E[\max_{k} SR_k | H_0] \approx \sqrt{2 \ln(K)} \cdot \frac{1}{\sqrt{T}}$$

For $K = 1000$ trials and $T = 2520$ days:
$$E[\max SR | H_0] \approx \sqrt{2 \ln(1000)} \cdot \frac{1}{\sqrt{2520}} \approx 0.074$$

**Deflated Sharpe Ratio:**
$$SR_{deflated} = SR_{observed} - E[\max SR | H_0]$$

### 4.3.4 Implementation & Application


``` r
# Function to calculate deflated Sharpe ratio
deflated_sharpe <- function(observed_sharpe, n_trials, n_observations) {
    # Expected max Sharpe under null
    expected_max <- sqrt(2 * log(n_trials)) / sqrt(n_observations)

    # Deflated Sharpe
    observed_sharpe - expected_max
}

# Example: You tested 500 parameter combinations
observed_sr <- 1.2
n_trials <- 500
n_obs <- nrow(train)

deflated_sr <- deflated_sharpe(observed_sr, n_trials, n_obs)

cat("=== Overfitting Adjustment ===\n")
```

```
## === Overfitting Adjustment ===
```

``` r
cat(sprintf("Observed Sharpe: %.2f\n", observed_sr))
```

```
## Observed Sharpe: 1.20
```

``` r
cat(sprintf("Number of trials: %d\n", n_trials))
```

```
## Number of trials: 500
```

``` r
cat(sprintf("Sample size: %d days\n", n_obs))
```

```
## Sample size: 2516 days
```

``` r
cat(sprintf("Expected max Sharpe under null: %.3f\n",
            sqrt(2 * log(n_trials)) / sqrt(n_obs)))
```

```
## Expected max Sharpe under null: 0.070
```

``` r
cat(sprintf("Deflated Sharpe: %.2f\n", deflated_sr))
```

```
## Deflated Sharpe: 1.13
```


``` r
# Implement time-series cross-validation
ts_cross_validate <- function(data, model_func, n_folds = 5, min_train = 500) {
    n <- nrow(data)
    fold_size <- floor((n - min_train) / n_folds)

    results <- vector("list", n_folds)

    for (i in 1:n_folds) {
        train_end <- min_train + (i - 1) * fold_size
        val_start <- train_end + 1
        val_end <- min(val_start + fold_size - 1, n)

        train_data <- data[1:train_end]
        val_data <- data[val_start:val_end]

        # Get train and validation performance
        train_perf <- model_func(train_data, train_data)
        val_perf <- model_func(train_data, val_data)

        results[[i]] <- data.table(
            fold = i,
            train_end_date = data$date[train_end],
            train_sharpe = train_perf,
            val_sharpe = val_perf
        )
    }

    rbindlist(results)
}

# Simple MA crossover model function
ma_model <- function(train_data, test_data) {
    # Use parameters from training
    train_dt <- copy(train_data)
    train_dt[, ma_fast := frollmean(adjusted, 20)]
    train_dt[, ma_slow := frollmean(adjusted, 50)]
    train_dt[, signal := shift(fifelse(ma_fast > ma_slow, 1, -1), 1)]
    train_dt[, strat_ret := signal * c(NA, diff(log(adjusted)))]

    # Apply to test data
    test_dt <- copy(test_data)
    test_dt[, ma_fast := frollmean(adjusted, 20)]
    test_dt[, ma_slow := frollmean(adjusted, 50)]
    test_dt[, signal := shift(fifelse(ma_fast > ma_slow, 1, -1), 1)]
    test_dt[, strat_ret := signal * c(NA, diff(log(adjusted)))]

    test_dt <- test_dt[!is.na(strat_ret)]
    if (nrow(test_dt) < 50) return(NA)

    mean(test_dt$strat_ret) / sd(test_dt$strat_ret) * sqrt(252)
}

cv_results <- ts_cross_validate(spy_data, ma_model, n_folds = 8)

cv_long <- melt(cv_results, id.vars = c("fold", "train_end_date"),
                measure.vars = c("train_sharpe", "val_sharpe"),
                variable.name = "Sample", value.name = "Sharpe")
cv_long[, Sample := fifelse(Sample == "train_sharpe", "Training", "Validation")]

ggplot(cv_long, aes(x = fold, y = Sharpe, fill = Sample)) +
    geom_col(position = "dodge", alpha = 0.8) +
    geom_hline(yintercept = 0, linetype = "dashed", colour = "grey50") +
    scale_fill_manual(values = c(tc[3], tc[4])) +
    labs(
        title = "Time-Series Cross-Validation Results",
        subtitle = "Large train-validation gaps indicate overfitting",
        x = "Fold",
        y = "Sharpe Ratio",
        fill = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Cross-validation helps detect overfitting. The gap between train and validation performance signals overfit.](/courses/financial-statistics-1-foundations/../figures/04-1_overfitting-regularisation-1.png)

---

## 4.4 Selection Bias and Data Mining

Selection bias occurs when you report only your best results while hiding the failures. Data mining is the process of searching through many hypotheses until you find one that "works."

### 4.4.1 Prose/Intuition

A researcher tests 1,000 trading strategies. By chance alone, about 50 will have p-values below 0.05. She publishes the best one with a Sharpe of 2.0 and never mentions the 999 failures.

This is publication bias, p-hacking, and data mining rolled into one. It's endemic in quantitative finance.

**The multiple testing problem:**

If you flip a coin 20 times, getting 15 heads (p = 0.02) seems significant. But if you flipped 1,000 different coins and reported only the best result, that 15/20 is completely expected.

### 4.4.2 Visual Evidence


``` r
# Demonstrate multiple testing problem
n_strategies <- 1000
n_days <- 500  # 2 years of daily data

# Simulate null strategies (no actual edge)
set.seed(456)
returns <- rnorm(n_days, mean = 0.0002, sd = 0.01)  # Market returns

simulate_null_strategy <- function(returns) {
    # Random weights that sum to 1
    n <- length(returns)
    entry_points <- sample(1:n, size = floor(n * 0.3))

    signal <- rep(0, n)
    signal[entry_points] <- sample(c(-1, 1), length(entry_points), replace = TRUE)

    strat_returns <- signal * returns

    # Calculate t-statistic for mean return
    mean_ret <- mean(strat_returns)
    se <- sd(strat_returns) / sqrt(n)
    t_stat <- mean_ret / se
    p_value <- 2 * (1 - pt(abs(t_stat), df = n - 1))
    sharpe <- mean_ret / sd(strat_returns) * sqrt(252)

    c(t_stat = t_stat, p_value = p_value, sharpe = sharpe)
}

strategy_results <- t(replicate(n_strategies, simulate_null_strategy(returns)))
strategy_dt <- as.data.table(strategy_results)

# Count "significant" results
n_significant <- sum(strategy_dt$p_value < 0.05)
best_sharpe <- max(strategy_dt$sharpe)
best_t <- max(abs(strategy_dt$t_stat))

cat("=== Multiple Testing Demonstration ===\n")
```

```
## === Multiple Testing Demonstration ===
```

``` r
cat(sprintf("Strategies tested: %d\n", n_strategies))
```

```
## Strategies tested: 1000
```

``` r
cat(sprintf("Significant at p < 0.05: %d (%.1f%%)\n",
            n_significant, n_significant / n_strategies * 100))
```

```
## Significant at p < 0.05: 46 (4.6%)
```

``` r
cat(sprintf("Best Sharpe ratio: %.2f\n", best_sharpe))
```

```
## Best Sharpe ratio: 1.91
```

``` r
cat(sprintf("Best t-statistic: %.2f\n", best_t))
```

```
## Best t-statistic: 3.40
```

``` r
cat(sprintf("Expected significant by chance: %d\n", floor(n_strategies * 0.05)))
```

```
## Expected significant by chance: 50
```

``` r
# Plot distribution of Sharpes
ggplot(strategy_dt, aes(x = sharpe)) +
    geom_histogram(bins = 50, fill = tc[5], alpha = 0.8) +
    geom_vline(xintercept = best_sharpe, colour = tc[4], linewidth = 1.2) +
    geom_vline(xintercept = 0, colour = "grey50", linetype = "dashed") +
    annotate("text", x = best_sharpe + 0.1, y = 50,
             label = sprintf("Best: %.2f", best_sharpe),
             colour = tc[4], hjust = 0) +
    labs(
        title = sprintf("Sharpe Distribution: %d Null Strategies", n_strategies),
        subtitle = "All strategies have zero true edge—best result is pure luck",
        x = "Sharpe Ratio",
        y = "Frequency"
    ) +
    theme_trading()
```

![Multiple testing in action: testing 1000 random strategies produces many 'significant' results by chance.](/courses/financial-statistics-1-foundations/../figures/04-1_multiple-testing-demo-1.png)


``` r
# Apply multiple testing corrections
raw_p <- strategy_dt$p_value

# Bonferroni correction (most conservative)
bonferroni_p <- pmin(raw_p * n_strategies, 1)

# Benjamini-Hochberg (FDR control)
bh_p <- p.adjust(raw_p, method = "BH")

# Holm correction
holm_p <- p.adjust(raw_p, method = "holm")

corrections_dt <- data.table(
    strategy = 1:n_strategies,
    raw = raw_p,
    bonferroni = bonferroni_p,
    bh = bh_p,
    holm = holm_p
)

# Count significant after corrections
cat("\n=== After Multiple Testing Correction ===\n")
```

```
## 
## === After Multiple Testing Correction ===
```

``` r
cat(sprintf("Raw p < 0.05: %d\n", sum(raw_p < 0.05)))
```

```
## Raw p < 0.05: 46
```

``` r
cat(sprintf("Bonferroni p < 0.05: %d\n", sum(bonferroni_p < 0.05)))
```

```
## Bonferroni p < 0.05: 0
```

``` r
cat(sprintf("Benjamini-Hochberg p < 0.05: %d\n", sum(bh_p < 0.05)))
```

```
## Benjamini-Hochberg p < 0.05: 0
```

``` r
cat(sprintf("Holm p < 0.05: %d\n", sum(holm_p < 0.05)))
```

```
## Holm p < 0.05: 0
```

``` r
# Plot correction comparison
corrections_long <- melt(corrections_dt[order(raw)][1:100],
                         id.vars = "strategy",
                         variable.name = "Method",
                         value.name = "p_value")

ggplot(corrections_long, aes(x = strategy, y = -log10(p_value), colour = Method)) +
    geom_line(linewidth = 0.8) +
    geom_hline(yintercept = -log10(0.05), linetype = "dashed", colour = "grey50") +
    annotate("text", x = 90, y = -log10(0.05) + 0.3, label = "p = 0.05", colour = "grey50") +
    scale_colour_manual(values = c(tc[1], tc[2], tc[3], tc[4])) +
    labs(
        title = "Multiple Testing Corrections Comparison",
        subtitle = "Top 100 strategies ranked by raw p-value",
        x = "Strategy Rank (by raw p-value)",
        y = "-log10(p-value)",
        colour = NULL
    ) +
    theme_trading() +
    theme(legend.position = "bottom")
```

![Multiple testing corrections adjust p-values to control false discovery rate.](/courses/financial-statistics-1-foundations/../figures/04-1_multiple-testing-corrections-1.png)

### 4.4.3 Mathematical Derivation

**Family-wise error rate (FWER):**

The probability of at least one false positive when testing $m$ hypotheses:
$$FWER = P(\text{at least one false positive}) = 1 - (1 - \alpha)^m$$

For $m = 100$ tests at $\alpha = 0.05$:
$$FWER = 1 - 0.95^{100} \approx 0.994$$

You're almost guaranteed at least one false positive!

**Bonferroni correction:**

To control FWER at level $\alpha$, use threshold $\alpha/m$:
$$\text{Reject } H_i \text{ if } p_i < \alpha/m$$

This is very conservative. For 1000 tests, you need $p < 0.00005$.

**Benjamini-Hochberg (False Discovery Rate):**

FDR controls the expected proportion of false discoveries:
$$FDR = E\left[\frac{\text{False Positives}}{\text{Total Positives}}\right]$$

Procedure:
1. Order p-values: $p_{(1)} \leq p_{(2)} \leq ... \leq p_{(m)}$
2. Find largest $k$ such that $p_{(k)} \leq \frac{k}{m} \alpha$
3. Reject hypotheses $1, 2, ..., k$

**Harvey et al. (2016) threshold:**

Given the historical number of tested factors in finance (hundreds to thousands), they recommend:
$$t > 3.0 \text{ for new factor discoveries}$$

This is much stricter than the traditional $t > 2.0$ threshold.

### 4.4.4 Implementation & Application


``` r
# Framework for preventing data mining

# 1. Pre-registration: Document hypotheses before testing
preregister_strategy <- function(strategy_name, hypothesis, parameters,
                                  test_period, success_criterion) {
    doc <- list(
        timestamp = Sys.time(),
        strategy_name = strategy_name,
        hypothesis = hypothesis,
        parameters = parameters,
        test_period = test_period,
        success_criterion = success_criterion,
        status = "pre-registered"
    )

    # In practice, save this to a file with timestamp
    cat("=== Strategy Pre-Registration ===\n")
    cat(sprintf("Strategy: %s\n", strategy_name))
    cat(sprintf("Hypothesis: %s\n", hypothesis))
    cat(sprintf("Success criterion: %s\n", success_criterion))
    cat(sprintf("Registered at: %s\n", doc$timestamp))

    doc
}

# Example pre-registration
preregister_strategy(
    strategy_name = "Momentum 12-1",
    hypothesis = "12-month momentum with 1-month skip predicts future returns",
    parameters = list(lookback = 252, skip = 21),
    test_period = "2020-01-01 to 2023-12-31",
    success_criterion = "Sharpe > 0.5, t-stat > 2.0"
)
```

```
## === Strategy Pre-Registration ===
## Strategy: Momentum 12-1
## Hypothesis: 12-month momentum with 1-month skip predicts future returns
## Success criterion: Sharpe > 0.5, t-stat > 2.0
## Registered at: 2026-01-21 10:39:01.766154
```

```
## $timestamp
## [1] "2026-01-21 10:39:01 CST"
## 
## $strategy_name
## [1] "Momentum 12-1"
## 
## $hypothesis
## [1] "12-month momentum with 1-month skip predicts future returns"
## 
## $parameters
## $parameters$lookback
## [1] 252
## 
## $parameters$skip
## [1] 21
## 
## 
## $test_period
## [1] "2020-01-01 to 2023-12-31"
## 
## $success_criterion
## [1] "Sharpe > 0.5, t-stat > 2.0"
## 
## $status
## [1] "pre-registered"
```


``` r
# 2. Keep a complete research log
# Track ALL strategies tested, not just winners

create_research_log <- function() {
    data.table(
        timestamp = as.POSIXct(character()),
        strategy_id = character(),
        description = character(),
        parameters = character(),
        in_sample_sharpe = numeric(),
        out_sample_sharpe = numeric(),
        t_statistic = numeric(),
        p_value = numeric(),
        notes = character()
    )
}

log_strategy_test <- function(log, strategy_id, description, parameters,
                               in_sample_sharpe, out_sample_sharpe,
                               t_stat, p_val, notes = "") {
    new_row <- data.table(
        timestamp = Sys.time(),
        strategy_id = strategy_id,
        description = description,
        parameters = paste(names(parameters), parameters, sep = "=", collapse = "; "),
        in_sample_sharpe = in_sample_sharpe,
        out_sample_sharpe = out_sample_sharpe,
        t_statistic = t_stat,
        p_value = p_val,
        notes = notes
    )

    rbind(log, new_row)
}

# Example usage
research_log <- create_research_log()
research_log <- log_strategy_test(
    research_log,
    strategy_id = "MA_001",
    description = "Simple MA crossover",
    parameters = list(fast = 20, slow = 50),
    in_sample_sharpe = 0.8,
    out_sample_sharpe = 0.3,
    t_stat = 1.5,
    p_val = 0.13,
    notes = "Underperforms in trending markets"
)

cat("\n=== Research Log Entry ===\n")
```

```
## 
## === Research Log Entry ===
```

``` r
print(research_log)
```

```
##              timestamp strategy_id         description       parameters
##                 <POSc>      <char>              <char>           <char>
## 1: 2026-01-21 10:39:01      MA_001 Simple MA crossover fast=20; slow=50
##    in_sample_sharpe out_sample_sharpe t_statistic p_value
##               <num>             <num>       <num>   <num>
## 1:              0.8               0.3         1.5    0.13
##                                notes
##                               <char>
## 1: Underperforms in trending markets
```


``` r
# 3. Adjusted inference for all tests

calculate_adjusted_significance <- function(observed_sharpe, n_tests, n_observations,
                                             confidence = 0.95) {
    # Expected max Sharpe under null
    expected_max_null <- sqrt(2 * log(n_tests)) / sqrt(n_observations)

    # Standard error of Sharpe (approximately)
    se_sharpe <- sqrt((1 + 0.5 * observed_sharpe^2) / n_observations)

    # Adjusted t-statistic
    t_adjusted <- (observed_sharpe - expected_max_null) / se_sharpe

    # Adjusted p-value (one-sided)
    p_adjusted <- 1 - pnorm(t_adjusted)

    # Bonferroni threshold
    bonferroni_threshold <- qnorm(1 - (1 - confidence) / (2 * n_tests)) * se_sharpe

    list(
        observed_sharpe = observed_sharpe,
        expected_under_null = expected_max_null,
        deflated_sharpe = observed_sharpe - expected_max_null,
        se_sharpe = se_sharpe,
        t_adjusted = t_adjusted,
        p_adjusted = p_adjusted,
        bonferroni_threshold = bonferroni_threshold,
        passes_bonferroni = observed_sharpe > bonferroni_threshold
    )
}

# Example: Did your best strategy survive adjustment?
adj_result <- calculate_adjusted_significance(
    observed_sharpe = 1.5,
    n_tests = 500,
    n_observations = 2520
)

cat("\n=== Adjusted Statistical Inference ===\n")
```

```
## 
## === Adjusted Statistical Inference ===
```

``` r
cat(sprintf("Observed Sharpe: %.2f\n", adj_result$observed_sharpe))
```

```
## Observed Sharpe: 1.50
```

``` r
cat(sprintf("Expected max under null: %.3f\n", adj_result$expected_under_null))
```

```
## Expected max under null: 0.070
```

``` r
cat(sprintf("Deflated Sharpe: %.2f\n", adj_result$deflated_sharpe))
```

```
## Deflated Sharpe: 1.43
```

``` r
cat(sprintf("Adjusted t-statistic: %.2f\n", adj_result$t_adjusted))
```

```
## Adjusted t-statistic: 49.24
```

``` r
cat(sprintf("Adjusted p-value: %.4f\n", adj_result$p_adjusted))
```

```
## Adjusted p-value: 0.0000
```

``` r
cat(sprintf("Bonferroni threshold: %.2f\n", adj_result$bonferroni_threshold))
```

```
## Bonferroni threshold: 0.11
```

``` r
cat(sprintf("Passes Bonferroni test: %s\n", adj_result$passes_bonferroni))
```

```
## Passes Bonferroni test: TRUE
```

---

## Quick Reference: Bias Detection Checklist

**Before running any backtest:**
- [ ] Clearly define hypothesis and parameters before looking at results
- [ ] Set success criteria in advance (Sharpe threshold, t-stat requirement)
- [ ] Document the test in a research log

**Look-Ahead Bias:**
- [ ] All signals are lagged by at least 1 period
- [ ] No future data used in any calculation
- [ ] Point-in-time data used for fundamentals
- [ ] Historical index constituents used (not current)
- [ ] Run lag correlation test on signals

**Survivorship Bias:**
- [ ] Data includes delisted securities
- [ ] Delisting returns properly incorporated
- [ ] No retroactive removal of bankruptcies
- [ ] Historical universe used at each point in time

**Overfitting:**
- [ ] Out-of-sample test period defined and untouched
- [ ] Number of parameters << √T
- [ ] Performance degrades gracefully with parameter changes
- [ ] Cross-validation shows consistent train/test performance
- [ ] Economic rationale for why strategy should work

**Data Mining:**
- [ ] Total number of tests recorded
- [ ] Multiple testing correction applied
- [ ] Deflated Sharpe ratio calculated
- [ ] Pre-registration documented
- [ ] All failures recorded alongside successes

## Summary

The biases covered in this chapter are not theoretical concerns—they are the primary reason most backtested strategies fail:

1. **Look-ahead bias** uses future information to make past decisions, creating artificial alpha that vanishes in live trading.

2. **Survivorship bias** excludes failures from history, inflating returns by 1-2% annually for broad market strategies.

3. **Overfitting** mistakes noise for signal, producing strategies that worked historically but have no predictive power.

4. **Data mining** finds spurious patterns through exhaustive search, guaranteeing that your best result is probably luck.

The solution is discipline: pre-register hypotheses, maintain complete research logs, use proper statistical corrections, and always assume your backtest is lying until proven otherwise.

## Exercises

1. **Look-Ahead Detection:** Take an existing strategy and implement a lag correlation test. What lag structure indicates the signal is properly constructed?

2. **Survivorship Simulation:** Extend the survivorship simulation to include different delisting scenarios (bankruptcy, acquisition at premium, voluntary delisting). How does each affect the bias magnitude?

3. **Overfitting Measurement:** Implement the deflated Sharpe ratio for a strategy you've tested. How many trials would make your result insignificant?

4. **Research Log:** Design and implement a research logging system for your strategy development. What fields should it include?
