---
letter: 'p'
word: 'ppo'
dictionary: 'mathematics'
category: 'machine-learning'
dataSource: 'assistant'
published: true
comments: true
linksTo: ['reinforcement-learning','markov-chain','policy-gradient']
linkedFrom: []
output:
  html_document:
    keep_md: true
---



<a id="ppo">PPO (Proximal Policy Optimization)</a> - An advanced **reinforcement learning** algorithm by OpenAI, improving **policy gradient** methods by controlling how far the new policy can deviate from the old policy. The objective uses a **clipped** surrogate function:

$$
L^{\mathrm{CLIP}}(\theta) = \hat{\mathbb{E}}_t\Bigl[\min(r_t(\theta)\hat{A}_t, \mathrm{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\Bigr],
$$
where:
- $r_t(\theta) = \frac{\pi_{\theta}(a_t\mid s_t)}{\pi_{\theta_{old}}(a_t\mid s_t)}$,
- $\hat{A}_t$ is an advantage estimate at time t,
- $\epsilon$ is a hyperparameter (like 0.1 or 0.2).

**Key points**:
- Prevents large policy updates that break old policy.
- Often combined with a value function critic for advantage estimation.

**R demonstration** (No standard PPO in base R, but let's conceptually illustrate partial code with `rlang`? We'll do a simplified snippet):


``` r
library(data.table)
set.seed(123)
cat("Implementing PPO in pure R is possible but quite complex. We'll pseudo-code a single update step:")
```

```
## Implementing PPO in pure R is possible but quite complex. We'll pseudo-code a single update step:
```

``` r
ppo_update_step <- function(log_prob_old, log_prob_new, advantage, epsilon=0.2) {
  ratio <- exp(log_prob_new - log_prob_old)
  unclipped <- ratio * advantage
  clipped <- pmax(pmin(ratio, 1+epsilon), 1-epsilon)* advantage
  # Surrogate objective
  obj <- mean(pmin(unclipped, clipped))
  obj
}

log_prob_old <- rnorm(10, mean=-1)
log_prob_new <- log_prob_old + rnorm(10, mean=0, sd=0.1)
advantage    <- rnorm(10, mean=1, sd=0.5)
ppo_update_step(log_prob_old, log_prob_new, advantage, 0.2)
```

```
## [1] 0.7827393
```
