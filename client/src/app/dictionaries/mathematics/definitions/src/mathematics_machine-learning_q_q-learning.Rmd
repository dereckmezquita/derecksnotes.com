---
letter: 'q'
word: 'q-learning'
dictionary: 'mathematics'
category: 'machine-learning'
dataSource: 'assistant'
published: true
comments: true

linksTo: ['reinforcement-learning','markov-chain']
linkedFrom: []
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
if(knitr::is_html_output()){
  knitr::knit_hooks$set(
    plot=function(x,options){
      cap<-options$fig.cap
      as.character(htmltools::tag("Figure",list(src=x,alt=cap,paste("\n\t",cap,"\n",sep=""))))
    }
  )
}
knitr::opts_knit$set(root.dir="/Users/work/Coding/derecksprojects/derecksnotes.com-playground/derecksnotes.com/client/src/app/dictionaries/mathematics/definitions")
knitr::knit_hooks$set(optipng=knitr::hook_optipng)
knitr::opts_chunk$set(dpi=300,fig.width=10,fig.height=7)
```

<a id="q-learning">Q-Learning</a> - A **reinforcement learning** algorithm that learns a value function \(Q(s,a)\) giving the expected **cumulative reward** for taking action \(a\) in state \(s\), then following some policy. The update rule:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \bigl[r + \gamma \max_{a'}Q(s', a') - Q(s,a)\bigr]
$$

where:
- \(\alpha\) is the learning rate,
- \(\gamma\) is the discount factor,
- \(r\) is the immediate reward after performing action \(a\) in state \(s\) to reach new state \(s'\).

**Key points**:
- Model-free: no prior knowledge of environment dynamics is needed.
- A type of [Markov-chain](#markov-chain) approach if states follow Markov property.

**R demonstration** (mini example of a gridworld Q-learning approach, conceptual code only):

```{r}
library(data.table)

# We'll define a small 1D environment: states 1..5, with 5 as terminal
Q <- matrix(0, nrow=5, ncol=2, dimnames=list(1:5, c("left","right")))
alpha <- 0.1
gamma <- 0.9

simulate_episode <- function() {
  s <- 3  # start in the middle
  total_reward <- 0
  while(s != 1 && s != 5) {
    # pick an action
    a <- sample(c("left","right"),1)
    s_new <- if(a=="left") s-1 else s+1
    r <- if(s_new==5) 10 else 0
    # Q update
    Q[s,a] <<- Q[s,a] + alpha*(r + gamma*max(Q[s_new,]) - Q[s,a])
    s <- s_new
    total_reward <- total_reward + r
  }
  total_reward
}

# run a few episodes
for(i in 1:100){
  simulate_episode()
}

Q
```
